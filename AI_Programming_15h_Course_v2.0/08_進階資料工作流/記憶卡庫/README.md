# 模組 8 記憶卡庫 - 進階資料工作流程

## 📋 記憶卡概述

本記憶卡庫包含 **30 張情境式記憶卡**，涵蓋 ETL、RAG、資料分析三大主題。

**設計原則**：
- 情境式問答（而非定義記憶）
- 強調決策能力（何時用什麼工具）
- 實戰導向（真實問題場景）

---

## 📚 記憶卡分類

### 類別 1：ETL 核心概念（10 張）

```
主題分布：
- ETL 基礎概念（3 張）
- 資料清洗策略（3 張）
- 工具選擇決策（2 張）
- 錯誤處理（2 張）
```

**示範記憶卡 1：工具選擇**
```
【情境】
你需要每天處理 5GB 的 CSV 資料，目前用 Pandas 會記憶體不足。

【問題】
你會如何調整工具選擇？列出 2-3 個選項。

【參考答案】
選項 1：使用 Polars
- 多核心平行處理
- 記憶體效率高
- API 與 Pandas 相似

選項 2：使用 Pandas + Chunk Processing
- 分批讀取（chunksize 參數）
- 逐批處理與載入
- 不需要學新工具

選項 3：考慮 Dask
- 延遲計算
- 自動平行化
- 適合更大規模

【決策邏輯】
資料量 < 10GB → Polars 最佳（效能與易用性平衡）
資料量 > 10GB → Dask（分散式處理）
```

**示範記憶卡 2：資料清洗**
```
【情境】
CSV 資料有 3 種日期格式：2025-01-30、30/01/2025、01-30-2025

【問題】
用 Pandas 如何標準化這些日期？

【參考答案】
方法 1：多格式解析
```python
def parse_date(date_str):
    formats = ['%Y-%m-%d', '%d/%m/%Y', '%m-%d-%Y']
    for fmt in formats:
        try:
            return pd.to_datetime(date_str, format=fmt)
        except:
            continue
    return pd.NaT

df['date'] = df['date'].apply(parse_date)
```

方法 2：使用 pd.to_datetime 的 infer
```python
df['date'] = pd.to_datetime(df['date'], infer_datetime_format=True, errors='coerce')
```

【記憶點】
errors='coerce' → 無效日期變成 NaT 而非報錯
```

---

### 類別 2：RAG 系統架構（10 張）

```
主題分布：
- RAG 核心概念（3 張）
- 切塊策略（2 張）
- 向量資料庫選擇（2 張）
- 檢索優化（3 張）
```

**示範記憶卡 3：RAG 工具選擇**
```
【情境】
你要建立公司內部文檔問答系統，有 500 份 PDF 技術文檔。

【問題】
如何選擇 RAG 工具組合？

【參考答案】
推薦組合：
1. 框架：LangChain
   - 生態完整
   - 文檔豐富
   - 易於擴展

2. 向量 DB：Chroma（開發）→ Pinecone（生產）
   - Chroma：本地測試，免費
   - Pinecone：雲端託管，穩定

3. 嵌入：OpenAI ada-002
   - 效果好
   - 成本可控
   - API 穩定

4. LLM：GPT-4（高品質）or GPT-3.5（成本低）

【記憶點】
開發階段用免費工具快速驗證，生產環境再升級為付費服務。
```

**示範記憶卡 4：切塊策略**
```
【情境】
RAG 系統回答經常不精確，檢索到的內容不相關。

【問題】
如何調整切塊策略？

【參考答案】
診斷步驟：
1. 檢查 chunk_size
   - 太大（> 1500）→ 上下文不精確
   - 太小（< 300）→ 語義不完整
   - 推薦：500-1000

2. 調整 chunk_overlap
   - 建議：chunk_size 的 10-20%
   - 保留上下文連貫性

3. 改用 RecursiveCharacterTextSplitter
   - 按段落、句子切分
   - 保持語義完整性

4. 增加檢索數量（k）
   - 從 k=3 增加到 k=5
   - 提高覆蓋率

【記憶點】
切塊策略沒有萬能配置，需根據文檔類型實驗調整。
```

---

### 類別 3：資料分析自動化（10 張）

```
主題分布：
- EDA 自動化（3 張）
- 視覺化選擇（3 張）
- 洞察提取（2 張）
- 報告生成（2 張）
```

**示範記憶卡 5：視覺化選擇**
```
【情境】
你需要分析銷售趨勢、產品比較、地區分佈。

【問題】
每個分析應該用什麼圖表？

【參考答案】
1. 銷售趨勢 → 折線圖（Line Chart）
   - 顯示時間序列變化
   - 容易識別趨勢

2. 產品比較 → 長條圖（Bar Chart）
   - 水平比較多個產品
   - 清楚顯示排名

3. 地區分佈 → 地圖（Map）或圓餅圖（Pie Chart）
   - 地圖：地理分佈直觀
   - 圓餅圖：佔比一目了然

【記憶點】
時序 → 折線圖
比較 → 長條圖
分佈 → 直方圖/箱型圖
關聯 → 散佈圖
佔比 → 圓餅圖
```

**示範記憶卡 6：AI Agent 選擇**
```
【情境】
你需要生成完整的資料分析報告，包含 EDA、視覺化、洞察。

【問題】
應該用哪個 AI Agent？

【參考答案】
選擇：data-analyst Agent

原因：
1. 專業的資料分析知識
2. 自動選擇合適的圖表類型
3. 識別資料中的模式與異常
4. 生成專業的分析報告

使用方式：
```bash
/agents:data-analyst

我有一個銷售數據 CSV，請幫我：
1. 進行探索性資料分析
2. 生成視覺化圖表
3. 提取關鍵洞察
4. 產出分析報告
```

【記憶點】
資料相關任務 → data-analyst / data-engineer Agent
```

---

## 🎯 使用建議

### 學習策略

**每日複習**：
- 每天複習 5-10 張記憶卡
- 重點記憶「決策邏輯」而非「定義」
- 遇到不熟悉的立即實作驗證

**間隔重複**：
- 使用 Anki 或類似工具
- 設定間隔：1 天 → 3 天 → 7 天 → 14 天
- 不熟悉的卡片縮短間隔

**情境練習**：
- 不只背答案，理解背後的邏輯
- 嘗試創造類似情境
- 與同事討論決策過程

---

## 📊 進度追蹤

使用以下清單追蹤你的學習進度：

### ETL 核心概念
- [ ] 工具選擇決策（5 張）
- [ ] 資料清洗技巧（5 張）

### RAG 系統架構
- [ ] 核心概念（3 張）
- [ ] 切塊策略（3 張）
- [ ] 向量資料庫選擇（2 張）
- [ ] 檢索優化（2 張）

### 資料分析自動化
- [ ] EDA 自動化（3 張）
- [ ] 視覺化選擇（3 張）
- [ ] 洞察提取（2 張）
- [ ] 報告生成（2 張）

---

## 💡 記憶卡設計原則（供參考）

### ❌ 不好的記憶卡

```
Q: 什麼是 ETL？
A: ETL 代表 Extract-Transform-Load
```

**問題**：只是背定義，無法應用

---

### ✅ 好的記憶卡

```
【情境】
你需要每天從 3 個資料庫提取資料、清洗、載入資料倉儲。

【問題】
這是什麼類型的工作流程？應該用什麼工具？

【答案】
類型：ETL 工作流程
工具建議：
- 簡單流程：Python 腳本
- 複雜流程：Airflow/Prefect
- 企業級：專業 ETL 工具

【記憶點】
ETL = 資料提取、轉換、載入的自動化流程
```

---

## 🔗 Anki 匯入格式

所有記憶卡可匯出為 Anki 格式：
```
情境：[情境描述]
問題：[問題]
答案：[參考答案]
記憶點：[關鍵記憶點]
```

---

**記憶卡庫版本**：v1.0
**最後更新**：2025-10-30
**總卡數**：30 張（規劃中）
**已完成**：6 張示範卡
