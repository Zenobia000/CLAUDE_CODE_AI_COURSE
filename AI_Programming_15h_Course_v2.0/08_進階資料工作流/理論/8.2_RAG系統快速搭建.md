# 8.2 RAG 系統快速搭建 - 10 分鐘搭建檢索增強生成系統

## 📋 章節概述

**學習時長**：15 分鐘（理論）+ 10 分鐘（實作）
**難度**：⭐⭐⭐ 中級
**前置知識**：Python 基礎、API 使用概念

本章教你：
- ✅ 理解 RAG 是什麼（Retrieval-Augmented Generation）
- ✅ 掌握 RAG 系統的基本架構
- ✅ 10 分鐘搭建最小可行原型
- ✅ 選擇合適的 RAG 工具與向量資料庫

---

## 🎯 什麼是 RAG？

### 基本定義

**RAG** = Retrieval (檢索) + Augmented (增強) + Generation (生成)

```
傳統 LLM 的問題：
你：請問我們公司的退貨政策是什麼？
LLM：我不知道你們公司的政策（或胡亂編造）

RAG 系統的解決方式：
1. 檢索：從公司文檔中找到相關段落
2. 增強：把找到的段落加入 prompt
3. 生成：LLM 基於真實文檔回答

你：請問我們公司的退貨政策是什麼？
RAG：根據《客戶服務手冊》第 3.2 節，退貨政策如下...
```

### 核心概念圖解

```
┌─────────────────────────────────────────────────────────┐
│                     RAG 系統架構                         │
└─────────────────────────────────────────────────────────┘

1. 建立知識庫（一次性）
   ┌─────────┐     ┌─────────┐     ┌─────────────┐
   │ 文檔集合 │────>│ 切塊處理 │────>│ 向量化      │
   │ (PDF等) │     │ Chunking│     │ Embedding   │
   └─────────┘     └─────────┘     └──────┬──────┘
                                            │
                                            ↓
                                   ┌────────────────┐
                                   │ 向量資料庫     │
                                   │ (Chroma/      │
                                   │  Pinecone)    │
                                   └────────────────┘

2. 問答流程（每次查詢）
   ┌─────────┐     ┌─────────┐     ┌─────────────┐
   │ 用戶提問 │────>│ 向量化   │────>│ 相似度搜尋  │
   │ Query   │     │ Embed   │     │ Search      │
   └─────────┘     └─────────┘     └──────┬──────┘
                                            │
                                            ↓
                                   ┌────────────────┐
                                   │ 檢索 Top-K     │
                                   │ 相關文檔       │
                                   └───────┬────────┘
                                            │
                                            ↓
   ┌─────────┐     ┌─────────────────────┐
   │ 最終答案 │<────│ LLM 生成            │
   │ Answer  │     │ (with context)      │
   └─────────┘     └─────────────────────┘
```

---

## 🤔 為什麼需要 RAG？

### 解決 LLM 的三大問題

**問題 1：幻覺（Hallucination）**
```
問：請列出我們公司 2024 Q4 的營收數字

純 LLM：
我看到貴公司 2024 Q4 營收約為 500 萬美元...
（完全是編造的！）

RAG 系統：
根據《2024 Q4 財報》，營收為 $1,234,567。
（基於真實文檔）
```

**問題 2：知識過時**
```
問：最新的產品功能有哪些？

純 LLM：
我的訓練數據截至 2023 年 10 月...
（無法回答最新信息）

RAG 系統：
根據《產品更新日誌 2025-01-15》，新功能包括...
（即時更新，無需重新訓練模型）
```

**問題 3：無法處理私有數據**
```
問：我們內部的技術規範是什麼？

純 LLM：
我無法訪問你們的內部文檔...

RAG 系統：
根據《技術規範 v3.2》，規範如下...
（可處理公司內部文檔）
```

### Linux 類比

```
純 LLM = 只有 RAM 的電腦
- 只能記住訓練時學到的東西
- 無法查閱外部資料

RAG = RAM + 硬碟
- RAM（LLM）：理解與生成能力
- 硬碟（向量資料庫）：儲存大量文檔
- 需要資料時，從硬碟檢索到 RAM

就像：
man command  →  從文檔中檢索資訊（RAG）
直接回答      →  從記憶中回答（純 LLM）
```

---

## 🏗️ RAG 系統架構速覽

### 四大核心組件

```
1. 文檔處理器（Document Processor）
   ├─ 文檔載入（Loader）：PDF, DOCX, HTML, Markdown
   ├─ 文本切塊（Chunker）：切成適合檢索的小段落
   └─ 元數據提取（Metadata）：標題、作者、日期等

2. 嵌入模型（Embedding Model）
   ├─ 文本向量化：將文字轉換成數字向量
   ├─ 語義表示：相似的文本在向量空間中距離近
   └─ 常用模型：OpenAI ada-002, Sentence Transformers

3. 向量資料庫（Vector Database）
   ├─ 向量儲存：高效儲存數百萬個向量
   ├─ 相似度搜尋：快速找到最相關的文檔
   └─ 常用工具：Chroma, Pinecone, Weaviate, Qdrant

4. 生成模型（LLM）
   ├─ 接收：用戶問題 + 檢索到的上下文
   ├─ 生成：基於上下文的答案
   └─ 常用模型：GPT-4, Claude, Gemini
```

### 資料流圖

```
離線流程（建立索引）：
文檔 → 切塊 → 嵌入 → 儲存到向量資料庫

線上流程（問答）：
問題 → 嵌入 → 檢索 Top-K → 注入 Prompt → LLM 生成 → 答案
```

---

## 🚀 AI 輔助 RAG 系統搭建

### 10 分鐘快速搭建流程

**時間分配**：
```
分鐘 1-3：環境設置與依賴安裝
分鐘 4-5：文檔載入與切塊
分鐘 6-7：建立向量資料庫
分鐘 8-9：實現問答功能
分鐘 10：測試與驗證
```

### 與 AI 對話範例

```
你：我想建立一個 RAG 系統，用來回答技術文檔的問題

Claude Code：
我會幫你快速搭建一個原型：

1. 選擇工具組合：
   - 框架：LangChain（最成熟）
   - 向量資料庫：Chroma（本地開發友善）
   - 嵌入：OpenAI ada-002（效果好）
   - LLM：GPT-4（生成品質高）

2. 生成完整代碼：
   - 文檔載入器
   - 切塊策略
   - 向量化與儲存
   - 問答接口

3. 提供測試案例
```

---

## 🎯 完整範例：技術文檔 RAG 系統

### 情境描述

**需求**：
- 公司有大量技術文檔（Markdown, PDF）
- 新人經常問相同的問題
- 需要一個自動問答系統

### 完整實現代碼

**步驟 1：安裝依賴**

```bash
# 安裝必要套件
pip install langchain openai chromadb tiktoken

# 或使用 poetry（推薦）
poetry add langchain openai chromadb tiktoken
```

**步驟 2：完整 RAG 系統代碼**

**`rag_system.py`**：
```python
#!/usr/bin/env python3
"""
Technical Documentation RAG System

A simple RAG system for answering questions about technical documentation.
"""

import os
from pathlib import Path
from typing import List, Dict, Any

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ===== Configuration =====
DOCS_DIR = "docs"  # 文檔目錄
CHROMA_DIR = "chroma_db"  # 向量資料庫目錄
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ===== 1. Document Loading =====
def load_documents(docs_path: str) -> List[Any]:
    """
    Load all markdown and text files from directory

    Args:
        docs_path: Path to documents directory

    Returns:
        List of loaded documents
    """
    print(f"📂 Loading documents from {docs_path}...")

    loader = DirectoryLoader(
        docs_path,
        glob="**/*.md",  # 載入所有 .md 檔案
        loader_cls=TextLoader,
        show_progress=True
    )

    documents = loader.load()
    print(f"✅ Loaded {len(documents)} documents")

    return documents


# ===== 2. Text Chunking =====
def chunk_documents(documents: List[Any], chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Any]:
    """
    Split documents into smaller chunks

    Args:
        documents: List of documents
        chunk_size: Size of each chunk
        chunk_overlap: Overlap between chunks

    Returns:
        List of chunked documents
    """
    print(f"✂️  Chunking documents (size={chunk_size}, overlap={chunk_overlap})...")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )

    chunks = text_splitter.split_documents(documents)
    print(f"✅ Created {len(chunks)} chunks")

    return chunks


# ===== 3. Create Vector Store =====
def create_vector_store(chunks: List[Any], persist_directory: str) -> Chroma:
    """
    Create and persist vector store

    Args:
        chunks: List of document chunks
        persist_directory: Directory to save vector store

    Returns:
        Chroma vector store
    """
    print(f"🔢 Creating vector embeddings...")

    embeddings = OpenAIEmbeddings()

    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=persist_directory
    )

    vectorstore.persist()
    print(f"✅ Vector store created and saved to {persist_directory}")

    return vectorstore


# ===== 4. Load Existing Vector Store =====
def load_vector_store(persist_directory: str) -> Chroma:
    """
    Load existing vector store

    Args:
        persist_directory: Directory of saved vector store

    Returns:
        Chroma vector store
    """
    print(f"📥 Loading vector store from {persist_directory}...")

    embeddings = OpenAIEmbeddings()

    vectorstore = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings
    )

    print(f"✅ Vector store loaded")
    return vectorstore


# ===== 5. Create QA Chain =====
def create_qa_chain(vectorstore: Chroma) -> RetrievalQA:
    """
    Create question-answering chain

    Args:
        vectorstore: Vector store for retrieval

    Returns:
        RetrievalQA chain
    """
    print(f"🔗 Creating QA chain...")

    # Custom prompt template
    prompt_template = """You are a helpful technical documentation assistant.
Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.

Context:
{context}

Question: {question}

Answer in Traditional Chinese with technical accuracy:"""

    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )

    llm = ChatOpenAI(
        model_name="gpt-4",
        temperature=0  # 降低隨機性，提高準確性
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}  # 檢索前 3 個最相關的文檔
        ),
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )

    print(f"✅ QA chain created")
    return qa_chain


# ===== 6. Ask Question =====
def ask_question(qa_chain: RetrievalQA, question: str) -> Dict[str, Any]:
    """
    Ask a question and get answer with sources

    Args:
        qa_chain: QA chain
        question: User question

    Returns:
        Dictionary with answer and source documents
    """
    print(f"\n❓ Question: {question}")
    print("🤔 Thinking...")

    result = qa_chain({"query": question})

    print(f"\n💬 Answer:\n{result['result']}")
    print(f"\n📚 Sources:")
    for i, doc in enumerate(result['source_documents'], 1):
        print(f"  {i}. {doc.metadata.get('source', 'Unknown')}")

    return result


# ===== Main Functions =====
def build_knowledge_base(docs_path: str, persist_directory: str) -> None:
    """
    Build knowledge base from documents

    Args:
        docs_path: Path to documents
        persist_directory: Where to save vector store
    """
    print("=" * 50)
    print("Building Knowledge Base")
    print("=" * 50)

    # Load documents
    documents = load_documents(docs_path)

    # Chunk documents
    chunks = chunk_documents(documents)

    # Create vector store
    create_vector_store(chunks, persist_directory)

    print("\n✅ Knowledge base built successfully!")


def run_qa_system(persist_directory: str) -> None:
    """
    Run interactive QA system

    Args:
        persist_directory: Path to vector store
    """
    print("=" * 50)
    print("Technical Documentation QA System")
    print("=" * 50)

    # Load vector store
    vectorstore = load_vector_store(persist_directory)

    # Create QA chain
    qa_chain = create_qa_chain(vectorstore)

    print("\n💡 Ready! Type your questions (or 'quit' to exit)\n")

    while True:
        question = input("Your question: ").strip()

        if question.lower() in ['quit', 'exit', 'q']:
            print("👋 Goodbye!")
            break

        if not question:
            continue

        ask_question(qa_chain, question)
        print("\n" + "-" * 50 + "\n")


# ===== CLI =====
if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='RAG System for Technical Documentation')
    parser.add_argument('--build', action='store_true', help='Build knowledge base')
    parser.add_argument('--docs', default=DOCS_DIR, help='Documents directory')
    parser.add_argument('--db', default=CHROMA_DIR, help='Vector store directory')

    args = parser.parse_args()

    if args.build:
        build_knowledge_base(args.docs, args.db)
    else:
        run_qa_system(args.db)
```

### 使用方式

**步驟 1：準備文檔**
```bash
# 建立文檔目錄
mkdir docs

# 放入你的文檔
cp /path/to/technical/*.md docs/
```

**步驟 2：建立知識庫（一次性）**
```bash
# 設定 OpenAI API Key
export OPENAI_API_KEY="sk-..."

# 建立知識庫
python rag_system.py --build --docs docs --db chroma_db
```

輸出：
```
==================================================
Building Knowledge Base
==================================================
📂 Loading documents from docs...
✅ Loaded 25 documents
✂️  Chunking documents (size=1000, overlap=200)...
✅ Created 342 chunks
🔢 Creating vector embeddings...
✅ Vector store created and saved to chroma_db

✅ Knowledge base built successfully!
```

**步驟 3：運行問答系統**
```bash
python rag_system.py --db chroma_db
```

**互動範例**：
```
==================================================
Technical Documentation QA System
==================================================
📥 Loading vector store from chroma_db...
✅ Vector store loaded
🔗 Creating QA chain...
✅ QA chain created

💡 Ready! Type your questions (or 'quit' to exit)

Your question: 如何安裝這個系統？

❓ Question: 如何安裝這個系統？
🤔 Thinking...

💬 Answer:
根據技術文檔，安裝步驟如下：

1. 確保系統需求：
   - Python 3.8 或更高版本
   - pip 或 poetry 套件管理工具

2. 安裝依賴：
   ```bash
   pip install -r requirements.txt
   ```

3. 設定環境變數：
   ```bash
   export DATABASE_URL="postgresql://..."
   ```

4. 執行資料庫遷移：
   ```bash
   python manage.py migrate
   ```

5. 啟動服務：
   ```bash
   python manage.py runserver
   ```

📚 Sources:
  1. docs/installation.md
  2. docs/quickstart.md
  3. docs/configuration.md

--------------------------------------------------
```

---

## 🔧 常用 RAG 工具概覽

### 框架選擇

```
工具          優勢                    劣勢                 推薦場景
─────────────────────────────────────────────────────────────────────
LangChain    ⭐⭐⭐⭐⭐              學習曲線陡峭          快速原型
             生態完整                                    企業應用
             文檔豐富

LlamaIndex   ⭐⭐⭐⭐              生態較小              資料整合
             專注資料整合            社群較小              複雜檢索
             檢索策略豐富

Haystack     ⭐⭐⭐                企業功能多            需求明確的
             德國製造                配置複雜              企業專案
             企業級

自己實現      ⭐⭐                  需要時間              學習目的
             完全控制                需要維護              特殊需求
             無依賴
```

### 向量資料庫選擇

```
資料庫        類型      優勢                    適用場景
───────────────────────────────────────────────────────────
Chroma       本地      免費、易用              開發測試
                      Python 友善             小型專案

Pinecone     雲端      全託管                  生產環境
                      效能優異                大規模應用
                      按使用計費              不想維運

Weaviate     開源      功能豐富                企業自建
                      可自架                  需要控制
                      GraphQL API             複雜查詢

Qdrant       開源      高效能                  大規模
                      Rust 編寫               效能要求高
                      可自架                  專業團隊

Milvus       開源      超大規模                數百萬級別
                      分散式                  企業級應用
                      生產級                  資料科學團隊
```

### 嵌入模型選擇

```
模型                    維度    效果    成本    推薦
─────────────────────────────────────────────────────
OpenAI ada-002         1536    ⭐⭐⭐⭐  $     快速開發
OpenAI text-3-small    1536    ⭐⭐⭐⭐⭐ $     最新推薦
OpenAI text-3-large    3072    ⭐⭐⭐⭐⭐ $$    高品質

Sentence Transformers  384-768 ⭐⭐⭐   免費   本地部署
(開源)                                       預算有限

Cohere Embed           1024    ⭐⭐⭐⭐  $     多語言支援

```

---

## 🎯 10 分鐘快速上手指南

### 最小可行原型

**目標**：10 分鐘內運行第一個 RAG 系統

**`minimal_rag.py`**（簡化版）：
```python
"""
Minimal RAG System - 10 minutes to deploy
"""

from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

# 1. Load document (1 minute)
loader = TextLoader("your_document.txt")
documents = loader.load()

# 2. Split into chunks (1 minute)
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# 3. Create embeddings and vector store (3 minutes)
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(texts, embeddings)

# 4. Create QA chain (2 minutes)
qa = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(),
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# 5. Ask questions (3 minutes - testing)
result = qa.run("What is this document about?")
print(result)
```

**執行**：
```bash
# 安裝（2 分鐘）
pip install langchain openai chromadb

# 執行（8 分鐘）
export OPENAI_API_KEY="sk-..."
python minimal_rag.py
```

---

## ⚠️ 常見陷阱與解決方案

### 陷阱 1：切塊策略不當

**問題**：
```python
# 錯誤：切塊太大或太小
text_splitter = CharacterTextSplitter(chunk_size=5000)  # 太大，上下文不精確
text_splitter = CharacterTextSplitter(chunk_size=100)   # 太小，語義不完整
```

**解決方案**：
```python
# 正確：根據內容類型選擇
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # 技術文檔推薦 500-1500
    chunk_overlap=200,    # 20% overlap 保留上下文
    separators=["\n\n", "\n", " ", ""]  # 按段落、句子、單詞切分
)
```

### 陷阱 2：檢索不到相關文檔

**問題**：
```python
# 錯誤：只檢索 1 個文檔
retriever = vectorstore.as_retriever(search_kwargs={"k": 1})
```

**解決方案**：
```python
# 正確：檢索 3-5 個，增加覆蓋率
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}  # 或根據需求調整
)

# 進階：使用 MMR（最大邊際相關性）避免重複
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 3, "fetch_k": 10}
)
```

### 陷阱 3：成本失控

**問題**：
```python
# 錯誤：每次重新嵌入所有文檔
for question in questions:
    vectorstore = Chroma.from_documents(documents, embeddings)  # 重複嵌入！
```

**解決方案**：
```python
# 正確：嵌入一次，持久化
vectorstore = Chroma.from_documents(
    documents,
    embeddings,
    persist_directory="chroma_db"  # 儲存到硬碟
)

# 後續使用：直接載入
vectorstore = Chroma(
    persist_directory="chroma_db",
    embedding_function=embeddings
)
```

### 陷阱 4：答案品質不佳

**問題**：LLM 生成的答案偏離文檔內容

**解決方案**：
```python
# 自訂 Prompt，強調「基於文檔」
prompt_template = """You must answer based ONLY on the provided context.
If the context doesn't contain the answer, say "I don't have enough information."
Do NOT make up information.

Context:
{context}

Question: {question}

Answer:"""
```

---

## 📊 效能優化建議

### 優化技巧

**1. 批次嵌入**
```python
# 不要：逐個嵌入
for doc in documents:
    embedding = embeddings.embed_query(doc)

# 要：批次嵌入
embeddings.embed_documents([doc.page_content for doc in documents])
```

**2. 快取檢索結果**
```python
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_retrieve(query: str):
    return vectorstore.similarity_search(query, k=3)
```

**3. 使用更快的嵌入模型**
```python
# 生產環境：自架 Sentence Transformers
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
# 速度快 10 倍，免費，適合大規模
```

---

## 🎯 實戰練習

### 練習 1：搭建你的第一個 RAG 系統

**任務**：
```
1. 收集 5-10 個 Markdown 文檔（或用範例文檔）
2. 使用上面的代碼搭建 RAG 系統
3. 測試 10 個問題
4. 評估答案品質
```

### 練習 2：優化檢索策略

**任務**：
```
1. 調整 chunk_size（試試 500, 1000, 1500）
2. 調整 k（檢索數量：1, 3, 5, 10）
3. 比較答案品質
4. 記錄最佳組合
```

---

## 📚 延伸閱讀

**框架文檔**：
- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)
- [LlamaIndex Docs](https://docs.llamaindex.ai/)

**向量資料庫**：
- [Chroma Docs](https://docs.trychroma.com/)
- [Pinecone Guides](https://docs.pinecone.io/)
- [Weaviate Docs](https://weaviate.io/developers/weaviate)

**進階技巧**：
- [Advanced RAG Techniques](https://blog.langchain.dev/advanced-rag/)
- [RAG Evaluation](https://www.confident-ai.com/blog/rag-evaluation)

---

## 🎯 學習檢查點

完成本章後，你應該能夠：

- [ ] 說明 RAG 的核心概念與架構
- [ ] 10 分鐘搭建最小可行 RAG 原型
- [ ] 選擇合適的 RAG 工具與向量資料庫
- [ ] 理解常見陷阱與優化技巧
- [ ] 評估 RAG 系統的答案品質

---

**章節版本**：v1.0
**最後更新**：2025-10-30
**預計學習時長**：15 分鐘（理論）+ 10 分鐘（實作）
