# 8.2 RAG ç³»çµ±å¿«é€Ÿæ­å»º - 10 åˆ†é˜æ­å»ºæª¢ç´¢å¢å¼·ç”Ÿæˆç³»çµ±

## ğŸ“‹ ç« ç¯€æ¦‚è¿°

**å­¸ç¿’æ™‚é•·**ï¼š15 åˆ†é˜ï¼ˆç†è«–ï¼‰+ 10 åˆ†é˜ï¼ˆå¯¦ä½œï¼‰
**é›£åº¦**ï¼šâ­â­â­ ä¸­ç´š
**å‰ç½®çŸ¥è­˜**ï¼šPython åŸºç¤ã€API ä½¿ç”¨æ¦‚å¿µ

æœ¬ç« æ•™ä½ ï¼š
- âœ… ç†è§£ RAG æ˜¯ä»€éº¼ï¼ˆRetrieval-Augmented Generationï¼‰
- âœ… æŒæ¡ RAG ç³»çµ±çš„åŸºæœ¬æ¶æ§‹
- âœ… 10 åˆ†é˜æ­å»ºæœ€å°å¯è¡ŒåŸå‹
- âœ… é¸æ“‡åˆé©çš„ RAG å·¥å…·èˆ‡å‘é‡è³‡æ–™åº«

---

## ğŸ¯ ä»€éº¼æ˜¯ RAGï¼Ÿ

### åŸºæœ¬å®šç¾©

**RAG** = Retrieval (æª¢ç´¢) + Augmented (å¢å¼·) + Generation (ç”Ÿæˆ)

```
å‚³çµ± LLM çš„å•é¡Œï¼š
ä½ ï¼šè«‹å•æˆ‘å€‘å…¬å¸çš„é€€è²¨æ”¿ç­–æ˜¯ä»€éº¼ï¼Ÿ
LLMï¼šæˆ‘ä¸çŸ¥é“ä½ å€‘å…¬å¸çš„æ”¿ç­–ï¼ˆæˆ–èƒ¡äº‚ç·¨é€ ï¼‰

RAG ç³»çµ±çš„è§£æ±ºæ–¹å¼ï¼š
1. æª¢ç´¢ï¼šå¾å…¬å¸æ–‡æª”ä¸­æ‰¾åˆ°ç›¸é—œæ®µè½
2. å¢å¼·ï¼šæŠŠæ‰¾åˆ°çš„æ®µè½åŠ å…¥ prompt
3. ç”Ÿæˆï¼šLLM åŸºæ–¼çœŸå¯¦æ–‡æª”å›ç­”

ä½ ï¼šè«‹å•æˆ‘å€‘å…¬å¸çš„é€€è²¨æ”¿ç­–æ˜¯ä»€éº¼ï¼Ÿ
RAGï¼šæ ¹æ“šã€Šå®¢æˆ¶æœå‹™æ‰‹å†Šã€‹ç¬¬ 3.2 ç¯€ï¼Œé€€è²¨æ”¿ç­–å¦‚ä¸‹...
```

### æ ¸å¿ƒæ¦‚å¿µåœ–è§£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RAG ç³»çµ±æ¶æ§‹                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. å»ºç«‹çŸ¥è­˜åº«ï¼ˆä¸€æ¬¡æ€§ï¼‰
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ æ–‡æª”é›†åˆ â”‚â”€â”€â”€â”€>â”‚ åˆ‡å¡Šè™•ç† â”‚â”€â”€â”€â”€>â”‚ å‘é‡åŒ–      â”‚
   â”‚ (PDFç­‰) â”‚     â”‚ Chunkingâ”‚     â”‚ Embedding   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                                            â†“
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚ å‘é‡è³‡æ–™åº«     â”‚
                                   â”‚ (Chroma/      â”‚
                                   â”‚  Pinecone)    â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. å•ç­”æµç¨‹ï¼ˆæ¯æ¬¡æŸ¥è©¢ï¼‰
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ ç”¨æˆ¶æå• â”‚â”€â”€â”€â”€>â”‚ å‘é‡åŒ–   â”‚â”€â”€â”€â”€>â”‚ ç›¸ä¼¼åº¦æœå°‹  â”‚
   â”‚ Query   â”‚     â”‚ Embed   â”‚     â”‚ Search      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                                            â†“
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚ æª¢ç´¢ Top-K     â”‚
                                   â”‚ ç›¸é—œæ–‡æª”       â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                                            â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ æœ€çµ‚ç­”æ¡ˆ â”‚<â”€â”€â”€â”€â”‚ LLM ç”Ÿæˆ            â”‚
   â”‚ Answer  â”‚     â”‚ (with context)      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤” ç‚ºä»€éº¼éœ€è¦ RAGï¼Ÿ

### è§£æ±º LLM çš„ä¸‰å¤§å•é¡Œ

**å•é¡Œ 1ï¼šå¹»è¦ºï¼ˆHallucinationï¼‰**
```
å•ï¼šè«‹åˆ—å‡ºæˆ‘å€‘å…¬å¸ 2024 Q4 çš„ç‡Ÿæ”¶æ•¸å­—

ç´” LLMï¼š
æˆ‘çœ‹åˆ°è²´å…¬å¸ 2024 Q4 ç‡Ÿæ”¶ç´„ç‚º 500 è¬ç¾å…ƒ...
ï¼ˆå®Œå…¨æ˜¯ç·¨é€ çš„ï¼ï¼‰

RAG ç³»çµ±ï¼š
æ ¹æ“šã€Š2024 Q4 è²¡å ±ã€‹ï¼Œç‡Ÿæ”¶ç‚º $1,234,567ã€‚
ï¼ˆåŸºæ–¼çœŸå¯¦æ–‡æª”ï¼‰
```

**å•é¡Œ 2ï¼šçŸ¥è­˜éæ™‚**
```
å•ï¼šæœ€æ–°çš„ç”¢å“åŠŸèƒ½æœ‰å“ªäº›ï¼Ÿ

ç´” LLMï¼š
æˆ‘çš„è¨“ç·´æ•¸æ“šæˆªè‡³ 2023 å¹´ 10 æœˆ...
ï¼ˆç„¡æ³•å›ç­”æœ€æ–°ä¿¡æ¯ï¼‰

RAG ç³»çµ±ï¼š
æ ¹æ“šã€Šç”¢å“æ›´æ–°æ—¥èªŒ 2025-01-15ã€‹ï¼Œæ–°åŠŸèƒ½åŒ…æ‹¬...
ï¼ˆå³æ™‚æ›´æ–°ï¼Œç„¡éœ€é‡æ–°è¨“ç·´æ¨¡å‹ï¼‰
```

**å•é¡Œ 3ï¼šç„¡æ³•è™•ç†ç§æœ‰æ•¸æ“š**
```
å•ï¼šæˆ‘å€‘å…§éƒ¨çš„æŠ€è¡“è¦ç¯„æ˜¯ä»€éº¼ï¼Ÿ

ç´” LLMï¼š
æˆ‘ç„¡æ³•è¨ªå•ä½ å€‘çš„å…§éƒ¨æ–‡æª”...

RAG ç³»çµ±ï¼š
æ ¹æ“šã€ŠæŠ€è¡“è¦ç¯„ v3.2ã€‹ï¼Œè¦ç¯„å¦‚ä¸‹...
ï¼ˆå¯è™•ç†å…¬å¸å…§éƒ¨æ–‡æª”ï¼‰
```

### Linux é¡æ¯”

```
ç´” LLM = åªæœ‰ RAM çš„é›»è…¦
- åªèƒ½è¨˜ä½è¨“ç·´æ™‚å­¸åˆ°çš„æ±è¥¿
- ç„¡æ³•æŸ¥é–±å¤–éƒ¨è³‡æ–™

RAG = RAM + ç¡¬ç¢Ÿ
- RAMï¼ˆLLMï¼‰ï¼šç†è§£èˆ‡ç”Ÿæˆèƒ½åŠ›
- ç¡¬ç¢Ÿï¼ˆå‘é‡è³‡æ–™åº«ï¼‰ï¼šå„²å­˜å¤§é‡æ–‡æª”
- éœ€è¦è³‡æ–™æ™‚ï¼Œå¾ç¡¬ç¢Ÿæª¢ç´¢åˆ° RAM

å°±åƒï¼š
man command  â†’  å¾æ–‡æª”ä¸­æª¢ç´¢è³‡è¨Šï¼ˆRAGï¼‰
ç›´æ¥å›ç­”      â†’  å¾è¨˜æ†¶ä¸­å›ç­”ï¼ˆç´” LLMï¼‰
```

---

## ğŸ—ï¸ RAG ç³»çµ±æ¶æ§‹é€Ÿè¦½

### å››å¤§æ ¸å¿ƒçµ„ä»¶

```
1. æ–‡æª”è™•ç†å™¨ï¼ˆDocument Processorï¼‰
   â”œâ”€ æ–‡æª”è¼‰å…¥ï¼ˆLoaderï¼‰ï¼šPDF, DOCX, HTML, Markdown
   â”œâ”€ æ–‡æœ¬åˆ‡å¡Šï¼ˆChunkerï¼‰ï¼šåˆ‡æˆé©åˆæª¢ç´¢çš„å°æ®µè½
   â””â”€ å…ƒæ•¸æ“šæå–ï¼ˆMetadataï¼‰ï¼šæ¨™é¡Œã€ä½œè€…ã€æ—¥æœŸç­‰

2. åµŒå…¥æ¨¡å‹ï¼ˆEmbedding Modelï¼‰
   â”œâ”€ æ–‡æœ¬å‘é‡åŒ–ï¼šå°‡æ–‡å­—è½‰æ›æˆæ•¸å­—å‘é‡
   â”œâ”€ èªç¾©è¡¨ç¤ºï¼šç›¸ä¼¼çš„æ–‡æœ¬åœ¨å‘é‡ç©ºé–“ä¸­è·é›¢è¿‘
   â””â”€ å¸¸ç”¨æ¨¡å‹ï¼šOpenAI ada-002, Sentence Transformers

3. å‘é‡è³‡æ–™åº«ï¼ˆVector Databaseï¼‰
   â”œâ”€ å‘é‡å„²å­˜ï¼šé«˜æ•ˆå„²å­˜æ•¸ç™¾è¬å€‹å‘é‡
   â”œâ”€ ç›¸ä¼¼åº¦æœå°‹ï¼šå¿«é€Ÿæ‰¾åˆ°æœ€ç›¸é—œçš„æ–‡æª”
   â””â”€ å¸¸ç”¨å·¥å…·ï¼šChroma, Pinecone, Weaviate, Qdrant

4. ç”Ÿæˆæ¨¡å‹ï¼ˆLLMï¼‰
   â”œâ”€ æ¥æ”¶ï¼šç”¨æˆ¶å•é¡Œ + æª¢ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
   â”œâ”€ ç”Ÿæˆï¼šåŸºæ–¼ä¸Šä¸‹æ–‡çš„ç­”æ¡ˆ
   â””â”€ å¸¸ç”¨æ¨¡å‹ï¼šGPT-4, Claude, Gemini
```

### è³‡æ–™æµåœ–

```
é›¢ç·šæµç¨‹ï¼ˆå»ºç«‹ç´¢å¼•ï¼‰ï¼š
æ–‡æª” â†’ åˆ‡å¡Š â†’ åµŒå…¥ â†’ å„²å­˜åˆ°å‘é‡è³‡æ–™åº«

ç·šä¸Šæµç¨‹ï¼ˆå•ç­”ï¼‰ï¼š
å•é¡Œ â†’ åµŒå…¥ â†’ æª¢ç´¢ Top-K â†’ æ³¨å…¥ Prompt â†’ LLM ç”Ÿæˆ â†’ ç­”æ¡ˆ
```

---

## ğŸš€ AI è¼”åŠ© RAG ç³»çµ±æ­å»º

### 10 åˆ†é˜å¿«é€Ÿæ­å»ºæµç¨‹

**æ™‚é–“åˆ†é…**ï¼š
```
åˆ†é˜ 1-3ï¼šç’°å¢ƒè¨­ç½®èˆ‡ä¾è³´å®‰è£
åˆ†é˜ 4-5ï¼šæ–‡æª”è¼‰å…¥èˆ‡åˆ‡å¡Š
åˆ†é˜ 6-7ï¼šå»ºç«‹å‘é‡è³‡æ–™åº«
åˆ†é˜ 8-9ï¼šå¯¦ç¾å•ç­”åŠŸèƒ½
åˆ†é˜ 10ï¼šæ¸¬è©¦èˆ‡é©—è­‰
```

### èˆ‡ AI å°è©±ç¯„ä¾‹

```
ä½ ï¼šæˆ‘æƒ³å»ºç«‹ä¸€å€‹ RAG ç³»çµ±ï¼Œç”¨ä¾†å›ç­”æŠ€è¡“æ–‡æª”çš„å•é¡Œ

Claude Codeï¼š
æˆ‘æœƒå¹«ä½ å¿«é€Ÿæ­å»ºä¸€å€‹åŸå‹ï¼š

1. é¸æ“‡å·¥å…·çµ„åˆï¼š
   - æ¡†æ¶ï¼šLangChainï¼ˆæœ€æˆç†Ÿï¼‰
   - å‘é‡è³‡æ–™åº«ï¼šChromaï¼ˆæœ¬åœ°é–‹ç™¼å‹å–„ï¼‰
   - åµŒå…¥ï¼šOpenAI ada-002ï¼ˆæ•ˆæœå¥½ï¼‰
   - LLMï¼šGPT-4ï¼ˆç”Ÿæˆå“è³ªé«˜ï¼‰

2. ç”Ÿæˆå®Œæ•´ä»£ç¢¼ï¼š
   - æ–‡æª”è¼‰å…¥å™¨
   - åˆ‡å¡Šç­–ç•¥
   - å‘é‡åŒ–èˆ‡å„²å­˜
   - å•ç­”æ¥å£

3. æä¾›æ¸¬è©¦æ¡ˆä¾‹
```

---

## ğŸ¯ å®Œæ•´ç¯„ä¾‹ï¼šæŠ€è¡“æ–‡æª” RAG ç³»çµ±

### æƒ…å¢ƒæè¿°

**éœ€æ±‚**ï¼š
- å…¬å¸æœ‰å¤§é‡æŠ€è¡“æ–‡æª”ï¼ˆMarkdown, PDFï¼‰
- æ–°äººç¶“å¸¸å•ç›¸åŒçš„å•é¡Œ
- éœ€è¦ä¸€å€‹è‡ªå‹•å•ç­”ç³»çµ±

### å®Œæ•´å¯¦ç¾ä»£ç¢¼

**æ­¥é©Ÿ 1ï¼šå®‰è£ä¾è³´**

```bash
# å®‰è£å¿…è¦å¥—ä»¶
pip install langchain openai chromadb tiktoken

# æˆ–ä½¿ç”¨ poetryï¼ˆæ¨è–¦ï¼‰
poetry add langchain openai chromadb tiktoken
```

**æ­¥é©Ÿ 2ï¼šå®Œæ•´ RAG ç³»çµ±ä»£ç¢¼**

**`rag_system.py`**ï¼š
```python
#!/usr/bin/env python3
"""
Technical Documentation RAG System

A simple RAG system for answering questions about technical documentation.
"""

import os
from pathlib import Path
from typing import List, Dict, Any

from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ===== Configuration =====
DOCS_DIR = "docs"  # æ–‡æª”ç›®éŒ„
CHROMA_DIR = "chroma_db"  # å‘é‡è³‡æ–™åº«ç›®éŒ„
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ===== 1. Document Loading =====
def load_documents(docs_path: str) -> List[Any]:
    """
    Load all markdown and text files from directory

    Args:
        docs_path: Path to documents directory

    Returns:
        List of loaded documents
    """
    print(f"ğŸ“‚ Loading documents from {docs_path}...")

    loader = DirectoryLoader(
        docs_path,
        glob="**/*.md",  # è¼‰å…¥æ‰€æœ‰ .md æª”æ¡ˆ
        loader_cls=TextLoader,
        show_progress=True
    )

    documents = loader.load()
    print(f"âœ… Loaded {len(documents)} documents")

    return documents


# ===== 2. Text Chunking =====
def chunk_documents(documents: List[Any], chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Any]:
    """
    Split documents into smaller chunks

    Args:
        documents: List of documents
        chunk_size: Size of each chunk
        chunk_overlap: Overlap between chunks

    Returns:
        List of chunked documents
    """
    print(f"âœ‚ï¸  Chunking documents (size={chunk_size}, overlap={chunk_overlap})...")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )

    chunks = text_splitter.split_documents(documents)
    print(f"âœ… Created {len(chunks)} chunks")

    return chunks


# ===== 3. Create Vector Store =====
def create_vector_store(chunks: List[Any], persist_directory: str) -> Chroma:
    """
    Create and persist vector store

    Args:
        chunks: List of document chunks
        persist_directory: Directory to save vector store

    Returns:
        Chroma vector store
    """
    print(f"ğŸ”¢ Creating vector embeddings...")

    embeddings = OpenAIEmbeddings()

    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=persist_directory
    )

    vectorstore.persist()
    print(f"âœ… Vector store created and saved to {persist_directory}")

    return vectorstore


# ===== 4. Load Existing Vector Store =====
def load_vector_store(persist_directory: str) -> Chroma:
    """
    Load existing vector store

    Args:
        persist_directory: Directory of saved vector store

    Returns:
        Chroma vector store
    """
    print(f"ğŸ“¥ Loading vector store from {persist_directory}...")

    embeddings = OpenAIEmbeddings()

    vectorstore = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings
    )

    print(f"âœ… Vector store loaded")
    return vectorstore


# ===== 5. Create QA Chain =====
def create_qa_chain(vectorstore: Chroma) -> RetrievalQA:
    """
    Create question-answering chain

    Args:
        vectorstore: Vector store for retrieval

    Returns:
        RetrievalQA chain
    """
    print(f"ğŸ”— Creating QA chain...")

    # Custom prompt template
    prompt_template = """You are a helpful technical documentation assistant.
Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.

Context:
{context}

Question: {question}

Answer in Traditional Chinese with technical accuracy:"""

    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )

    llm = ChatOpenAI(
        model_name="gpt-4",
        temperature=0  # é™ä½éš¨æ©Ÿæ€§ï¼Œæé«˜æº–ç¢ºæ€§
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}  # æª¢ç´¢å‰ 3 å€‹æœ€ç›¸é—œçš„æ–‡æª”
        ),
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )

    print(f"âœ… QA chain created")
    return qa_chain


# ===== 6. Ask Question =====
def ask_question(qa_chain: RetrievalQA, question: str) -> Dict[str, Any]:
    """
    Ask a question and get answer with sources

    Args:
        qa_chain: QA chain
        question: User question

    Returns:
        Dictionary with answer and source documents
    """
    print(f"\nâ“ Question: {question}")
    print("ğŸ¤” Thinking...")

    result = qa_chain({"query": question})

    print(f"\nğŸ’¬ Answer:\n{result['result']}")
    print(f"\nğŸ“š Sources:")
    for i, doc in enumerate(result['source_documents'], 1):
        print(f"  {i}. {doc.metadata.get('source', 'Unknown')}")

    return result


# ===== Main Functions =====
def build_knowledge_base(docs_path: str, persist_directory: str) -> None:
    """
    Build knowledge base from documents

    Args:
        docs_path: Path to documents
        persist_directory: Where to save vector store
    """
    print("=" * 50)
    print("Building Knowledge Base")
    print("=" * 50)

    # Load documents
    documents = load_documents(docs_path)

    # Chunk documents
    chunks = chunk_documents(documents)

    # Create vector store
    create_vector_store(chunks, persist_directory)

    print("\nâœ… Knowledge base built successfully!")


def run_qa_system(persist_directory: str) -> None:
    """
    Run interactive QA system

    Args:
        persist_directory: Path to vector store
    """
    print("=" * 50)
    print("Technical Documentation QA System")
    print("=" * 50)

    # Load vector store
    vectorstore = load_vector_store(persist_directory)

    # Create QA chain
    qa_chain = create_qa_chain(vectorstore)

    print("\nğŸ’¡ Ready! Type your questions (or 'quit' to exit)\n")

    while True:
        question = input("Your question: ").strip()

        if question.lower() in ['quit', 'exit', 'q']:
            print("ğŸ‘‹ Goodbye!")
            break

        if not question:
            continue

        ask_question(qa_chain, question)
        print("\n" + "-" * 50 + "\n")


# ===== CLI =====
if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='RAG System for Technical Documentation')
    parser.add_argument('--build', action='store_true', help='Build knowledge base')
    parser.add_argument('--docs', default=DOCS_DIR, help='Documents directory')
    parser.add_argument('--db', default=CHROMA_DIR, help='Vector store directory')

    args = parser.parse_args()

    if args.build:
        build_knowledge_base(args.docs, args.db)
    else:
        run_qa_system(args.db)
```

### ä½¿ç”¨æ–¹å¼

**æ­¥é©Ÿ 1ï¼šæº–å‚™æ–‡æª”**
```bash
# å»ºç«‹æ–‡æª”ç›®éŒ„
mkdir docs

# æ”¾å…¥ä½ çš„æ–‡æª”
cp /path/to/technical/*.md docs/
```

**æ­¥é©Ÿ 2ï¼šå»ºç«‹çŸ¥è­˜åº«ï¼ˆä¸€æ¬¡æ€§ï¼‰**
```bash
# è¨­å®š OpenAI API Key
export OPENAI_API_KEY="sk-..."

# å»ºç«‹çŸ¥è­˜åº«
python rag_system.py --build --docs docs --db chroma_db
```

è¼¸å‡ºï¼š
```
==================================================
Building Knowledge Base
==================================================
ğŸ“‚ Loading documents from docs...
âœ… Loaded 25 documents
âœ‚ï¸  Chunking documents (size=1000, overlap=200)...
âœ… Created 342 chunks
ğŸ”¢ Creating vector embeddings...
âœ… Vector store created and saved to chroma_db

âœ… Knowledge base built successfully!
```

**æ­¥é©Ÿ 3ï¼šé‹è¡Œå•ç­”ç³»çµ±**
```bash
python rag_system.py --db chroma_db
```

**äº’å‹•ç¯„ä¾‹**ï¼š
```
==================================================
Technical Documentation QA System
==================================================
ğŸ“¥ Loading vector store from chroma_db...
âœ… Vector store loaded
ğŸ”— Creating QA chain...
âœ… QA chain created

ğŸ’¡ Ready! Type your questions (or 'quit' to exit)

Your question: å¦‚ä½•å®‰è£é€™å€‹ç³»çµ±ï¼Ÿ

â“ Question: å¦‚ä½•å®‰è£é€™å€‹ç³»çµ±ï¼Ÿ
ğŸ¤” Thinking...

ğŸ’¬ Answer:
æ ¹æ“šæŠ€è¡“æ–‡æª”ï¼Œå®‰è£æ­¥é©Ÿå¦‚ä¸‹ï¼š

1. ç¢ºä¿ç³»çµ±éœ€æ±‚ï¼š
   - Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
   - pip æˆ– poetry å¥—ä»¶ç®¡ç†å·¥å…·

2. å®‰è£ä¾è³´ï¼š
   ```bash
   pip install -r requirements.txt
   ```

3. è¨­å®šç’°å¢ƒè®Šæ•¸ï¼š
   ```bash
   export DATABASE_URL="postgresql://..."
   ```

4. åŸ·è¡Œè³‡æ–™åº«é·ç§»ï¼š
   ```bash
   python manage.py migrate
   ```

5. å•Ÿå‹•æœå‹™ï¼š
   ```bash
   python manage.py runserver
   ```

ğŸ“š Sources:
  1. docs/installation.md
  2. docs/quickstart.md
  3. docs/configuration.md

--------------------------------------------------
```

---

## ğŸ”§ å¸¸ç”¨ RAG å·¥å…·æ¦‚è¦½

### æ¡†æ¶é¸æ“‡

```
å·¥å…·          å„ªå‹¢                    åŠ£å‹¢                 æ¨è–¦å ´æ™¯
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LangChain    â­â­â­â­â­              å­¸ç¿’æ›²ç·šé™¡å³­          å¿«é€ŸåŸå‹
             ç”Ÿæ…‹å®Œæ•´                                    ä¼æ¥­æ‡‰ç”¨
             æ–‡æª”è±å¯Œ

LlamaIndex   â­â­â­â­              ç”Ÿæ…‹è¼ƒå°              è³‡æ–™æ•´åˆ
             å°ˆæ³¨è³‡æ–™æ•´åˆ            ç¤¾ç¾¤è¼ƒå°              è¤‡é›œæª¢ç´¢
             æª¢ç´¢ç­–ç•¥è±å¯Œ

Haystack     â­â­â­                ä¼æ¥­åŠŸèƒ½å¤š            éœ€æ±‚æ˜ç¢ºçš„
             å¾·åœ‹è£½é€                 é…ç½®è¤‡é›œ              ä¼æ¥­å°ˆæ¡ˆ
             ä¼æ¥­ç´š

è‡ªå·±å¯¦ç¾      â­â­                  éœ€è¦æ™‚é–“              å­¸ç¿’ç›®çš„
             å®Œå…¨æ§åˆ¶                éœ€è¦ç¶­è­·              ç‰¹æ®Šéœ€æ±‚
             ç„¡ä¾è³´
```

### å‘é‡è³‡æ–™åº«é¸æ“‡

```
è³‡æ–™åº«        é¡å‹      å„ªå‹¢                    é©ç”¨å ´æ™¯
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Chroma       æœ¬åœ°      å…è²»ã€æ˜“ç”¨              é–‹ç™¼æ¸¬è©¦
                      Python å‹å–„             å°å‹å°ˆæ¡ˆ

Pinecone     é›²ç«¯      å…¨è¨—ç®¡                  ç”Ÿç”¢ç’°å¢ƒ
                      æ•ˆèƒ½å„ªç•°                å¤§è¦æ¨¡æ‡‰ç”¨
                      æŒ‰ä½¿ç”¨è¨ˆè²»              ä¸æƒ³ç¶­é‹

Weaviate     é–‹æº      åŠŸèƒ½è±å¯Œ                ä¼æ¥­è‡ªå»º
                      å¯è‡ªæ¶                  éœ€è¦æ§åˆ¶
                      GraphQL API             è¤‡é›œæŸ¥è©¢

Qdrant       é–‹æº      é«˜æ•ˆèƒ½                  å¤§è¦æ¨¡
                      Rust ç·¨å¯«               æ•ˆèƒ½è¦æ±‚é«˜
                      å¯è‡ªæ¶                  å°ˆæ¥­åœ˜éšŠ

Milvus       é–‹æº      è¶…å¤§è¦æ¨¡                æ•¸ç™¾è¬ç´šåˆ¥
                      åˆ†æ•£å¼                  ä¼æ¥­ç´šæ‡‰ç”¨
                      ç”Ÿç”¢ç´š                  è³‡æ–™ç§‘å­¸åœ˜éšŠ
```

### åµŒå…¥æ¨¡å‹é¸æ“‡

```
æ¨¡å‹                    ç¶­åº¦    æ•ˆæœ    æˆæœ¬    æ¨è–¦
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OpenAI ada-002         1536    â­â­â­â­  $     å¿«é€Ÿé–‹ç™¼
OpenAI text-3-small    1536    â­â­â­â­â­ $     æœ€æ–°æ¨è–¦
OpenAI text-3-large    3072    â­â­â­â­â­ $$    é«˜å“è³ª

Sentence Transformers  384-768 â­â­â­   å…è²»   æœ¬åœ°éƒ¨ç½²
(é–‹æº)                                       é ç®—æœ‰é™

Cohere Embed           1024    â­â­â­â­  $     å¤šèªè¨€æ”¯æ´

```

---

## ğŸ¯ 10 åˆ†é˜å¿«é€Ÿä¸Šæ‰‹æŒ‡å—

### æœ€å°å¯è¡ŒåŸå‹

**ç›®æ¨™**ï¼š10 åˆ†é˜å…§é‹è¡Œç¬¬ä¸€å€‹ RAG ç³»çµ±

**`minimal_rag.py`**ï¼ˆç°¡åŒ–ç‰ˆï¼‰ï¼š
```python
"""
Minimal RAG System - 10 minutes to deploy
"""

from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

# 1. Load document (1 minute)
loader = TextLoader("your_document.txt")
documents = loader.load()

# 2. Split into chunks (1 minute)
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# 3. Create embeddings and vector store (3 minutes)
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(texts, embeddings)

# 4. Create QA chain (2 minutes)
qa = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(),
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# 5. Ask questions (3 minutes - testing)
result = qa.run("What is this document about?")
print(result)
```

**åŸ·è¡Œ**ï¼š
```bash
# å®‰è£ï¼ˆ2 åˆ†é˜ï¼‰
pip install langchain openai chromadb

# åŸ·è¡Œï¼ˆ8 åˆ†é˜ï¼‰
export OPENAI_API_KEY="sk-..."
python minimal_rag.py
```

---

## âš ï¸ å¸¸è¦‹é™·é˜±èˆ‡è§£æ±ºæ–¹æ¡ˆ

### é™·é˜± 1ï¼šåˆ‡å¡Šç­–ç•¥ä¸ç•¶

**å•é¡Œ**ï¼š
```python
# éŒ¯èª¤ï¼šåˆ‡å¡Šå¤ªå¤§æˆ–å¤ªå°
text_splitter = CharacterTextSplitter(chunk_size=5000)  # å¤ªå¤§ï¼Œä¸Šä¸‹æ–‡ä¸ç²¾ç¢º
text_splitter = CharacterTextSplitter(chunk_size=100)   # å¤ªå°ï¼Œèªç¾©ä¸å®Œæ•´
```

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# æ­£ç¢ºï¼šæ ¹æ“šå…§å®¹é¡å‹é¸æ“‡
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # æŠ€è¡“æ–‡æª”æ¨è–¦ 500-1500
    chunk_overlap=200,    # 20% overlap ä¿ç•™ä¸Šä¸‹æ–‡
    separators=["\n\n", "\n", " ", ""]  # æŒ‰æ®µè½ã€å¥å­ã€å–®è©åˆ‡åˆ†
)
```

### é™·é˜± 2ï¼šæª¢ç´¢ä¸åˆ°ç›¸é—œæ–‡æª”

**å•é¡Œ**ï¼š
```python
# éŒ¯èª¤ï¼šåªæª¢ç´¢ 1 å€‹æ–‡æª”
retriever = vectorstore.as_retriever(search_kwargs={"k": 1})
```

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# æ­£ç¢ºï¼šæª¢ç´¢ 3-5 å€‹ï¼Œå¢åŠ è¦†è“‹ç‡
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}  # æˆ–æ ¹æ“šéœ€æ±‚èª¿æ•´
)

# é€²éšï¼šä½¿ç”¨ MMRï¼ˆæœ€å¤§é‚Šéš›ç›¸é—œæ€§ï¼‰é¿å…é‡è¤‡
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 3, "fetch_k": 10}
)
```

### é™·é˜± 3ï¼šæˆæœ¬å¤±æ§

**å•é¡Œ**ï¼š
```python
# éŒ¯èª¤ï¼šæ¯æ¬¡é‡æ–°åµŒå…¥æ‰€æœ‰æ–‡æª”
for question in questions:
    vectorstore = Chroma.from_documents(documents, embeddings)  # é‡è¤‡åµŒå…¥ï¼
```

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# æ­£ç¢ºï¼šåµŒå…¥ä¸€æ¬¡ï¼ŒæŒä¹…åŒ–
vectorstore = Chroma.from_documents(
    documents,
    embeddings,
    persist_directory="chroma_db"  # å„²å­˜åˆ°ç¡¬ç¢Ÿ
)

# å¾ŒçºŒä½¿ç”¨ï¼šç›´æ¥è¼‰å…¥
vectorstore = Chroma(
    persist_directory="chroma_db",
    embedding_function=embeddings
)
```

### é™·é˜± 4ï¼šç­”æ¡ˆå“è³ªä¸ä½³

**å•é¡Œ**ï¼šLLM ç”Ÿæˆçš„ç­”æ¡ˆåé›¢æ–‡æª”å…§å®¹

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# è‡ªè¨‚ Promptï¼Œå¼·èª¿ã€ŒåŸºæ–¼æ–‡æª”ã€
prompt_template = """You must answer based ONLY on the provided context.
If the context doesn't contain the answer, say "I don't have enough information."
Do NOT make up information.

Context:
{context}

Question: {question}

Answer:"""
```

---

## ğŸ“Š æ•ˆèƒ½å„ªåŒ–å»ºè­°

### å„ªåŒ–æŠ€å·§

**1. æ‰¹æ¬¡åµŒå…¥**
```python
# ä¸è¦ï¼šé€å€‹åµŒå…¥
for doc in documents:
    embedding = embeddings.embed_query(doc)

# è¦ï¼šæ‰¹æ¬¡åµŒå…¥
embeddings.embed_documents([doc.page_content for doc in documents])
```

**2. å¿«å–æª¢ç´¢çµæœ**
```python
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_retrieve(query: str):
    return vectorstore.similarity_search(query, k=3)
```

**3. ä½¿ç”¨æ›´å¿«çš„åµŒå…¥æ¨¡å‹**
```python
# ç”Ÿç”¢ç’°å¢ƒï¼šè‡ªæ¶ Sentence Transformers
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
# é€Ÿåº¦å¿« 10 å€ï¼Œå…è²»ï¼Œé©åˆå¤§è¦æ¨¡
```

---

## ğŸ¯ å¯¦æˆ°ç·´ç¿’

### ç·´ç¿’ 1ï¼šæ­å»ºä½ çš„ç¬¬ä¸€å€‹ RAG ç³»çµ±

**ä»»å‹™**ï¼š
```
1. æ”¶é›† 5-10 å€‹ Markdown æ–‡æª”ï¼ˆæˆ–ç”¨ç¯„ä¾‹æ–‡æª”ï¼‰
2. ä½¿ç”¨ä¸Šé¢çš„ä»£ç¢¼æ­å»º RAG ç³»çµ±
3. æ¸¬è©¦ 10 å€‹å•é¡Œ
4. è©•ä¼°ç­”æ¡ˆå“è³ª
```

### ç·´ç¿’ 2ï¼šå„ªåŒ–æª¢ç´¢ç­–ç•¥

**ä»»å‹™**ï¼š
```
1. èª¿æ•´ chunk_sizeï¼ˆè©¦è©¦ 500, 1000, 1500ï¼‰
2. èª¿æ•´ kï¼ˆæª¢ç´¢æ•¸é‡ï¼š1, 3, 5, 10ï¼‰
3. æ¯”è¼ƒç­”æ¡ˆå“è³ª
4. è¨˜éŒ„æœ€ä½³çµ„åˆ
```

---

## ğŸ“š å»¶ä¼¸é–±è®€

**æ¡†æ¶æ–‡æª”**ï¼š
- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)
- [LlamaIndex Docs](https://docs.llamaindex.ai/)

**å‘é‡è³‡æ–™åº«**ï¼š
- [Chroma Docs](https://docs.trychroma.com/)
- [Pinecone Guides](https://docs.pinecone.io/)
- [Weaviate Docs](https://weaviate.io/developers/weaviate)

**é€²éšæŠ€å·§**ï¼š
- [Advanced RAG Techniques](https://blog.langchain.dev/advanced-rag/)
- [RAG Evaluation](https://www.confident-ai.com/blog/rag-evaluation)

---

## ğŸ¯ å­¸ç¿’æª¢æŸ¥é»

å®Œæˆæœ¬ç« å¾Œï¼Œä½ æ‡‰è©²èƒ½å¤ ï¼š

- [ ] èªªæ˜ RAG çš„æ ¸å¿ƒæ¦‚å¿µèˆ‡æ¶æ§‹
- [ ] 10 åˆ†é˜æ­å»ºæœ€å°å¯è¡Œ RAG åŸå‹
- [ ] é¸æ“‡åˆé©çš„ RAG å·¥å…·èˆ‡å‘é‡è³‡æ–™åº«
- [ ] ç†è§£å¸¸è¦‹é™·é˜±èˆ‡å„ªåŒ–æŠ€å·§
- [ ] è©•ä¼° RAG ç³»çµ±çš„ç­”æ¡ˆå“è³ª

---

**ç« ç¯€ç‰ˆæœ¬**ï¼šv1.0
**æœ€å¾Œæ›´æ–°**ï¼š2025-10-30
**é è¨ˆå­¸ç¿’æ™‚é•·**ï¼š15 åˆ†é˜ï¼ˆç†è«–ï¼‰+ 10 åˆ†é˜ï¼ˆå¯¦ä½œï¼‰
