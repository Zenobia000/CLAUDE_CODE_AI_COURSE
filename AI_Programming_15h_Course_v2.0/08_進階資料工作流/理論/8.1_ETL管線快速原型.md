# 8.1 ETL ç®¡ç·šå¿«é€ŸåŸå‹ - ç”¨ AI åŠ é€Ÿè³‡æ–™è™•ç†

## ğŸ“‹ ç« ç¯€æ¦‚è¿°

**å­¸ç¿’æ™‚é•·**ï¼š15 åˆ†é˜
**é›£åº¦**ï¼šâ­â­ åŸºç¤è‡³ä¸­ç´š
**å‰ç½®çŸ¥è­˜**ï¼šPython åŸºç¤ã€CSV è³‡æ–™è™•ç†æ¦‚å¿µ

æœ¬ç« æ•™ä½ ï¼š
- âœ… ç†è§£ ETL çš„æ ¸å¿ƒæ¦‚å¿µï¼ˆExtract-Transform-Loadï¼‰
- âœ… ç”¨ AI å¿«é€Ÿç”Ÿæˆ ETL ç®¡ç·šä»£ç¢¼
- âœ… å¯¦æˆ°ï¼šCSV to Database å®Œæ•´æµç¨‹
- âœ… çŸ¥é“ä½•æ™‚éœ€è¦å°ˆæ¥­ ETL å·¥å…·

---

## ğŸ¯ ä»€éº¼æ˜¯ ETLï¼Ÿ

### åŸºæœ¬å®šç¾©

**ETL** = Extract + Transform + Load

```
è³‡æ–™æ—…ç¨‹ï¼š
åŸå§‹è³‡æ–™ â†’ æå– â†’ è½‰æ› â†’ è¼‰å…¥ â†’ å¯ç”¨è³‡æ–™

å°±åƒé£Ÿç‰©è™•ç†ï¼š
é£Ÿæ â†’ æ¸…æ´— â†’ åˆ‡å¡Šèª¿å‘³ â†’ è£ç›¤ â†’ ç¾é£Ÿ
```

### ä¸‰å€‹éšæ®µè©³è§£

#### 1. Extractï¼ˆæå–ï¼‰
```python
# å¾å„ç¨®ä¾†æºæå–è³‡æ–™
sources = {
    'CSVæª”æ¡ˆ': 'data/sales.csv',
    'è³‡æ–™åº«': 'postgresql://...',
    'API': 'https://api.example.com/data',
    'Excel': 'reports/Q4.xlsx'
}
```

#### 2. Transformï¼ˆè½‰æ›ï¼‰
```python
# å¸¸è¦‹è½‰æ›æ“ä½œ
transformations = [
    'è³‡æ–™æ¸…æ´—ï¼ˆç§»é™¤ç©ºå€¼ã€ç•°å¸¸å€¼ï¼‰',
    'æ ¼å¼è½‰æ›ï¼ˆæ—¥æœŸã€æ•¸å­—æ ¼å¼ï¼‰',
    'è³‡æ–™åˆä½µï¼ˆå¤šå€‹ä¾†æºæ•´åˆï¼‰',
    'æ¬„ä½è¨ˆç®—ï¼ˆæ–°å¢è¡ç”Ÿæ¬„ä½ï¼‰',
    'è³‡æ–™é©—è­‰ï¼ˆç¢ºä¿å“è³ªï¼‰'
]
```

#### 3. Loadï¼ˆè¼‰å…¥ï¼‰
```python
# è¼‰å…¥åˆ°ç›®æ¨™ç³»çµ±
targets = {
    'é—œè¯å¼è³‡æ–™åº«': 'PostgreSQL, MySQL',
    'è³‡æ–™å€‰å„²': 'Snowflake, BigQuery',
    'æª”æ¡ˆç³»çµ±': 'Parquet, JSON',
    'å³æ™‚ä¸²æµ': 'Kafka, RabbitMQ'
}
```

---

## ğŸ¤” ç‚ºä»€éº¼éœ€è¦ ETLï¼Ÿ

### è§£æ±ºçš„å•é¡Œ

```
å•é¡Œå ´æ™¯ 1ï¼šè³‡æ–™åˆ†æ•£åœ¨å¤šå€‹ç³»çµ±
â”œâ”€ éŠ·å”®è³‡æ–™åœ¨ CRM
â”œâ”€ åº«å­˜è³‡æ–™åœ¨ ERP
â”œâ”€ å®¢æˆ¶è³‡æ–™åœ¨ MySQL
â””â”€ éœ€è¦æ•´åˆåˆ†æ â†’ ETL ç®¡ç·šæ•´åˆ

å•é¡Œå ´æ™¯ 2ï¼šè³‡æ–™å“è³ªä¸ä½³
â”œâ”€ ç©ºå€¼ã€é‡è¤‡å€¼
â”œâ”€ æ ¼å¼ä¸ä¸€è‡´
â”œâ”€ ç•°å¸¸å€¼
â””â”€ éœ€è¦æ¸…æ´—è½‰æ› â†’ ETL ç®¡ç·šæ¸…æ´—

å•é¡Œå ´æ™¯ 3ï¼šéœ€è¦å®šæ™‚æ›´æ–°
â”œâ”€ æ¯å¤©æ›´æ–°å ±è¡¨
â”œâ”€ æ¯å°æ™‚åŒæ­¥è³‡æ–™
â”œâ”€ å³æ™‚è³‡æ–™è™•ç†
â””â”€ éœ€è¦è‡ªå‹•åŒ– â†’ ETL ç®¡ç·šæ’ç¨‹
```

### Linux é¡æ¯”

```
ETL å°±åƒ Linux çš„ pipeï¼ˆç®¡é“ï¼‰ï¼š

cat raw_data.txt | grep "error" | sort | uniq > clean_data.txt
â”‚                  â”‚              â”‚      â”‚      â”‚
æå–               éæ¿¾          æ’åº   å»é‡   è¼‰å…¥

ETL:
Extract raw_data â†’ Transform (filter, sort) â†’ Load clean_data
```

---

## ğŸ†š å‚³çµ± ETL å·¥å…· vs AI è¼”åŠ© ETL

### å·¥å…·å°æ¯”

```
å ´æ™¯              å‚³çµ±å·¥å…·              AI è¼”åŠ©æ–¹å¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ç°¡å–® CSV è™•ç†    æ‰‹å¯« Pandas ä»£ç¢¼      AI ç”Ÿæˆè…³æœ¬ï¼ˆ5 åˆ†é˜ï¼‰
                (30 åˆ†é˜)            å¿«é€Ÿã€æº–ç¢º

è¤‡é›œè³‡æ–™ç®¡ç·š     Airflow/Prefect      AI è¨­è¨ˆæµç¨‹åœ–
                (æ•¸å¤©å­¸ç¿’èˆ‡é–‹ç™¼)      å‚³çµ±å·¥å…·åŸ·è¡Œ

ä¼æ¥­ç´š ETL      å°ˆæ¥­ ETL å·¥å…·         AI è¼”åŠ©é…ç½®
                (Talend, Informatica) åŠ é€Ÿé–‹ç™¼

å³æ™‚ä¸²æµ         Kafka + Flink        AI ç”Ÿæˆè™•ç†é‚è¼¯
                (å°ˆæ¥­çŸ¥è­˜)           é™ä½é–€æª»
```

### AI è¼”åŠ©çš„å„ªå‹¢

**å„ªå‹¢ 1ï¼šå¿«é€ŸåŸå‹**
```
å‚³çµ±æ–¹å¼ï¼š
1. ç ”ç©¶ Pandas API æ–‡æª”ï¼ˆ30 åˆ†é˜ï¼‰
2. å¯«è³‡æ–™æ¸…æ´—ä»£ç¢¼ï¼ˆ1 å°æ™‚ï¼‰
3. é™¤éŒ¯ä¿®æ­£ï¼ˆ30 åˆ†é˜ï¼‰
ç¸½è¨ˆï¼š2 å°æ™‚

AI è¼”åŠ©ï¼š
1. æè¿°éœ€æ±‚çµ¦ Claude Codeï¼ˆ2 åˆ†é˜ï¼‰
2. AI ç”Ÿæˆå®Œæ•´ä»£ç¢¼ï¼ˆ3 åˆ†é˜ï¼‰
3. æ¸¬è©¦é©—è­‰ï¼ˆ5 åˆ†é˜ï¼‰
ç¸½è¨ˆï¼š10 åˆ†é˜
```

**å„ªå‹¢ 2ï¼šæœ€ä½³å¯¦è¸å…§å»º**
```python
# AI ç”Ÿæˆçš„ä»£ç¢¼è‡ªå‹•åŒ…å«ï¼š
- éŒ¯èª¤è™•ç†ï¼ˆtry-exceptï¼‰
- æ—¥èªŒè¨˜éŒ„ï¼ˆloggingï¼‰
- è³‡æ–™é©—è­‰ï¼ˆassert/validateï¼‰
- æ•ˆèƒ½å„ªåŒ–ï¼ˆchunk processingï¼‰
```

**å„ªå‹¢ 3ï¼šæ˜“æ–¼ç¶­è­·**
```python
# AI å¯ä»¥ï¼š
- è‡ªå‹•ç”Ÿæˆæ–‡æª”
- æ·»åŠ æ¸…æ™°çš„è¨»è§£
- éµå¾ªæœ€ä½³å¯¦è¸
- å–®å…ƒæ¸¬è©¦ç”Ÿæˆ
```

---

## ğŸš€ AI è¼”åŠ© ETL å·¥ä½œæµç¨‹

### å®Œæ•´æµç¨‹åœ–

```
1. éœ€æ±‚æè¿°
   â†“
   ã€Œæˆ‘éœ€è¦å°‡ CSV çš„éŠ·å”®è³‡æ–™æ¸…æ´—å¾Œè¼‰å…¥ PostgreSQLã€
   â†“
2. AI åˆ†æè³‡æ–™
   â†“
   Claude Code åˆ†æ CSV çµæ§‹ã€æ¬„ä½é¡å‹
   â†“
3. è¨­è¨ˆè½‰æ›é‚è¼¯
   â†“
   AI å»ºè­°ï¼šç§»é™¤ç©ºå€¼ã€æ ¼å¼è½‰æ›ã€è³‡æ–™é©—è­‰
   â†“
4. ç”Ÿæˆ ETL ä»£ç¢¼
   â†“
   å®Œæ•´çš„ Python è…³æœ¬ï¼ˆå«éŒ¯èª¤è™•ç†ï¼‰
   â†“
5. æ¸¬è©¦èˆ‡é©—è­‰
   â†“
   åŸ·è¡Œè…³æœ¬ï¼Œæª¢æŸ¥è³‡æ–™å“è³ª
   â†“
6. æ’ç¨‹è‡ªå‹•åŒ–
   â†“
   è¨­å®š Cron job æˆ– Airflow
```

### é—œéµæ­¥é©Ÿè©³è§£

#### æ­¥é©Ÿ 1ï¼šè³‡æ–™æºé€£æ¥ï¼ˆExtractï¼‰

**èˆ‡ AI å°è©±ç¯„ä¾‹**ï¼š
```
ä½ ï¼šæˆ‘æœ‰ä¸€å€‹ sales.csv æª”æ¡ˆï¼Œè«‹å¹«æˆ‘åˆ†æçµæ§‹

Claude Codeï¼š
1. è®€å– CSV
2. é¡¯ç¤ºå‰ 5 è¡Œ
3. æ¬„ä½é¡å‹åˆ†æ
4. è³‡æ–™å“è³ªå ±å‘Šï¼ˆç©ºå€¼ã€ç•°å¸¸å€¼ï¼‰
```

**AI ç”Ÿæˆçš„ä»£ç¢¼**ï¼š
```python
import pandas as pd
import logging

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def extract_csv(file_path: str) -> pd.DataFrame:
    """
    Extract data from CSV file with error handling

    Args:
        file_path: Path to CSV file

    Returns:
        DataFrame with extracted data
    """
    try:
        logger.info(f"Reading CSV from {file_path}")
        df = pd.read_csv(file_path)
        logger.info(f"Successfully loaded {len(df)} rows")

        # Data quality report
        logger.info(f"Columns: {list(df.columns)}")
        logger.info(f"Missing values:\n{df.isnull().sum()}")

        return df

    except FileNotFoundError:
        logger.error(f"File not found: {file_path}")
        raise
    except pd.errors.EmptyDataError:
        logger.error(f"Empty CSV file: {file_path}")
        raise
    except Exception as e:
        logger.error(f"Error reading CSV: {str(e)}")
        raise

# ä½¿ç”¨ç¯„ä¾‹
df = extract_csv('data/sales.csv')
```

#### æ­¥é©Ÿ 2ï¼šè³‡æ–™æ¸…æ´—è¦å‰‡ï¼ˆTransformï¼‰

**èˆ‡ AI å°è©±ç¯„ä¾‹**ï¼š
```
ä½ ï¼šé€™å€‹ CSV æœ‰ä»¥ä¸‹å•é¡Œï¼š
1. date æ¬„ä½æ ¼å¼ä¸ä¸€è‡´
2. amount æœ‰ç©ºå€¼
3. product_name æœ‰é‡è¤‡çš„ç©ºæ ¼
è«‹å¹«æˆ‘è¨­è¨ˆæ¸…æ´—é‚è¼¯

Claude Codeï¼š
æˆ‘æœƒç”Ÿæˆæ¸…æ´—å‡½æ•¸ï¼ŒåŒ…å«ï¼š
- æ—¥æœŸæ ¼å¼æ¨™æº–åŒ–
- ç©ºå€¼è™•ç†ç­–ç•¥
- å­—ä¸²æ¸…ç†
- è³‡æ–™é©—è­‰
```

**AI ç”Ÿæˆçš„æ¸…æ´—ä»£ç¢¼**ï¼š
```python
from datetime import datetime
import re

def transform_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Transform and clean sales data

    Args:
        df: Raw DataFrame

    Returns:
        Cleaned DataFrame
    """
    logger.info("Starting data transformation")

    df_clean = df.copy()

    # 1. æ¨™æº–åŒ–æ—¥æœŸæ ¼å¼
    def parse_date(date_str):
        """Parse various date formats"""
        formats = ['%Y-%m-%d', '%d/%m/%Y', '%m-%d-%Y']
        for fmt in formats:
            try:
                return pd.to_datetime(date_str, format=fmt)
            except:
                continue
        return pd.NaT

    df_clean['date'] = df_clean['date'].apply(parse_date)
    logger.info(f"Parsed {df_clean['date'].notna().sum()} dates")

    # 2. è™•ç† amount ç©ºå€¼ï¼ˆç”¨ä¸­ä½æ•¸å¡«å……ï¼‰
    median_amount = df_clean['amount'].median()
    df_clean['amount'].fillna(median_amount, inplace=True)
    logger.info(f"Filled missing amounts with median: {median_amount}")

    # 3. æ¸…ç† product_nameï¼ˆç§»é™¤å¤šé¤˜ç©ºæ ¼ï¼‰
    df_clean['product_name'] = df_clean['product_name'].str.strip()
    df_clean['product_name'] = df_clean['product_name'].str.replace(r'\s+', ' ', regex=True)

    # 4. ç§»é™¤é‡è¤‡è¨˜éŒ„
    before_dedup = len(df_clean)
    df_clean.drop_duplicates(inplace=True)
    after_dedup = len(df_clean)
    logger.info(f"Removed {before_dedup - after_dedup} duplicate rows")

    # 5. è³‡æ–™é©—è­‰
    assert df_clean['date'].notna().all(), "Date column still has NaN values"
    assert df_clean['amount'].notna().all(), "Amount column still has NaN values"
    assert (df_clean['amount'] >= 0).all(), "Amount column has negative values"

    logger.info("Data transformation completed successfully")
    return df_clean

# ä½¿ç”¨ç¯„ä¾‹
df_clean = transform_data(df)
```

#### æ­¥é©Ÿ 3ï¼šè¼‰å…¥ç›®æ¨™ï¼ˆLoadï¼‰

**èˆ‡ AI å°è©±ç¯„ä¾‹**ï¼š
```
ä½ ï¼šè«‹å¹«æˆ‘æŠŠæ¸…æ´—å¾Œçš„è³‡æ–™è¼‰å…¥ PostgreSQL çš„ sales è¡¨

Claude Codeï¼š
æˆ‘æœƒç”Ÿæˆè¼‰å…¥ä»£ç¢¼ï¼ŒåŒ…å«ï¼š
- è³‡æ–™åº«é€£æ¥ç®¡ç†
- æ‰¹æ¬¡è¼‰å…¥ï¼ˆæå‡æ•ˆèƒ½ï¼‰
- éŒ¯èª¤è™•ç†èˆ‡å›æ»¾
- è¼‰å…¥ç‹€æ…‹å ±å‘Š
```

**AI ç”Ÿæˆçš„è¼‰å…¥ä»£ç¢¼**ï¼š
```python
from sqlalchemy import create_engine, text
from contextlib import contextmanager

@contextmanager
def get_db_connection(connection_string: str):
    """
    Context manager for database connection

    Args:
        connection_string: PostgreSQL connection string
    """
    engine = create_engine(connection_string)
    connection = engine.connect()
    try:
        yield connection
        connection.commit()
    except Exception as e:
        connection.rollback()
        logger.error(f"Transaction rolled back: {str(e)}")
        raise
    finally:
        connection.close()

def load_to_postgres(df: pd.DataFrame,
                     connection_string: str,
                     table_name: str = 'sales',
                     if_exists: str = 'append') -> None:
    """
    Load DataFrame to PostgreSQL

    Args:
        df: Cleaned DataFrame
        connection_string: PostgreSQL connection string
        table_name: Target table name
        if_exists: 'append', 'replace', or 'fail'
    """
    logger.info(f"Loading {len(df)} rows to {table_name}")

    try:
        with get_db_connection(connection_string) as conn:
            # æ‰¹æ¬¡è¼‰å…¥ï¼ˆæ¯æ¬¡ 1000 ç­†ï¼‰
            df.to_sql(
                name=table_name,
                con=conn,
                if_exists=if_exists,
                index=False,
                chunksize=1000,
                method='multi'  # æ•ˆèƒ½å„ªåŒ–
            )

            # é©—è­‰è¼‰å…¥çµæœ
            result = conn.execute(
                text(f"SELECT COUNT(*) FROM {table_name}")
            ).fetchone()

            logger.info(f"Successfully loaded data. Total rows in table: {result[0]}")

    except Exception as e:
        logger.error(f"Failed to load data: {str(e)}")
        raise

# ä½¿ç”¨ç¯„ä¾‹
connection_string = 'postgresql://user:password@localhost:5432/mydb'
load_to_postgres(df_clean, connection_string, table_name='sales')
```

---

## ğŸ¯ å®Œæ•´ç¯„ä¾‹ï¼šCSV to Database ETL ç®¡ç·š

### å ´æ™¯æè¿°

**éœ€æ±‚**ï¼š
- æ¯å¤©æœ‰æ–°çš„éŠ·å”®è³‡æ–™ CSV
- éœ€è¦æ¸…æ´—å¾Œè¼‰å…¥ PostgreSQL
- è‡ªå‹•åŒ–åŸ·è¡Œ

### å®Œæ•´ ETL è…³æœ¬

**`etl_pipeline.py`**ï¼š
```python
#!/usr/bin/env python3
"""
Sales Data ETL Pipeline

This script extracts sales data from CSV, transforms it,
and loads it into PostgreSQL database.

Usage:
    python etl_pipeline.py --input data/sales.csv --db postgresql://...
"""

import pandas as pd
import logging
import argparse
from datetime import datetime
from pathlib import Path
from sqlalchemy import create_engine, text
from contextlib import contextmanager

# ===== Configuration =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('etl_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ===== Extract =====
def extract_csv(file_path: str) -> pd.DataFrame:
    """Extract data from CSV file"""
    try:
        logger.info(f"[EXTRACT] Reading CSV from {file_path}")
        df = pd.read_csv(file_path)
        logger.info(f"[EXTRACT] Loaded {len(df)} rows, {len(df.columns)} columns")
        return df
    except Exception as e:
        logger.error(f"[EXTRACT] Failed: {str(e)}")
        raise

# ===== Transform =====
def transform_data(df: pd.DataFrame) -> pd.DataFrame:
    """Transform and clean data"""
    logger.info("[TRANSFORM] Starting data transformation")

    df_clean = df.copy()
    initial_rows = len(df_clean)

    # Date standardization
    df_clean['date'] = pd.to_datetime(df_clean['date'], errors='coerce')

    # Handle missing values
    df_clean['amount'].fillna(df_clean['amount'].median(), inplace=True)

    # Clean strings
    df_clean['product_name'] = df_clean['product_name'].str.strip()

    # Remove duplicates
    df_clean.drop_duplicates(inplace=True)

    # Filter invalid records
    df_clean = df_clean[df_clean['date'].notna()]
    df_clean = df_clean[df_clean['amount'] > 0]

    final_rows = len(df_clean)
    logger.info(f"[TRANSFORM] Completed: {initial_rows} â†’ {final_rows} rows")

    return df_clean

# ===== Load =====
@contextmanager
def get_db_connection(connection_string: str):
    """Database connection context manager"""
    engine = create_engine(connection_string)
    connection = engine.connect()
    try:
        yield connection
        connection.commit()
    except Exception as e:
        connection.rollback()
        logger.error(f"[LOAD] Transaction rolled back: {str(e)}")
        raise
    finally:
        connection.close()

def load_to_postgres(df: pd.DataFrame, connection_string: str, table_name: str) -> None:
    """Load data to PostgreSQL"""
    logger.info(f"[LOAD] Loading {len(df)} rows to {table_name}")

    try:
        with get_db_connection(connection_string) as conn:
            df.to_sql(
                name=table_name,
                con=conn,
                if_exists='append',
                index=False,
                chunksize=1000
            )

            result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}")).fetchone()
            logger.info(f"[LOAD] Success. Total rows in table: {result[0]}")

    except Exception as e:
        logger.error(f"[LOAD] Failed: {str(e)}")
        raise

# ===== Main Pipeline =====
def run_etl_pipeline(input_file: str, connection_string: str, table_name: str = 'sales') -> None:
    """Run complete ETL pipeline"""
    start_time = datetime.now()
    logger.info("=" * 50)
    logger.info("ETL Pipeline Started")
    logger.info("=" * 50)

    try:
        # Extract
        df_raw = extract_csv(input_file)

        # Transform
        df_clean = transform_data(df_raw)

        # Load
        load_to_postgres(df_clean, connection_string, table_name)

        elapsed_time = (datetime.now() - start_time).total_seconds()
        logger.info("=" * 50)
        logger.info(f"ETL Pipeline Completed Successfully ({elapsed_time:.2f}s)")
        logger.info("=" * 50)

    except Exception as e:
        logger.error(f"ETL Pipeline Failed: {str(e)}")
        raise

# ===== CLI =====
def main():
    parser = argparse.ArgumentParser(description='Sales Data ETL Pipeline')
    parser.add_argument('--input', required=True, help='Input CSV file path')
    parser.add_argument('--db', required=True, help='PostgreSQL connection string')
    parser.add_argument('--table', default='sales', help='Target table name')

    args = parser.parse_args()

    run_etl_pipeline(
        input_file=args.input,
        connection_string=args.db,
        table_name=args.table
    )

if __name__ == '__main__':
    main()
```

### ä½¿ç”¨æ–¹å¼

**åŸ·è¡Œ ETL ç®¡ç·š**ï¼š
```bash
# å–®æ¬¡åŸ·è¡Œ
python etl_pipeline.py \
    --input data/sales_2025_01_30.csv \
    --db postgresql://user:pass@localhost:5432/mydb \
    --table sales

# è¨­å®š Cron job æ¯å¤©åŸ·è¡Œ
# ç·¨è¼¯ crontab
crontab -e

# æ–°å¢æ’ç¨‹ï¼ˆæ¯å¤©å‡Œæ™¨ 2 é»åŸ·è¡Œï¼‰
0 2 * * * /usr/bin/python3 /path/to/etl_pipeline.py --input /data/sales.csv --db postgresql://...
```

### è¼¸å‡ºæ—¥èªŒç¯„ä¾‹

```
2025-10-30 02:00:00 - __main__ - INFO - ==================================================
2025-10-30 02:00:00 - __main__ - INFO - ETL Pipeline Started
2025-10-30 02:00:00 - __main__ - INFO - ==================================================
2025-10-30 02:00:01 - __main__ - INFO - [EXTRACT] Reading CSV from data/sales.csv
2025-10-30 02:00:02 - __main__ - INFO - [EXTRACT] Loaded 10000 rows, 5 columns
2025-10-30 02:00:02 - __main__ - INFO - [TRANSFORM] Starting data transformation
2025-10-30 02:00:05 - __main__ - INFO - [TRANSFORM] Completed: 10000 â†’ 9876 rows
2025-10-30 02:00:05 - __main__ - INFO - [LOAD] Loading 9876 rows to sales
2025-10-30 02:00:08 - __main__ - INFO - [LOAD] Success. Total rows in table: 50000
2025-10-30 02:00:08 - __main__ - INFO - ==================================================
2025-10-30 02:00:08 - __main__ - INFO - ETL Pipeline Completed Successfully (8.23s)
2025-10-30 02:00:08 - __main__ - INFO - ==================================================
```

---

## ğŸ”§ AI Agent é¸æ“‡

### æ¨è–¦ Agent

**å ´æ™¯ï¼šETL ç®¡ç·šé–‹ç™¼**
```bash
# åˆ‡æ›åˆ°è³‡æ–™å·¥ç¨‹å¸« Agent
/agents:data-engineer

# æè¿°éœ€æ±‚
æˆ‘éœ€è¦å»ºç«‹ä¸€å€‹ ETL ç®¡ç·šï¼š
- ä¾†æºï¼šCSV æª”æ¡ˆï¼ˆéŠ·å”®è³‡æ–™ï¼‰
- è½‰æ›ï¼šæ¸…æ´—ã€æ ¼å¼åŒ–ã€é©—è­‰
- ç›®æ¨™ï¼šPostgreSQL è³‡æ–™åº«

è«‹å¹«æˆ‘è¨­è¨ˆå®Œæ•´çš„ ETL è…³æœ¬
```

**ç‚ºä»€éº¼é¸ data-engineer Agentï¼Ÿ**
- âœ… å°ˆæ¥­çš„è³‡æ–™ç®¡ç·šè¨­è¨ˆçŸ¥è­˜
- âœ… è‡ªå‹•è€ƒæ…®éŒ¯èª¤è™•ç†èˆ‡æ—¥èªŒ
- âœ… éµå¾ª ETL æœ€ä½³å¯¦è¸
- âœ… ç”Ÿæˆé«˜æ•ˆèƒ½çš„æ‰¹æ¬¡è™•ç†ä»£ç¢¼

---

## âš ï¸ å¸¸è¦‹é™·é˜±èˆ‡è§£æ±ºæ–¹æ¡ˆ

### é™·é˜± 1ï¼šè¨˜æ†¶é«”ä¸è¶³

**å•é¡Œ**ï¼š
```python
# éŒ¯èª¤ï¼šä¸€æ¬¡è¼‰å…¥ 10GB CSV
df = pd.read_csv('huge_file.csv')  # MemoryError!
```

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# æ­£ç¢ºï¼šä½¿ç”¨ chunk processing
chunk_size = 10000
for chunk in pd.read_csv('huge_file.csv', chunksize=chunk_size):
    process_chunk(chunk)
```

**AI è¼”åŠ©**ï¼š
```
ä½ ï¼šé€™å€‹ CSV æª”æ¡ˆæœ‰ 10GBï¼ŒPandas æœƒè¨˜æ†¶é«”ä¸è¶³

Claude Codeï¼š
æˆ‘æœƒç”Ÿæˆ chunk processing ä»£ç¢¼ï¼š
- åˆ†æ‰¹è®€å–ï¼ˆæ¯æ¬¡ 10,000 ç­†ï¼‰
- é€æ‰¹è™•ç†èˆ‡è¼‰å…¥
- é€²åº¦ç›£æ§
```

### é™·é˜± 2ï¼šè³‡æ–™å‹åˆ¥éŒ¯èª¤

**å•é¡Œ**ï¼š
```python
# éŒ¯èª¤ï¼šè‡ªå‹•æ¨æ–·å‹åˆ¥å¯èƒ½éŒ¯èª¤
df = pd.read_csv('sales.csv')
# '123' è¢«ç•¶æˆå­—ä¸²ï¼Œå¯¦éš›æ‡‰è©²æ˜¯æ•´æ•¸
```

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# æ­£ç¢ºï¼šæ˜ç¢ºæŒ‡å®šè³‡æ–™å‹åˆ¥
dtype = {
    'product_id': 'int64',
    'amount': 'float64',
    'date': 'str'  # å…ˆè®€æˆå­—ä¸²ï¼Œå†ç”¨ pd.to_datetime è½‰æ›
}
df = pd.read_csv('sales.csv', dtype=dtype)
```

### é™·é˜± 3ï¼šé‡è¤‡è¼‰å…¥

**å•é¡Œ**ï¼š
```python
# éŒ¯èª¤ï¼šæ¯æ¬¡åŸ·è¡Œéƒ½é‡è¤‡è¼‰å…¥ç›¸åŒè³‡æ–™
load_to_postgres(df, db, table='sales', if_exists='append')
```

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# æ­£ç¢ºï¼šæª¢æŸ¥æ˜¯å¦å·²è¼‰å…¥
def is_already_loaded(connection, date):
    query = f"SELECT COUNT(*) FROM sales WHERE date = '{date}'"
    result = connection.execute(text(query)).fetchone()
    return result[0] > 0

if not is_already_loaded(conn, target_date):
    load_to_postgres(df, db, table='sales')
else:
    logger.info(f"Data for {target_date} already loaded, skipping")
```

### é™·é˜± 4ï¼šç¼ºä¹éŒ¯èª¤è™•ç†

**å•é¡Œ**ï¼š
```python
# éŒ¯èª¤ï¼šæ²’æœ‰éŒ¯èª¤è™•ç†ï¼Œä¸€å€‹éŒ¯èª¤å°è‡´æ•´å€‹ç®¡ç·šå¤±æ•—
df = pd.read_csv('file.csv')
df_clean = transform(df)
load(df_clean)
```

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# æ­£ç¢ºï¼šå®Œå–„çš„éŒ¯èª¤è™•ç†
try:
    df = pd.read_csv('file.csv')
except FileNotFoundError:
    logger.error("File not found")
    send_alert("ETL Failed: File not found")
    sys.exit(1)

try:
    df_clean = transform(df)
except ValidationError as e:
    logger.error(f"Data validation failed: {e}")
    save_failed_records(df, 'failed_records.csv')
    sys.exit(1)

try:
    load(df_clean)
except DatabaseError as e:
    logger.error(f"Database load failed: {e}")
    rollback()
    sys.exit(1)
```

---

## ğŸ“ é€²éšï¼šä½•æ™‚éœ€è¦å°ˆæ¥­ ETL å·¥å…·ï¼Ÿ

### ç°¡å–®å ´æ™¯ vs è¤‡é›œå ´æ™¯

```
å ´æ™¯è¤‡é›œåº¦         å»ºè­°å·¥å…·               ç†ç”±
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
å–®ä¸€ CSV è™•ç†     Pandas + AI ç”Ÿæˆ       å¿«é€Ÿã€å¤ ç”¨
è³‡æ–™é‡ < 1GB      Python è…³æœ¬
åŸ·è¡Œé »ç‡ï¼šæ¯å¤©

å¤šä¾†æºæ•´åˆ        Airflow / Prefect      å·¥ä½œæµç¨‹ç·¨æ’
è³‡æ–™é‡ 1-10GB     + Python               ä¾è³´ç®¡ç†
åŸ·è¡Œé »ç‡ï¼šæ¯å°æ™‚                         éŒ¯èª¤é‡è©¦

ä¼æ¥­ç´š ETL        å°ˆæ¥­å·¥å…·               åœ–å½¢åŒ–ä»‹é¢
è³‡æ–™é‡ > 10GB     (Talend, Informatica)  ä¼æ¥­æ”¯æ´
è¤‡é›œè½‰æ›é‚è¼¯                             ç›£æ§èˆ‡æ²»ç†

å³æ™‚ä¸²æµ          Kafka + Flink          ä½å»¶é²
æ¯«ç§’ç´šè™•ç†        Spark Streaming        åˆ†æ•£å¼è™•ç†
```

### æ±ºç­–æ¨¹

```
ä½ çš„ ETL éœ€æ±‚ï¼š

â”œâ”€ è³‡æ–™é‡ < 1GBï¼Œä¾†æº < 3 å€‹
â”‚  â””â”€ ç”¨ Pandas + AI ç”Ÿæˆè…³æœ¬ âœ…
â”‚
â”œâ”€ è³‡æ–™é‡ 1-10GBï¼Œä¾†æº 3-10 å€‹
â”‚  â””â”€ ç”¨ Airflow ç·¨æ’ Python è…³æœ¬ âœ…
â”‚
â””â”€ è³‡æ–™é‡ > 10GBï¼Œä¾†æº > 10 å€‹
   â””â”€ æ‰¾å°ˆæ¥­è³‡æ–™å·¥ç¨‹å¸« ğŸ‘¨â€ğŸ’»
      æˆ–ä½¿ç”¨ä¼æ¥­ç´š ETL å·¥å…·
```

---

## ğŸ“Š æ•ˆèƒ½å„ªåŒ–å»ºè­°

### å„ªåŒ–æŠ€å·§

**1. ä½¿ç”¨ Chunk Processing**
```python
# ä¸è¦ï¼šä¸€æ¬¡è¼‰å…¥å…¨éƒ¨
df = pd.read_csv('huge.csv')

# è¦ï¼šåˆ†æ‰¹è™•ç†
for chunk in pd.read_csv('huge.csv', chunksize=10000):
    process_and_load(chunk)
```

**2. ä½¿ç”¨æ­£ç¢ºçš„è³‡æ–™å‹åˆ¥**
```python
# int64 â†’ int32 å¯ç¯€çœ 50% è¨˜æ†¶é«”
df['id'] = df['id'].astype('int32')

# object â†’ category å¯ç¯€çœ 90% è¨˜æ†¶é«”ï¼ˆé‡è¤‡å€¼å¤šæ™‚ï¼‰
df['status'] = df['status'].astype('category')
```

**3. å¹³è¡Œè™•ç†**
```python
from multiprocessing import Pool

def process_file(file_path):
    df = pd.read_csv(file_path)
    return transform_and_load(df)

# å¹³è¡Œè™•ç†å¤šå€‹æª”æ¡ˆ
with Pool(4) as p:
    results = p.map(process_file, file_list)
```

**4. ä½¿ç”¨æ›´å¿«çš„å·¥å…·**
```python
# Pandas å¤ªæ…¢ï¼Ÿè©¦è©¦ Polars
import polars as pl

# é€Ÿåº¦å¿« 5-10 å€
df = pl.read_csv('large.csv')
df_clean = df.filter(pl.col('amount') > 0)
```

---

## ğŸ¯ å¯¦æˆ°ç·´ç¿’

### ç·´ç¿’ 1ï¼šç”Ÿæˆä½ çš„ç¬¬ä¸€å€‹ ETL ç®¡ç·š

**ä»»å‹™**ï¼š
```
1. æº–å‚™æ¸¬è©¦è³‡æ–™ï¼ˆæˆ–ä½¿ç”¨ç¯„ä¾‹ CSVï¼‰
2. ç”¨ Claude Code ç”Ÿæˆ ETL è…³æœ¬
3. åŸ·è¡Œä¸¦é©—è­‰çµæœ
4. æª¢æŸ¥æ—¥èªŒè¼¸å‡º
```

**æç¤º**ï¼š
```bash
# èˆ‡ Claude Code å°è©±
æˆ‘æœ‰ä¸€å€‹ CSV æª”æ¡ˆåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
- dateï¼ˆæ—¥æœŸï¼Œæ ¼å¼ä¸çµ±ä¸€ï¼‰
- customer_idï¼ˆå®¢æˆ¶ IDï¼‰
- productï¼ˆç”¢å“åç¨±ï¼‰
- amountï¼ˆé‡‘é¡ï¼Œæœ‰ç©ºå€¼ï¼‰

è«‹å¹«æˆ‘ç”Ÿæˆä¸€å€‹ ETL è…³æœ¬ï¼š
1. è®€å– CSV
2. æ¨™æº–åŒ–æ—¥æœŸæ ¼å¼
3. å¡«å……é‡‘é¡ç©ºå€¼ï¼ˆç”¨ä¸­ä½æ•¸ï¼‰
4. ç§»é™¤é‡è¤‡è¨˜éŒ„
5. è¼¸å‡ºåˆ° cleaned_sales.csv
```

### ç·´ç¿’ 2ï¼šåŠ å…¥è³‡æ–™å“è³ªæª¢æŸ¥

**ä»»å‹™**ï¼š
```
æ“´å±• ETL è…³æœ¬ï¼ŒåŠ å…¥ï¼š
- è³‡æ–™å“è³ªå ±å‘Šï¼ˆç©ºå€¼ã€ç•°å¸¸å€¼çµ±è¨ˆï¼‰
- è½‰æ›å‰å¾Œå°æ¯”
- å“è³ªé–¾å€¼è­¦å‘Š
```

---

## ğŸ“š å»¶ä¼¸é–±è®€

**Pandas ç›¸é—œ**ï¼š
- [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/)
- [10 Minutes to Pandas](https://pandas.pydata.org/docs/user_guide/10min.html)

**ETL å·¥å…·**ï¼š
- [Apache Airflow](https://airflow.apache.org/) - æœ€æµè¡Œçš„é–‹æºå·¥ä½œæµç¨‹ç·¨æ’å·¥å…·
- [Prefect](https://www.prefect.io/) - ç¾ä»£åŒ– ETL å·¥å…·
- [dbt](https://www.getdbt.com/) - è³‡æ–™è½‰æ›å·¥å…·

**é€²éšæ•ˆèƒ½**ï¼š
- [Polars](https://pola-rs.github.io/polars/) - æ¯” Pandas å¿« 5-10 å€
- [Dask](https://dask.org/) - å¹³è¡Œé‹ç®—æ¡†æ¶

---

## ğŸ¯ å­¸ç¿’æª¢æŸ¥é»

å®Œæˆæœ¬ç« å¾Œï¼Œä½ æ‡‰è©²èƒ½å¤ ï¼š

- [ ] èªªæ˜ ETL çš„ä¸‰å€‹éšæ®µï¼ˆExtract-Transform-Loadï¼‰
- [ ] ç”¨ AI ç”Ÿæˆç°¡å–®çš„ ETL è…³æœ¬
- [ ] ç†è§£å¸¸è¦‹çš„è³‡æ–™æ¸…æ´—æ“ä½œ
- [ ] çŸ¥é“ä½•æ™‚éœ€è¦å°ˆæ¥­ ETL å·¥å…·
- [ ] èƒ½å¤ åŸ·è¡Œå®Œæ•´çš„ CSV to Database ç®¡ç·š

---

**ç« ç¯€ç‰ˆæœ¬**ï¼šv1.0
**æœ€å¾Œæ›´æ–°**ï¼š2025-10-30
**é è¨ˆå­¸ç¿’æ™‚é•·**ï¼š15 åˆ†é˜ï¼ˆç†è«–ï¼‰+ 30 åˆ†é˜ï¼ˆå¯¦ä½œï¼‰
