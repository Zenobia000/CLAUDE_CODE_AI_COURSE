# 8.1 ETL ÁÆ°Á∑öÂø´ÈÄüÂéüÂûã - Áî® AI Âä†ÈÄüË≥áÊñôËôïÁêÜ

## üìã Á´†ÁØÄÊ¶ÇËø∞

**Â≠∏ÁøíÊôÇÈï∑**Ôºö15 ÂàÜÈêò
**Èõ£Â∫¶**Ôºö‚≠ê‚≠ê Âü∫Á§éËá≥‰∏≠Á¥ö
**ÂâçÁΩÆÁü•Ë≠ò**ÔºöPython Âü∫Á§é„ÄÅCSV Ë≥áÊñôËôïÁêÜÊ¶ÇÂøµ

Êú¨Á´†Êïô‰Ω†Ôºö
- ‚úÖ ÁêÜËß£ ETL ÁöÑÊ†∏ÂøÉÊ¶ÇÂøµÔºàExtract-Transform-LoadÔºâ
- ‚úÖ Áî® AI Âø´ÈÄüÁîüÊàê ETL ÁÆ°Á∑ö‰ª£Á¢º
- ‚úÖ ÂØ¶Êà∞ÔºöCSV to Database ÂÆåÊï¥ÊµÅÁ®ã
- ‚úÖ Áü•ÈÅì‰ΩïÊôÇÈúÄË¶ÅÂ∞àÊ•≠ ETL Â∑•ÂÖ∑

---

## üéØ ‰ªÄÈ∫ºÊòØ ETLÔºü

### Âü∫Êú¨ÂÆöÁæ©

**ETL** = Extract + Transform + Load

```
Ë≥áÊñôÊóÖÁ®ãÔºö
ÂéüÂßãË≥áÊñô ‚Üí ÊèêÂèñ ‚Üí ËΩâÊèõ ‚Üí ËºâÂÖ• ‚Üí ÂèØÁî®Ë≥áÊñô

Â∞±ÂÉèÈ£üÁâ©ËôïÁêÜÔºö
È£üÊùê ‚Üí Ê∏ÖÊ¥ó ‚Üí ÂàáÂ°äË™øÂë≥ ‚Üí Ë£ùÁõ§ ‚Üí ÁæéÈ£ü
```

### ‰∏âÂÄãÈöéÊÆµË©≥Ëß£

#### 1. ExtractÔºàÊèêÂèñÔºâ
```python
# ÂæûÂêÑÁ®Æ‰æÜÊ∫êÊèêÂèñË≥áÊñô
sources = {
    'CSVÊ™îÊ°à': 'data/sales.csv',
    'Ë≥áÊñôÂ∫´': 'postgresql://...',
    'API': 'https://api.example.com/data',
    'Excel': 'reports/Q4.xlsx'
}
```

#### 2. TransformÔºàËΩâÊèõÔºâ
```python
# Â∏∏Ë¶ãËΩâÊèõÊìç‰Ωú
transformations = [
    'Ë≥áÊñôÊ∏ÖÊ¥óÔºàÁßªÈô§Á©∫ÂÄº„ÄÅÁï∞Â∏∏ÂÄºÔºâ',
    'Ê†ºÂºèËΩâÊèõÔºàÊó•Êúü„ÄÅÊï∏Â≠óÊ†ºÂºèÔºâ',
    'Ë≥áÊñôÂêà‰ΩµÔºàÂ§öÂÄã‰æÜÊ∫êÊï¥ÂêàÔºâ',
    'Ê¨Ñ‰ΩçË®àÁÆóÔºàÊñ∞Â¢ûË°çÁîüÊ¨Ñ‰ΩçÔºâ',
    'Ë≥áÊñôÈ©óË≠âÔºàÁ¢∫‰øùÂìÅË≥™Ôºâ'
]
```

#### 3. LoadÔºàËºâÂÖ•Ôºâ
```python
# ËºâÂÖ•Âà∞ÁõÆÊ®ôÁ≥ªÁµ±
targets = {
    'ÈóúËÅØÂºèË≥áÊñôÂ∫´': 'PostgreSQL, MySQL',
    'Ë≥áÊñôÂÄâÂÑ≤': 'Snowflake, BigQuery',
    'Ê™îÊ°àÁ≥ªÁµ±': 'Parquet, JSON',
    'Âç≥ÊôÇ‰∏≤ÊµÅ': 'Kafka, RabbitMQ'
}
```

---

## ü§î ÁÇ∫‰ªÄÈ∫ºÈúÄË¶Å ETLÔºü

### Ëß£Ê±∫ÁöÑÂïèÈ°å

```
ÂïèÈ°åÂ†¥ÊôØ 1ÔºöË≥áÊñôÂàÜÊï£Âú®Â§öÂÄãÁ≥ªÁµ±
‚îú‚îÄ Èä∑ÂîÆË≥áÊñôÂú® CRM
‚îú‚îÄ Â∫´Â≠òË≥áÊñôÂú® ERP
‚îú‚îÄ ÂÆ¢Êà∂Ë≥áÊñôÂú® MySQL
‚îî‚îÄ ÈúÄË¶ÅÊï¥ÂêàÂàÜÊûê ‚Üí ETL ÁÆ°Á∑öÊï¥Âêà

ÂïèÈ°åÂ†¥ÊôØ 2ÔºöË≥áÊñôÂìÅË≥™‰∏ç‰Ω≥
‚îú‚îÄ Á©∫ÂÄº„ÄÅÈáçË§áÂÄº
‚îú‚îÄ Ê†ºÂºè‰∏ç‰∏ÄËá¥
‚îú‚îÄ Áï∞Â∏∏ÂÄº
‚îî‚îÄ ÈúÄË¶ÅÊ∏ÖÊ¥óËΩâÊèõ ‚Üí ETL ÁÆ°Á∑öÊ∏ÖÊ¥ó

ÂïèÈ°åÂ†¥ÊôØ 3ÔºöÈúÄË¶ÅÂÆöÊôÇÊõ¥Êñ∞
‚îú‚îÄ ÊØèÂ§©Êõ¥Êñ∞Â†±Ë°®
‚îú‚îÄ ÊØèÂ∞èÊôÇÂêåÊ≠•Ë≥áÊñô
‚îú‚îÄ Âç≥ÊôÇË≥áÊñôËôïÁêÜ
‚îî‚îÄ ÈúÄË¶ÅËá™ÂãïÂåñ ‚Üí ETL ÁÆ°Á∑öÊéíÁ®ã
```

### Linux È°ûÊØî

```
ETL Â∞±ÂÉè Linux ÁöÑ pipeÔºàÁÆ°ÈÅìÔºâÔºö

cat raw_data.txt | grep "error" | sort | uniq > clean_data.txt
‚îÇ                  ‚îÇ              ‚îÇ      ‚îÇ      ‚îÇ
ÊèêÂèñ               ÈÅéÊøæ          ÊéíÂ∫è   ÂéªÈáç   ËºâÂÖ•

ETL:
Extract raw_data ‚Üí Transform (filter, sort) ‚Üí Load clean_data
```

---

## üÜö ÂÇ≥Áµ± ETL Â∑•ÂÖ∑ vs AI ËºîÂä© ETL

### Â∑•ÂÖ∑Â∞çÊØî

```
Â†¥ÊôØ              ÂÇ≥Áµ±Â∑•ÂÖ∑              AI ËºîÂä©ÊñπÂºè
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Á∞°ÂñÆ CSV ËôïÁêÜ    ÊâãÂØ´ Pandas ‰ª£Á¢º      AI ÁîüÊàêËÖ≥Êú¨Ôºà5 ÂàÜÈêòÔºâ
                (30 ÂàÜÈêò)            Âø´ÈÄü„ÄÅÊ∫ñÁ¢∫

Ë§áÈõúË≥áÊñôÁÆ°Á∑ö     Airflow/Prefect      AI Ë®≠Ë®àÊµÅÁ®ãÂúñ
                (Êï∏Â§©Â≠∏ÁøíËàáÈñãÁôº)      ÂÇ≥Áµ±Â∑•ÂÖ∑Âü∑Ë°å

‰ºÅÊ•≠Á¥ö ETL      Â∞àÊ•≠ ETL Â∑•ÂÖ∑         AI ËºîÂä©ÈÖçÁΩÆ
                (Talend, Informatica) Âä†ÈÄüÈñãÁôº

Âç≥ÊôÇ‰∏≤ÊµÅ         Kafka + Flink        AI ÁîüÊàêËôïÁêÜÈÇèËºØ
                (Â∞àÊ•≠Áü•Ë≠ò)           Èôç‰ΩéÈñÄÊ™ª
```

### AI ËºîÂä©ÁöÑÂÑ™Âã¢

**ÂÑ™Âã¢ 1ÔºöÂø´ÈÄüÂéüÂûã**
```
ÂÇ≥Áµ±ÊñπÂºèÔºö
1. Á†îÁ©∂ Pandas API ÊñáÊ™îÔºà30 ÂàÜÈêòÔºâ
2. ÂØ´Ë≥áÊñôÊ∏ÖÊ¥ó‰ª£Á¢ºÔºà1 Â∞èÊôÇÔºâ
3. Èô§ÈåØ‰øÆÊ≠£Ôºà30 ÂàÜÈêòÔºâ
Á∏ΩË®àÔºö2 Â∞èÊôÇ

AI ËºîÂä©Ôºö
1. ÊèèËø∞ÈúÄÊ±ÇÁµ¶ Claude CodeÔºà2 ÂàÜÈêòÔºâ
2. AI ÁîüÊàêÂÆåÊï¥‰ª£Á¢ºÔºà3 ÂàÜÈêòÔºâ
3. Ê∏¨Ë©¶È©óË≠âÔºà5 ÂàÜÈêòÔºâ
Á∏ΩË®àÔºö10 ÂàÜÈêò
```

**ÂÑ™Âã¢ 2ÔºöÊúÄ‰Ω≥ÂØ¶Ë∏êÂÖßÂª∫**
```python
# AI ÁîüÊàêÁöÑ‰ª£Á¢ºËá™ÂãïÂåÖÂê´Ôºö
- ÈåØË™§ËôïÁêÜÔºàtry-exceptÔºâ
- Êó•Ë™åË®òÈåÑÔºàloggingÔºâ
- Ë≥áÊñôÈ©óË≠âÔºàassert/validateÔºâ
- ÊïàËÉΩÂÑ™ÂåñÔºàchunk processingÔºâ
```

**ÂÑ™Âã¢ 3ÔºöÊòìÊñºÁ∂≠Ë≠∑**
```python
# AI ÂèØ‰ª•Ôºö
- Ëá™ÂãïÁîüÊàêÊñáÊ™î
- Ê∑ªÂä†Ê∏ÖÊô∞ÁöÑË®ªËß£
- ÈÅµÂæ™ÊúÄ‰Ω≥ÂØ¶Ë∏ê
- ÂñÆÂÖÉÊ∏¨Ë©¶ÁîüÊàê
```

---

## üöÄ AI ËºîÂä© ETL Â∑•‰ΩúÊµÅÁ®ã

### ÂÆåÊï¥ÊµÅÁ®ãÂúñ

```
1. ÈúÄÊ±ÇÊèèËø∞
   ‚Üì
   „ÄåÊàëÈúÄË¶ÅÂ∞á CSV ÁöÑÈä∑ÂîÆË≥áÊñôÊ∏ÖÊ¥óÂæåËºâÂÖ• PostgreSQL„Äç
   ‚Üì
2. AI ÂàÜÊûêË≥áÊñô
   ‚Üì
   Claude Code ÂàÜÊûê CSV ÁµêÊßã„ÄÅÊ¨Ñ‰ΩçÈ°ûÂûã
   ‚Üì
3. Ë®≠Ë®àËΩâÊèõÈÇèËºØ
   ‚Üì
   AI Âª∫Ë≠∞ÔºöÁßªÈô§Á©∫ÂÄº„ÄÅÊ†ºÂºèËΩâÊèõ„ÄÅË≥áÊñôÈ©óË≠â
   ‚Üì
4. ÁîüÊàê ETL ‰ª£Á¢º
   ‚Üì
   ÂÆåÊï¥ÁöÑ Python ËÖ≥Êú¨ÔºàÂê´ÈåØË™§ËôïÁêÜÔºâ
   ‚Üì
5. Ê∏¨Ë©¶ËàáÈ©óË≠â
   ‚Üì
   Âü∑Ë°åËÖ≥Êú¨ÔºåÊ™¢Êü•Ë≥áÊñôÂìÅË≥™
   ‚Üì
6. ÊéíÁ®ãËá™ÂãïÂåñ
   ‚Üì
   Ë®≠ÂÆö Cron job Êàñ Airflow
```

### ÈóúÈçµÊ≠•È©üË©≥Ëß£

#### Ê≠•È©ü 1ÔºöË≥áÊñôÊ∫êÈÄ£Êé•ÔºàExtractÔºâ

**Ëàá AI Â∞çË©±ÁØÑ‰æã**Ôºö
```
‰Ω†ÔºöÊàëÊúâ‰∏ÄÂÄã sales.csv Ê™îÊ°àÔºåË´ãÂπ´ÊàëÂàÜÊûêÁµêÊßã

Claude CodeÔºö
1. ËÆÄÂèñ CSV
2. È°ØÁ§∫Ââç 5 Ë°å
3. Ê¨Ñ‰ΩçÈ°ûÂûãÂàÜÊûê
4. Ë≥áÊñôÂìÅË≥™Â†±ÂëäÔºàÁ©∫ÂÄº„ÄÅÁï∞Â∏∏ÂÄºÔºâ
```

**AI ÁîüÊàêÁöÑ‰ª£Á¢º**Ôºö
```python
import pandas as pd
import logging

# Ë®≠ÂÆöÊó•Ë™å
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def extract_csv(file_path: str) -> pd.DataFrame:
    """
    Extract data from CSV file with error handling

    Args:
        file_path: Path to CSV file

    Returns:
        DataFrame with extracted data
    """
    try:
        logger.info(f"Reading CSV from {file_path}")
        df = pd.read_csv(file_path)
        logger.info(f"Successfully loaded {len(df)} rows")

        # Data quality report
        logger.info(f"Columns: {list(df.columns)}")
        logger.info(f"Missing values:\n{df.isnull().sum()}")

        return df

    except FileNotFoundError:
        logger.error(f"File not found: {file_path}")
        raise
    except pd.errors.EmptyDataError:
        logger.error(f"Empty CSV file: {file_path}")
        raise
    except Exception as e:
        logger.error(f"Error reading CSV: {str(e)}")
        raise

# ‰ΩøÁî®ÁØÑ‰æã
df = extract_csv('data/sales.csv')
```

#### Ê≠•È©ü 2ÔºöË≥áÊñôÊ∏ÖÊ¥óË¶èÂâáÔºàTransformÔºâ

**Ëàá AI Â∞çË©±ÁØÑ‰æã**Ôºö
```
‰Ω†ÔºöÈÄôÂÄã CSV Êúâ‰ª•‰∏ãÂïèÈ°åÔºö
1. date Ê¨Ñ‰ΩçÊ†ºÂºè‰∏ç‰∏ÄËá¥
2. amount ÊúâÁ©∫ÂÄº
3. product_name ÊúâÈáçË§áÁöÑÁ©∫Ê†º
Ë´ãÂπ´ÊàëË®≠Ë®àÊ∏ÖÊ¥óÈÇèËºØ

Claude CodeÔºö
ÊàëÊúÉÁîüÊàêÊ∏ÖÊ¥óÂáΩÊï∏ÔºåÂåÖÂê´Ôºö
- Êó•ÊúüÊ†ºÂºèÊ®ôÊ∫ñÂåñ
- Á©∫ÂÄºËôïÁêÜÁ≠ñÁï•
- Â≠ó‰∏≤Ê∏ÖÁêÜ
- Ë≥áÊñôÈ©óË≠â
```

**AI ÁîüÊàêÁöÑÊ∏ÖÊ¥ó‰ª£Á¢º**Ôºö
```python
from datetime import datetime
import re

def transform_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Transform and clean sales data

    Args:
        df: Raw DataFrame

    Returns:
        Cleaned DataFrame
    """
    logger.info("Starting data transformation")

    df_clean = df.copy()

    # 1. Ê®ôÊ∫ñÂåñÊó•ÊúüÊ†ºÂºè
    def parse_date(date_str):
        """Parse various date formats"""
        formats = ['%Y-%m-%d', '%d/%m/%Y', '%m-%d-%Y']
        for fmt in formats:
            try:
                return pd.to_datetime(date_str, format=fmt)
            except:
                continue
        return pd.NaT

    df_clean['date'] = df_clean['date'].apply(parse_date)
    logger.info(f"Parsed {df_clean['date'].notna().sum()} dates")

    # 2. ËôïÁêÜ amount Á©∫ÂÄºÔºàÁî®‰∏≠‰ΩçÊï∏Â°´ÂÖÖÔºâ
    median_amount = df_clean['amount'].median()
    df_clean['amount'].fillna(median_amount, inplace=True)
    logger.info(f"Filled missing amounts with median: {median_amount}")

    # 3. Ê∏ÖÁêÜ product_nameÔºàÁßªÈô§Â§öÈ§òÁ©∫Ê†ºÔºâ
    df_clean['product_name'] = df_clean['product_name'].str.strip()
    df_clean['product_name'] = df_clean['product_name'].str.replace(r'\s+', ' ', regex=True)

    # 4. ÁßªÈô§ÈáçË§áË®òÈåÑ
    before_dedup = len(df_clean)
    df_clean.drop_duplicates(inplace=True)
    after_dedup = len(df_clean)
    logger.info(f"Removed {before_dedup - after_dedup} duplicate rows")

    # 5. Ë≥áÊñôÈ©óË≠â
    assert df_clean['date'].notna().all(), "Date column still has NaN values"
    assert df_clean['amount'].notna().all(), "Amount column still has NaN values"
    assert (df_clean['amount'] >= 0).all(), "Amount column has negative values"

    logger.info("Data transformation completed successfully")
    return df_clean

# ‰ΩøÁî®ÁØÑ‰æã
df_clean = transform_data(df)
```

#### Ê≠•È©ü 3ÔºöËºâÂÖ•ÁõÆÊ®ôÔºàLoadÔºâ

**Ëàá AI Â∞çË©±ÁØÑ‰æã**Ôºö
```
‰Ω†ÔºöË´ãÂπ´ÊàëÊääÊ∏ÖÊ¥óÂæåÁöÑË≥áÊñôËºâÂÖ• PostgreSQL ÁöÑ sales Ë°®

Claude CodeÔºö
ÊàëÊúÉÁîüÊàêËºâÂÖ•‰ª£Á¢ºÔºåÂåÖÂê´Ôºö
- Ë≥áÊñôÂ∫´ÈÄ£Êé•ÁÆ°ÁêÜ
- ÊâπÊ¨°ËºâÂÖ•ÔºàÊèêÂçáÊïàËÉΩÔºâ
- ÈåØË™§ËôïÁêÜËàáÂõûÊªæ
- ËºâÂÖ•ÁãÄÊÖãÂ†±Âëä
```

**AI ÁîüÊàêÁöÑËºâÂÖ•‰ª£Á¢º**Ôºö
```python
from sqlalchemy import create_engine, text
from contextlib import contextmanager

@contextmanager
def get_db_connection(connection_string: str):
    """
    Context manager for database connection

    Args:
        connection_string: PostgreSQL connection string
    """
    engine = create_engine(connection_string)
    connection = engine.connect()
    try:
        yield connection
        connection.commit()
    except Exception as e:
        connection.rollback()
        logger.error(f"Transaction rolled back: {str(e)}")
        raise
    finally:
        connection.close()

def load_to_postgres(df: pd.DataFrame,
                     connection_string: str,
                     table_name: str = 'sales',
                     if_exists: str = 'append') -> None:
    """
    Load DataFrame to PostgreSQL

    Args:
        df: Cleaned DataFrame
        connection_string: PostgreSQL connection string
        table_name: Target table name
        if_exists: 'append', 'replace', or 'fail'
    """
    logger.info(f"Loading {len(df)} rows to {table_name}")

    try:
        with get_db_connection(connection_string) as conn:
            # ÊâπÊ¨°ËºâÂÖ•ÔºàÊØèÊ¨° 1000 Á≠ÜÔºâ
            df.to_sql(
                name=table_name,
                con=conn,
                if_exists=if_exists,
                index=False,
                chunksize=1000,
                method='multi'  # ÊïàËÉΩÂÑ™Âåñ
            )

            # È©óË≠âËºâÂÖ•ÁµêÊûú
            result = conn.execute(
                text(f"SELECT COUNT(*) FROM {table_name}")
            ).fetchone()

            logger.info(f"Successfully loaded data. Total rows in table: {result[0]}")

    except Exception as e:
        logger.error(f"Failed to load data: {str(e)}")
        raise

# ‰ΩøÁî®ÁØÑ‰æã
connection_string = 'postgresql://user:password@localhost:5432/mydb'
load_to_postgres(df_clean, connection_string, table_name='sales')
```

---

## üéØ ÂÆåÊï¥ÁØÑ‰æãÔºöCSV to Database ETL ÁÆ°Á∑ö

### Â†¥ÊôØÊèèËø∞

**ÈúÄÊ±Ç**Ôºö
- ÊØèÂ§©ÊúâÊñ∞ÁöÑÈä∑ÂîÆË≥áÊñô CSV
- ÈúÄË¶ÅÊ∏ÖÊ¥óÂæåËºâÂÖ• PostgreSQL
- Ëá™ÂãïÂåñÂü∑Ë°å

### ÂÆåÊï¥ ETL ËÖ≥Êú¨

**`etl_pipeline.py`**Ôºö
```python
#!/usr/bin/env python3
"""
Sales Data ETL Pipeline

This script extracts sales data from CSV, transforms it,
and loads it into PostgreSQL database.

Usage:
    python etl_pipeline.py --input data/sales.csv --db postgresql://...
"""

import pandas as pd
import logging
import argparse
from datetime import datetime
from pathlib import Path
from sqlalchemy import create_engine, text
from contextlib import contextmanager

# ===== Configuration =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('etl_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ===== Extract =====
def extract_csv(file_path: str) -> pd.DataFrame:
    """Extract data from CSV file"""
    try:
        logger.info(f"[EXTRACT] Reading CSV from {file_path}")
        df = pd.read_csv(file_path)
        logger.info(f"[EXTRACT] Loaded {len(df)} rows, {len(df.columns)} columns")
        return df
    except Exception as e:
        logger.error(f"[EXTRACT] Failed: {str(e)}")
        raise

# ===== Transform =====
def transform_data(df: pd.DataFrame) -> pd.DataFrame:
    """Transform and clean data"""
    logger.info("[TRANSFORM] Starting data transformation")

    df_clean = df.copy()
    initial_rows = len(df_clean)

    # Date standardization
    df_clean['date'] = pd.to_datetime(df_clean['date'], errors='coerce')

    # Handle missing values
    df_clean['amount'].fillna(df_clean['amount'].median(), inplace=True)

    # Clean strings
    df_clean['product_name'] = df_clean['product_name'].str.strip()

    # Remove duplicates
    df_clean.drop_duplicates(inplace=True)

    # Filter invalid records
    df_clean = df_clean[df_clean['date'].notna()]
    df_clean = df_clean[df_clean['amount'] > 0]

    final_rows = len(df_clean)
    logger.info(f"[TRANSFORM] Completed: {initial_rows} ‚Üí {final_rows} rows")

    return df_clean

# ===== Load =====
@contextmanager
def get_db_connection(connection_string: str):
    """Database connection context manager"""
    engine = create_engine(connection_string)
    connection = engine.connect()
    try:
        yield connection
        connection.commit()
    except Exception as e:
        connection.rollback()
        logger.error(f"[LOAD] Transaction rolled back: {str(e)}")
        raise
    finally:
        connection.close()

def load_to_postgres(df: pd.DataFrame, connection_string: str, table_name: str) -> None:
    """Load data to PostgreSQL"""
    logger.info(f"[LOAD] Loading {len(df)} rows to {table_name}")

    try:
        with get_db_connection(connection_string) as conn:
            df.to_sql(
                name=table_name,
                con=conn,
                if_exists='append',
                index=False,
                chunksize=1000
            )

            result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}")).fetchone()
            logger.info(f"[LOAD] Success. Total rows in table: {result[0]}")

    except Exception as e:
        logger.error(f"[LOAD] Failed: {str(e)}")
        raise

# ===== Main Pipeline =====
def run_etl_pipeline(input_file: str, connection_string: str, table_name: str = 'sales') -> None:
    """Run complete ETL pipeline"""
    start_time = datetime.now()
    logger.info("=" * 50)
    logger.info("ETL Pipeline Started")
    logger.info("=" * 50)

    try:
        # Extract
        df_raw = extract_csv(input_file)

        # Transform
        df_clean = transform_data(df_raw)

        # Load
        load_to_postgres(df_clean, connection_string, table_name)

        elapsed_time = (datetime.now() - start_time).total_seconds()
        logger.info("=" * 50)
        logger.info(f"ETL Pipeline Completed Successfully ({elapsed_time:.2f}s)")
        logger.info("=" * 50)

    except Exception as e:
        logger.error(f"ETL Pipeline Failed: {str(e)}")
        raise

# ===== CLI =====
def main():
    parser = argparse.ArgumentParser(description='Sales Data ETL Pipeline')
    parser.add_argument('--input', required=True, help='Input CSV file path')
    parser.add_argument('--db', required=True, help='PostgreSQL connection string')
    parser.add_argument('--table', default='sales', help='Target table name')

    args = parser.parse_args()

    run_etl_pipeline(
        input_file=args.input,
        connection_string=args.db,
        table_name=args.table
    )

if __name__ == '__main__':
    main()
```

### ‰ΩøÁî®ÊñπÂºè

**Âü∑Ë°å ETL ÁÆ°Á∑ö**Ôºö
```bash
# ÂñÆÊ¨°Âü∑Ë°å
python etl_pipeline.py \
    --input data/sales_2025_01_30.csv \
    --db postgresql://user:pass@localhost:5432/mydb \
    --table sales

# Ë®≠ÂÆö Cron job ÊØèÂ§©Âü∑Ë°å
# Á∑®ËºØ crontab
crontab -e

# Êñ∞Â¢ûÊéíÁ®ãÔºàÊØèÂ§©ÂáåÊô® 2 ÈªûÂü∑Ë°åÔºâ
0 2 * * * /usr/bin/python3 /path/to/etl_pipeline.py --input /data/sales.csv --db postgresql://...
```

### Ëº∏Âá∫Êó•Ë™åÁØÑ‰æã

```
2025-10-30 02:00:00 - __main__ - INFO - ==================================================
2025-10-30 02:00:00 - __main__ - INFO - ETL Pipeline Started
2025-10-30 02:00:00 - __main__ - INFO - ==================================================
2025-10-30 02:00:01 - __main__ - INFO - [EXTRACT] Reading CSV from data/sales.csv
2025-10-30 02:00:02 - __main__ - INFO - [EXTRACT] Loaded 10000 rows, 5 columns
2025-10-30 02:00:02 - __main__ - INFO - [TRANSFORM] Starting data transformation
2025-10-30 02:00:05 - __main__ - INFO - [TRANSFORM] Completed: 10000 ‚Üí 9876 rows
2025-10-30 02:00:05 - __main__ - INFO - [LOAD] Loading 9876 rows to sales
2025-10-30 02:00:08 - __main__ - INFO - [LOAD] Success. Total rows in table: 50000
2025-10-30 02:00:08 - __main__ - INFO - ==================================================
2025-10-30 02:00:08 - __main__ - INFO - ETL Pipeline Completed Successfully (8.23s)
2025-10-30 02:00:08 - __main__ - INFO - ==================================================
```

---

## üîß AI Agent ÈÅ∏Êìá

### Êé®Ëñ¶ Agent

**Â†¥ÊôØÔºöETL ÁÆ°Á∑öÈñãÁôº**
```bash
# ÂàáÊèõÂà∞Ë≥áÊñôÂ∑•Á®ãÂ∏´ Agent
/agents:data-engineer

# ÊèèËø∞ÈúÄÊ±Ç
ÊàëÈúÄË¶ÅÂª∫Á´ã‰∏ÄÂÄã ETL ÁÆ°Á∑öÔºö
- ‰æÜÊ∫êÔºöCSV Ê™îÊ°àÔºàÈä∑ÂîÆË≥áÊñôÔºâ
- ËΩâÊèõÔºöÊ∏ÖÊ¥ó„ÄÅÊ†ºÂºèÂåñ„ÄÅÈ©óË≠â
- ÁõÆÊ®ôÔºöPostgreSQL Ë≥áÊñôÂ∫´

Ë´ãÂπ´ÊàëË®≠Ë®àÂÆåÊï¥ÁöÑ ETL ËÖ≥Êú¨
```

**ÁÇ∫‰ªÄÈ∫ºÈÅ∏ data-engineer AgentÔºü**
- ‚úÖ Â∞àÊ•≠ÁöÑË≥áÊñôÁÆ°Á∑öË®≠Ë®àÁü•Ë≠ò
- ‚úÖ Ëá™ÂãïËÄÉÊÖÆÈåØË™§ËôïÁêÜËàáÊó•Ë™å
- ‚úÖ ÈÅµÂæ™ ETL ÊúÄ‰Ω≥ÂØ¶Ë∏ê
- ‚úÖ ÁîüÊàêÈ´òÊïàËÉΩÁöÑÊâπÊ¨°ËôïÁêÜ‰ª£Á¢º

---

## ‚ö†Ô∏è Â∏∏Ë¶ãÈô∑Èò±ËàáËß£Ê±∫ÊñπÊ°à

### Èô∑Èò± 1ÔºöË®òÊÜ∂È´î‰∏çË∂≥

**ÂïèÈ°å**Ôºö
```python
# ÈåØË™§Ôºö‰∏ÄÊ¨°ËºâÂÖ• 10GB CSV
df = pd.read_csv('huge_file.csv')  # MemoryError!
```

**Ëß£Ê±∫ÊñπÊ°à**Ôºö
```python
# Ê≠£Á¢∫Ôºö‰ΩøÁî® chunk processing
chunk_size = 10000
for chunk in pd.read_csv('huge_file.csv', chunksize=chunk_size):
    process_chunk(chunk)
```

**AI ËºîÂä©**Ôºö
```
‰Ω†ÔºöÈÄôÂÄã CSV Ê™îÊ°àÊúâ 10GBÔºåPandas ÊúÉË®òÊÜ∂È´î‰∏çË∂≥

Claude CodeÔºö
ÊàëÊúÉÁîüÊàê chunk processing ‰ª£Á¢ºÔºö
- ÂàÜÊâπËÆÄÂèñÔºàÊØèÊ¨° 10,000 Á≠ÜÔºâ
- ÈÄêÊâπËôïÁêÜËàáËºâÂÖ•
- ÈÄ≤Â∫¶Áõ£Êéß
```

### Èô∑Èò± 2ÔºöË≥áÊñôÂûãÂà•ÈåØË™§

**ÂïèÈ°å**Ôºö
```python
# ÈåØË™§ÔºöËá™ÂãïÊé®Êñ∑ÂûãÂà•ÂèØËÉΩÈåØË™§
df = pd.read_csv('sales.csv')
# '123' Ë¢´Áï∂ÊàêÂ≠ó‰∏≤ÔºåÂØ¶ÈöõÊáâË©≤ÊòØÊï¥Êï∏
```

**Ëß£Ê±∫ÊñπÊ°à**Ôºö
```python
# Ê≠£Á¢∫ÔºöÊòéÁ¢∫ÊåáÂÆöË≥áÊñôÂûãÂà•
dtype = {
    'product_id': 'int64',
    'amount': 'float64',
    'date': 'str'  # ÂÖàËÆÄÊàêÂ≠ó‰∏≤ÔºåÂÜçÁî® pd.to_datetime ËΩâÊèõ
}
df = pd.read_csv('sales.csv', dtype=dtype)
```

### Èô∑Èò± 3ÔºöÈáçË§áËºâÂÖ•

**ÂïèÈ°å**Ôºö
```python
# ÈåØË™§ÔºöÊØèÊ¨°Âü∑Ë°åÈÉΩÈáçË§áËºâÂÖ•Áõ∏ÂêåË≥áÊñô
load_to_postgres(df, db, table='sales', if_exists='append')
```

**Ëß£Ê±∫ÊñπÊ°à**Ôºö
```python
# Ê≠£Á¢∫ÔºöÊ™¢Êü•ÊòØÂê¶Â∑≤ËºâÂÖ•
def is_already_loaded(connection, date):
    query = f"SELECT COUNT(*) FROM sales WHERE date = '{date}'"
    result = connection.execute(text(query)).fetchone()
    return result[0] > 0

if not is_already_loaded(conn, target_date):
    load_to_postgres(df, db, table='sales')
else:
    logger.info(f"Data for {target_date} already loaded, skipping")
```

### Èô∑Èò± 4ÔºöÁº∫‰πèÈåØË™§ËôïÁêÜ

**ÂïèÈ°å**Ôºö
```python
# ÈåØË™§ÔºöÊ≤íÊúâÈåØË™§ËôïÁêÜÔºå‰∏ÄÂÄãÈåØË™§Â∞éËá¥Êï¥ÂÄãÁÆ°Á∑öÂ§±Êïó
df = pd.read_csv('file.csv')
df_clean = transform(df)
load(df_clean)
```

**Ëß£Ê±∫ÊñπÊ°à**Ôºö
```python
# Ê≠£Á¢∫ÔºöÂÆåÂñÑÁöÑÈåØË™§ËôïÁêÜ
try:
    df = pd.read_csv('file.csv')
except FileNotFoundError:
    logger.error("File not found")
    send_alert("ETL Failed: File not found")
    sys.exit(1)

try:
    df_clean = transform(df)
except ValidationError as e:
    logger.error(f"Data validation failed: {e}")
    save_failed_records(df, 'failed_records.csv')
    sys.exit(1)

try:
    load(df_clean)
except DatabaseError as e:
    logger.error(f"Database load failed: {e}")
    rollback()
    sys.exit(1)
```

---

## üéì ÈÄ≤ÈöéÔºö‰ΩïÊôÇÈúÄË¶ÅÂ∞àÊ•≠ ETL Â∑•ÂÖ∑Ôºü

### Á∞°ÂñÆÂ†¥ÊôØ vs Ë§áÈõúÂ†¥ÊôØ

```
Â†¥ÊôØË§áÈõúÂ∫¶         Âª∫Ë≠∞Â∑•ÂÖ∑               ÁêÜÁî±
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ÂñÆ‰∏Ä CSV ËôïÁêÜ     Pandas + AI ÁîüÊàê       Âø´ÈÄü„ÄÅÂ§†Áî®
Ë≥áÊñôÈáè < 1GB      Python ËÖ≥Êú¨
Âü∑Ë°åÈ†ªÁéáÔºöÊØèÂ§©

Â§ö‰æÜÊ∫êÊï¥Âêà        Airflow / Prefect      Â∑•‰ΩúÊµÅÁ®ãÁ∑®Êéí
Ë≥áÊñôÈáè 1-10GB     + Python               ‰æùË≥¥ÁÆ°ÁêÜ
Âü∑Ë°åÈ†ªÁéáÔºöÊØèÂ∞èÊôÇ                         ÈåØË™§ÈáçË©¶

‰ºÅÊ•≠Á¥ö ETL        Â∞àÊ•≠Â∑•ÂÖ∑               ÂúñÂΩ¢Âåñ‰ªãÈù¢
Ë≥áÊñôÈáè > 10GB     (Talend, Informatica)  ‰ºÅÊ•≠ÊîØÊè¥
Ë§áÈõúËΩâÊèõÈÇèËºØ                             Áõ£ÊéßËàáÊ≤ªÁêÜ

Âç≥ÊôÇ‰∏≤ÊµÅ          Kafka + Flink          ‰ΩéÂª∂ÈÅ≤
ÊØ´ÁßíÁ¥öËôïÁêÜ        Spark Streaming        ÂàÜÊï£ÂºèËôïÁêÜ
```

### Ê±∫Á≠ñÊ®π

```
‰Ω†ÁöÑ ETL ÈúÄÊ±ÇÔºö

‚îú‚îÄ Ë≥áÊñôÈáè < 1GBÔºå‰æÜÊ∫ê < 3 ÂÄã
‚îÇ  ‚îî‚îÄ Áî® Pandas + AI ÁîüÊàêËÖ≥Êú¨ ‚úÖ
‚îÇ
‚îú‚îÄ Ë≥áÊñôÈáè 1-10GBÔºå‰æÜÊ∫ê 3-10 ÂÄã
‚îÇ  ‚îî‚îÄ Áî® Airflow Á∑®Êéí Python ËÖ≥Êú¨ ‚úÖ
‚îÇ
‚îî‚îÄ Ë≥áÊñôÈáè > 10GBÔºå‰æÜÊ∫ê > 10 ÂÄã
   ‚îî‚îÄ ÊâæÂ∞àÊ•≠Ë≥áÊñôÂ∑•Á®ãÂ∏´ üë®‚Äçüíª
      Êàñ‰ΩøÁî®‰ºÅÊ•≠Á¥ö ETL Â∑•ÂÖ∑
```

---

## üìä ÊïàËÉΩÂÑ™ÂåñÂª∫Ë≠∞

### ÂÑ™ÂåñÊäÄÂ∑ß

**1. ‰ΩøÁî® Chunk Processing**
```python
# ‰∏çË¶ÅÔºö‰∏ÄÊ¨°ËºâÂÖ•ÂÖ®ÈÉ®
df = pd.read_csv('huge.csv')

# Ë¶ÅÔºöÂàÜÊâπËôïÁêÜ
for chunk in pd.read_csv('huge.csv', chunksize=10000):
    process_and_load(chunk)
```

**2. ‰ΩøÁî®Ê≠£Á¢∫ÁöÑË≥áÊñôÂûãÂà•**
```python
# int64 ‚Üí int32 ÂèØÁØÄÁúÅ 50% Ë®òÊÜ∂È´î
df['id'] = df['id'].astype('int32')

# object ‚Üí category ÂèØÁØÄÁúÅ 90% Ë®òÊÜ∂È´îÔºàÈáçË§áÂÄºÂ§öÊôÇÔºâ
df['status'] = df['status'].astype('category')
```

**3. Âπ≥Ë°åËôïÁêÜ**
```python
from multiprocessing import Pool

def process_file(file_path):
    df = pd.read_csv(file_path)
    return transform_and_load(df)

# Âπ≥Ë°åËôïÁêÜÂ§öÂÄãÊ™îÊ°à
with Pool(4) as p:
    results = p.map(process_file, file_list)
```

**4. ‰ΩøÁî®Êõ¥Âø´ÁöÑÂ∑•ÂÖ∑**
```python
# Pandas Â§™ÊÖ¢ÔºüË©¶Ë©¶ Polars
import polars as pl

# ÈÄüÂ∫¶Âø´ 5-10 ÂÄç
df = pl.read_csv('large.csv')
df_clean = df.filter(pl.col('amount') > 0)
```

---

## üéØ ÂØ¶Êà∞Á∑¥Áøí

### Á∑¥Áøí 1ÔºöÁîüÊàê‰Ω†ÁöÑÁ¨¨‰∏ÄÂÄã ETL ÁÆ°Á∑ö

**‰ªªÂãô**Ôºö
```
1. Ê∫ñÂÇôÊ∏¨Ë©¶Ë≥áÊñôÔºàÊàñ‰ΩøÁî®ÁØÑ‰æã CSVÔºâ
2. Áî® Claude Code ÁîüÊàê ETL ËÖ≥Êú¨
3. Âü∑Ë°å‰∏¶È©óË≠âÁµêÊûú
4. Ê™¢Êü•Êó•Ë™åËº∏Âá∫
```

**ÊèêÁ§∫**Ôºö
```bash
# Ëàá Claude Code Â∞çË©±
ÊàëÊúâ‰∏ÄÂÄã CSV Ê™îÊ°àÂåÖÂê´‰ª•‰∏ãÊ¨Ñ‰ΩçÔºö
- dateÔºàÊó•ÊúüÔºåÊ†ºÂºè‰∏çÁµ±‰∏ÄÔºâ
- customer_idÔºàÂÆ¢Êà∂ IDÔºâ
- productÔºàÁî¢ÂìÅÂêçÁ®±Ôºâ
- amountÔºàÈáëÈ°çÔºåÊúâÁ©∫ÂÄºÔºâ

Ë´ãÂπ´ÊàëÁîüÊàê‰∏ÄÂÄã ETL ËÖ≥Êú¨Ôºö
1. ËÆÄÂèñ CSV
2. Ê®ôÊ∫ñÂåñÊó•ÊúüÊ†ºÂºè
3. Â°´ÂÖÖÈáëÈ°çÁ©∫ÂÄºÔºàÁî®‰∏≠‰ΩçÊï∏Ôºâ
4. ÁßªÈô§ÈáçË§áË®òÈåÑ
5. Ëº∏Âá∫Âà∞ cleaned_sales.csv
```

### Á∑¥Áøí 2ÔºöÂä†ÂÖ•Ë≥áÊñôÂìÅË≥™Ê™¢Êü•

**‰ªªÂãô**Ôºö
```
Êì¥Â±ï ETL ËÖ≥Êú¨ÔºåÂä†ÂÖ•Ôºö
- Ë≥áÊñôÂìÅË≥™Â†±ÂëäÔºàÁ©∫ÂÄº„ÄÅÁï∞Â∏∏ÂÄºÁµ±Ë®àÔºâ
- ËΩâÊèõÂâçÂæåÂ∞çÊØî
- ÂìÅË≥™ÈñæÂÄºË≠¶Âëä
```

---

## üìö Âª∂‰º∏Èñ±ËÆÄ

**Pandas Áõ∏Èóú**Ôºö
- [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/)
- [10 Minutes to Pandas](https://pandas.pydata.org/docs/user_guide/10min.html)

**ETL Â∑•ÂÖ∑**Ôºö
- [Apache Airflow](https://airflow.apache.org/) - ÊúÄÊµÅË°åÁöÑÈñãÊ∫êÂ∑•‰ΩúÊµÅÁ®ãÁ∑®ÊéíÂ∑•ÂÖ∑
- [Prefect](https://www.prefect.io/) - Áèæ‰ª£Âåñ ETL Â∑•ÂÖ∑
- [dbt](https://www.getdbt.com/) - Ë≥áÊñôËΩâÊèõÂ∑•ÂÖ∑

**ÈÄ≤ÈöéÊïàËÉΩ**Ôºö
- [Polars](https://pola-rs.github.io/polars/) - ÊØî Pandas Âø´ 5-10 ÂÄç
- [Dask](https://dask.org/) - Âπ≥Ë°åÈÅãÁÆóÊ°ÜÊû∂

---

## üéØ Â≠∏ÁøíÊ™¢Êü•Èªû

ÂÆåÊàêÊú¨Á´†ÂæåÔºå‰Ω†ÊáâË©≤ËÉΩÂ§†Ôºö

- [ ] Ë™™Êòé ETL ÁöÑ‰∏âÂÄãÈöéÊÆµÔºàExtract-Transform-LoadÔºâ
- [ ] Áî® AI ÁîüÊàêÁ∞°ÂñÆÁöÑ ETL ËÖ≥Êú¨
- [ ] ÁêÜËß£Â∏∏Ë¶ãÁöÑË≥áÊñôÊ∏ÖÊ¥óÊìç‰Ωú
- [ ] Áü•ÈÅì‰ΩïÊôÇÈúÄË¶ÅÂ∞àÊ•≠ ETL Â∑•ÂÖ∑
- [ ] ËÉΩÂ§†Âü∑Ë°åÂÆåÊï¥ÁöÑ CSV to Database ÁÆ°Á∑ö

---

**Á´†ÁØÄÁâàÊú¨**Ôºöv1.0
**ÊúÄÂæåÊõ¥Êñ∞**Ôºö2025-10-30
**È†êË®àÂ≠∏ÁøíÊôÇÈï∑**Ôºö15 ÂàÜÈêòÔºàÁêÜË´ñÔºâ+ 30 ÂàÜÈêòÔºàÂØ¶‰ΩúÔºâ
