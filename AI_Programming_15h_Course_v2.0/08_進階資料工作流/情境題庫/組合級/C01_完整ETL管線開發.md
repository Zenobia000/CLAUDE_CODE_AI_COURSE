# C01 - 完整 ETL 管線開發

## 📌 情境描述

### 業務背景
你的團隊負責一個電商平台的數據分析,每天需要從多個來源(MySQL訂單庫、MongoDB用戶行為、CSV供應商數據)整合數據到數據倉庫,供 BI 團隊分析。

### 現況痛點
- **數據來源分散**:訂單在 MySQL、行為在 MongoDB、供應商給 CSV/Excel
- **手動整合低效**:分析師每天花 2 小時複製貼上
- **數據不一致**:時區、貨幣單位、產品編碼各有標準
- **錯誤難追蹤**:某天數據錯了找不到是哪個環節出問題

### 任務目標
使用 Claude Code 協助開發一個自動化 ETL 管線,實現:
- 自動從多源提取數據
- 標準化轉換(時區、格式、編碼)
- 載入到數據倉庫並驗證
- 完整的錯誤處理和日誌
- 每日自動執行

---

## 🎯 學習目標

完成本情境後,你將能夠:

- [ ] **設計完整 ETL 架構**:理解 Extract-Transform-Load 各階段職責
- [ ] **多源數據提取**:從 SQL、NoSQL、文件系統讀取數據
- [ ] **數據轉換管線**:使用 Pandas 進行清洗、標準化、驗證
- [ ] **錯誤處理機制**:實作重試、日誌、告警
- [ ] **管線編排**:使用 Prefect/Airflow 或簡單 Cron 調度
- [ ] **數據品質驗證**:確保轉換後數據符合 Schema
- [ ] **CI/CD 整合**:管線代碼版本控制與測試

**時間估計**:2-3 小時

---

## 🏗️ 系統架構

```
┌─────────────┐  ┌─────────────┐  ┌─────────────┐
│   MySQL     │  │  MongoDB    │  │  CSV Files  │
│ (訂單數據)   │  │ (用戶行為)   │  │ (供應商)     │
└──────┬──────┘  └──────┬──────┘  └──────┬──────┘
       │                 │                 │
       └─────────────────┼─────────────────┘
                         ▼
                ┌─────────────────┐
                │  Extract Layer  │
                │  - 連接管理      │
                │  - 增量提取      │
                │  - 錯誤重試      │
                └────────┬────────┘
                         ▼
                ┌─────────────────┐
                │ Transform Layer │
                │  - 數據清洗      │
                │  - 格式標準化    │
                │  - Schema驗證    │
                │  - 數據增強      │
                └────────┬────────┘
                         ▼
                ┌─────────────────┐
                │   Load Layer    │
                │  - DuckDB寫入    │
                │  - 分區管理      │
                │  - 完整性檢查    │
                └────────┬────────┘
                         ▼
                ┌─────────────────┐
                │  Data Warehouse │
                │    (DuckDB)     │
                └─────────────────┘
                         │
                         ▼
                ┌─────────────────┐
                │   Monitoring    │
                │  - 執行日誌      │
                │  - 數據量統計    │
                │  - 異常告警      │
                └─────────────────┘
```

---

## 📋 核心任務

### 階段一:Extract 層實作(30 分鐘)

**任務**:
1. 設計通用的數據提取介面
2. 實作 MySQL 提取器(增量提取訂單)
3. 實作 MongoDB 提取器(讀取用戶行為)
4. 實作 CSV 提取器(讀取供應商文件)

**與 AI 協作要點**:
- "設計一個抽象的 DataExtractor 基類,支援增量提取"
- "實作 MySQLExtractor,使用 created_at 欄位做增量"
- "加入連接池管理和自動重試機制"

**檢查點**:
- [ ] 可以從三個來源獨立提取數據
- [ ] 增量提取邏輯正確(不重複讀取舊數據)
- [ ] 連接失敗會自動重試 3 次

---

### 階段二:Transform 層實作(45 分鐘)

**任務**:
1. 數據清洗(處理 NULL、異常值、重複)
2. 格式標準化(時區統一為 UTC、貨幣轉換)
3. Schema 驗證(使用 Pydantic 或 Pandera)
4. 數據增強(關聯產品類別、計算衍生指標)

**與 AI 協作要點**:
- "使用 Pandas 建立數據清洗管線"
- "實作時區轉換函數,將所有時間轉為 UTC"
- "用 Pandera 定義 Schema,驗證轉換後數據"

**檢查點**:
- [ ] NULL 值按業務規則處理(填充/刪除)
- [ ] 所有時間欄位統一為 UTC
- [ ] Schema 驗證通過,數據類型正確
- [ ] 產品編碼已標準化

---

### 階段三:Load 層實作(30 分鐘)

**任務**:
1. 設計 DuckDB 數據倉庫 Schema
2. 實作批次寫入邏輯
3. 實作分區策略(按日期分區)
4. 數據完整性驗證(行數、主鍵唯一性)

**與 AI 協作要點**:
- "設計訂單事實表和維度表 Schema"
- "實作 append 模式寫入,避免重複"
- "加入交易(transaction)確保寫入原子性"

**檢查點**:
- [ ] 數據成功寫入 DuckDB
- [ ] 分區結構正確(可以按日期查詢)
- [ ] 主鍵無重複
- [ ] 行數與來源一致

---

### 階段四:錯誤處理與監控(30 分鐘)

**任務**:
1. 實作結構化日誌記錄
2. 各階段錯誤處理策略
3. 數據品質監控指標
4. 執行結果告警(Email/Slack)

**與 AI 協作要點**:
- "使用 loguru 實作結構化日誌"
- "設計錯誤處理策略:哪些錯誤重試、哪些跳過、哪些中斷"
- "實作簡單的 Slack Webhook 告警"

**檢查點**:
- [ ] 日誌包含時間戳、階段、狀態、數據量
- [ ] 網絡錯誤會重試,Schema 錯誤會中斷
- [ ] 監控指標:提取行數、轉換失敗數、載入時長
- [ ] 失敗時會發送告警

---

### 階段五:編排與自動化(30 分鐘)

**任務**:
1. 將管線包裝為可執行腳本
2. 設計命令行介面(CLI)
3. 配置環境變數與參數
4. 設定 Cron 每日自動執行

**與 AI 協作要點**:
- "使用 Click 或 Typer 建立 CLI"
- "支援 --date 參數指定提取日期範圍"
- "寫一個 crontab 設定每天凌晨 2 點執行"

**檢查點**:
- [ ] 可以用 `python etl.py --date 2024-01-01` 執行
- [ ] 所有敏感資訊(DB密碼)存在 .env
- [ ] Cron 設定正確,每天自動執行
- [ ] 執行後有完整日誌可追蹤

---

## 💡 學習重點

### 1. ETL 設計模式

**關鍵概念**:
- **分層架構**:Extract、Transform、Load 獨立,易測試
- **增量提取**:只讀取新數據,提升效率
- **冪等性**:重複執行同一批數據結果一致

**AI 協作策略**:
- "解釋 ETL 中的冪等性設計原則"
- "比較全量提取 vs 增量提取的取捨"

---

### 2. 數據品質保證

**關鍵概念**:
- **Schema 驗證**:Pandera 或 Great Expectations
- **數據血緣**:追蹤每筆數據的來源
- **斷言測試**:在關鍵節點檢查數據特性

**AI 協作策略**:
- "用 Pandera 定義嚴格的數據 Schema"
- "實作數據血緣追蹤,記錄來源表和時間"

---

### 3. 錯誤處理最佳實踐

**關鍵概念**:
- **重試機制**:網絡錯誤 exponential backoff 重試
- **失敗隔離**:一筆數據失敗不影響其他
- **Dead Letter Queue**:無法處理的數據單獨存放

**AI 協作策略**:
- "設計三層錯誤處理策略:重試/跳過/中斷"
- "實作 DLQ 將失敗數據存入單獨表"

---

### 4. 效能優化

**關鍵概念**:
- **批次處理**:避免逐筆寫入
- **並行提取**:多源數據同時讀取
- **分區策略**:按日期分區加速查詢

**AI 協作策略**:
- "將單執行緒改為 concurrent.futures 並行"
- "分析瓶頸:是 I/O 還是 CPU 密集?"

---

## 🔧 技術棧建議

### 核心工具
- **數據處理**:Pandas / Polars(大數據)
- **數據庫連接**:SQLAlchemy(SQL)、PyMongo(MongoDB)
- **數據倉庫**:DuckDB(本地原型)、PostgreSQL(生產)
- **驗證框架**:Pandera、Pydantic
- **日誌**:loguru
- **CLI**:Click / Typer
- **編排**(選配):Prefect / Airflow

### 環境管理
```bash
# 建立虛擬環境
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
.venv\Scripts\activate     # Windows

# 安裝依賴
pip install pandas sqlalchemy pymongo duckdb pandera loguru click python-dotenv
```

---

## 📊 成功標準

完成後應該能夠:

### 功能完整性
- [ ] 自動從三個來源提取數據
- [ ] 數據正確轉換並通過 Schema 驗證
- [ ] 載入到數據倉庫且分區正確
- [ ] 完整的錯誤處理和日誌記錄
- [ ] 每日自動執行

### 品質指標
- [ ] **可靠性**:連續 7 天無失敗
- [ ] **效能**:整個管線 < 5 分鐘完成
- [ ] **數據準確性**:行數與來源 100% 一致
- [ ] **可觀測性**:日誌清晰,錯誤可追蹤

### 工程實踐
- [ ] 代碼結構清晰,分層明確
- [ ] 敏感資訊在 .env 中
- [ ] 有單元測試覆蓋關鍵函數
- [ ] README 說明如何運行

---

## 🚀 擴展挑戰

### 進階挑戰 1:實時 CDC 管線
改用 Debezium 或 DuckDB ATTACH 功能實現 Change Data Capture,實時同步資料庫變更。

**提示**:
- "研究 DuckDB ATTACH 功能連接 Postgres"
- "實作增量同步邏輯,只處理變更的行"

---

### 進階挑戰 2:使用 Prefect 編排
將管線遷移到 Prefect,實現任務依賴管理、失敗重試、執行狀態可視化。

**提示**:
- "將 Extract/Transform/Load 包裝為 Prefect Task"
- "定義 Flow 的依賴關係圖"
- "使用 Prefect Cloud 查看執行歷史"

---

### 進階挑戰 3:數據品質儀表板
使用 Great Expectations 建立數據品質規則,並用 Streamlit 建立儀表板展示品質指標。

**提示**:
- "用 Great Expectations 定義期望值(行數範圍、NULL比例)"
- "將驗證結果存入 DuckDB"
- "Streamlit 讀取驗證歷史繪製趨勢圖"

---

### 進階挑戰 4:多環境部署
實作開發/測試/生產環境配置,使用 Docker 容器化管線。

**提示**:
- "用 config.yaml 管理多環境配置"
- "Dockerfile 包含所有依賴"
- "docker-compose 同時啟動 DuckDB 和 ETL"

---

## 📚 相關資源

### 理論文件
- `理論/8.1_ETL管線快速原型.md` - ETL 基本概念

### 快速上手
- `快速上手/ETL管線模板.md` - 可立即使用的模板

### 工具參考
- `工具速查/README.md` - 常用工具決策樹

### 其他情境
- `B01_多源數據整合原型` - 簡化版多源提取
- `C04_資料品質監控系統` - 深入數據品質保證

---

## 💬 AI 提示詞範例

### 架構設計階段
```
我需要設計一個 ETL 管線,從 MySQL、MongoDB、CSV 三個來源
整合數據到 DuckDB。請幫我:

1. 設計分層架構(Extract/Transform/Load 各自職責)
2. 給出 Python 類別設計草圖
3. 建議錯誤處理策略

重點考量:
- 增量提取邏輯
- Schema 驗證時機
- 冪等性保證
```

### 實作階段
```
目前 Extract 層實作遇到問題:

```python
# 當前代碼
def extract_mysql(date_from):
    conn = mysql.connector.connect(...)
    cursor = conn.execute(f"SELECT * FROM orders WHERE date > {date_from}")
    return cursor.fetchall()
```

問題:
1. SQL injection 風險
2. 記憶體可能不足(fetchall)
3. 連接沒有關閉

請改進這段代碼,並解釋改進原因。
```

### 除錯階段
```
ETL 管線執行失敗,日誌如下:

[錯誤日誌]

請幫我:
1. 分析根本原因
2. 給出修復建議
3. 如何預防未來發生
```

---

**相關情境題**:
- 🟢 B01_多源數據整合原型 - 更簡化的入門版
- 🟡 C04_資料品質監控系統 - 關注數據品質保證
- 🟡 C05_混合資料處理工作流 - 結合批次與流式處理
