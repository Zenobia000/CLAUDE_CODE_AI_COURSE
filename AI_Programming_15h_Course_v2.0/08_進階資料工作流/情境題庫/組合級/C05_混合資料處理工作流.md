# C05 - 混合資料處理工作流

## 📌 情境描述

### 業務背景
你的電商平台需要處理兩類數據需求:
1. **批次處理**:每日訂單報表、庫存盤點(可以延遲,數據量大)
2. **實時處理**:用戶點擊追蹤、庫存告警、即時推薦(需要秒級響應)

目前的痛點是這兩類需求用完全不同的技術棧和代碼,維護成本高。

### 現況痛點
- **技術孤島**:批次用 Python + Airflow,實時用 Kafka + Spark Streaming
- **代碼重複**:相同的業務邏輯在批次和實時各寫一遍
- **數據一致性**:批次和實時計算結果常常不一致
- **學習曲線陡峭**:新人需要學習兩套完全不同的工具
- **除錯困難**:實時流出問題很難重現和除錯

### 任務目標
使用 Claude Code 協助開發混合數據處理工作流,實現:
- 統一 API(批次和實時用同一套代碼)
- Lambda 架構實踐(批次補全、實時增量、服務合併)
- 錯誤處理與回溯(流處理失敗可以用批次補救)
- 性能平衡(批次高吞吐、實時低延遲)
- 本地可除錯(模擬流數據進行測試)

---

## 🎯 學習目標

完成本情境後,你將能夠:

- [ ] **理解 Lambda 架構**:批次層、速度層、服務層的職責
- [ ] **統一處理邏輯**:DuckDB 同時支援批次和流式查詢
- [ ] **實時數據攝取**:模擬事件流(Kafka 或簡單 Queue)
- [ ] **增量處理**:只處理新增數據,避免全量重算
- [ ] **數據一致性**:批次和實時結果最終一致
- [ ] **錯誤恢復**:流處理失敗後批次補救
- [ ] **性能調優**:批次優化吞吐量、實時優化延遲

**時間估計**:3 小時

---

## 🏗️ 系統架構

```
┌─────────────────────────────────────────────────┐
│            Data Sources (數據來源)               │
├────────────┬────────────────────────────────────┤
│  Batch     │          Streaming                 │
│  - Orders  │  - Click events (Kafka topic)      │
│  - Logs    │  - Sensor data (MQTT)              │
│  - Files   │  - User actions (Websocket)        │
└──────┬─────┴──────────┬─────────────────────────┘
       │                │
       │  Batch Layer   │  Speed Layer (實時層)
       ▼                ▼
┌──────────────┐  ┌──────────────────┐
│ Batch ETL    │  │ Stream Processor │
│ - Full scan  │  │ - Incremental    │
│ - High acc.  │  │ - Low latency    │
│ - DuckDB     │  │ - Micro-batch    │
└──────┬───────┘  └────────┬─────────┘
       │                   │
       │  Batch Views      │  Real-time Views
       ▼                   ▼
┌──────────────┐  ┌──────────────────┐
│ DuckDB Batch │  │ DuckDB Streaming │
│ - daily_agg  │  │ - minute_agg     │
│ - Accurate   │  │ - Approximate    │
└──────┬───────┘  └────────┬─────────┘
       │                   │
       └─────────┬─────────┘
                 ▼
        ┌─────────────────┐
        │  Serving Layer  │
        │ - Query merger  │
        │ - Batch + RT    │
        │ - API endpoint  │
        └────────┬────────┘
                 ▼
        ┌─────────────────┐
        │   Applications  │
        │ - Dashboard     │
        │ - API consumers │
        │ - Alerts        │
        └─────────────────┘

        ┌─────────────────┐
        │  Reconciliation │
        │ - Compare B+RT  │
        │ - Backfill gaps │
        │ - Data quality  │
        └─────────────────┘
```

---

## 📋 核心任務

### 階段一:批次處理層實作(40 分鐘)

**任務**:
1. 設計批次 ETL 管線(訂單聚合、用戶行為分析)
2. 實作每日批次任務(計算當日統計)
3. 歷史數據回填(重算過去 N 天數據)
4. 批次視圖存儲(DuckDB 表)

**與 AI 協作要點**:
- "設計批次 ETL:讀取訂單表 → 按日期/產品聚合 → 寫入 daily_sales"
- "實作增量批次:只處理昨天的數據,避免全量重算"
- "歷史回填:backfill.py --start-date 2024-01-01 --end-date 2024-01-31"
- "批次優化:使用 DuckDB COPY 而非逐行插入"

**檢查點**:
- [ ] 批次任務可以處理每日訂單數據
- [ ] 聚合結果存入 `daily_sales` 表
- [ ] 支援歷史回填(重算過去數據)
- [ ] 批次執行 < 5 分鐘(10 萬筆訂單)

---

### 階段二:流式處理層實作(50 分鐘)

**任務**:
1. 模擬事件流(使用 Queue 或 Kafka)
2. 實作微批處理(每 10 秒處理一批事件)
3. 滑動窗口聚合(最近 1 分鐘、5 分鐘、15 分鐘)
4. 實時視圖更新(寫入 `realtime_sales` 表)

**與 AI 協作要點**:
- "使用 Python Queue 模擬事件流:每秒產生 10 個訂單事件"
- "實作消費者:每 10 秒批次處理 Queue 中的事件"
- "滑動窗口:用 DuckDB 計算最近 1 分鐘的銷售額"
- "Upsert 邏輯:相同窗口的統計覆蓋舊值"

**檢查點**:
- [ ] 事件流模擬運行(每秒 10 個事件)
- [ ] 微批處理每 10 秒執行一次
- [ ] 實時視圖正確更新(最近 1 分鐘銷售額)
- [ ] 延遲 < 10 秒(事件產生到統計可見)

---

### 階段三:服務層實作(合併查詢)(35 分鐘)

**任務**:
1. 設計查詢 API(get_sales(start_time, end_time))
2. 查詢路由邏輯(歷史查批次、最近查實時)
3. 結果合併(批次 + 實時拼接)
4. 快取策略(歷史數據快取,實時數據不快取)

**與 AI 協作要點**:
- "設計查詢函數:如果查詢範圍 < 1 小時前,查批次表;否則合併查詢"
- "實作合併邏輯:SELECT * FROM batch WHERE time < cutoff UNION ALL SELECT * FROM realtime"
- "快取批次查詢:@cache_for(3600) 快取 1 小時"
- "API 設計:GET /sales?start=2024-01-01T00:00:00&end=2024-01-15T23:59:59"

**檢查點**:
- [ ] API 可以查詢任意時間範圍銷售數據
- [ ] 歷史數據查詢命中批次表
- [ ] 最近數據查詢命中實時表
- [ ] 結果正確合併(無縫銜接)

---

### 階段四:數據一致性與校對(40 分鐘)

**任務**:
1. 批次與實時結果對比(檢測偏差)
2. 自動校對(實時結果用批次結果校正)
3. 缺失數據補填(流處理失敗的數據用批次補)
4. 一致性監控(追蹤偏差趨勢)

**與 AI 協作要點**:
- "實作校對任務:比較批次和實時在重疊時間的統計,計算偏差率"
- "偏差 > 10% 時告警:實時計算可能有問題"
- "自動補填:檢測實時表的時間空隙,用批次結果填補"
- "監控儀表板:展示批次 vs 實時偏差趨勢"

**檢查點**:
- [ ] 校對任務每小時執行一次
- [ ] 偏差 > 10% 時發送告警
- [ ] 缺失數據自動從批次補填
- [ ] 儀表板顯示一致性趨勢

---

### 階段五:錯誤處理與除錯(35 分鐘)

**任務**:
1. 流處理錯誤捕獲(事件格式錯誤、處理異常)
2. Dead Letter Queue(無法處理的事件單獨存放)
3. 重放機制(手動重放失敗事件)
4. 本地除錯模式(用文件模擬流數據)

**與 AI 協作要點**:
- "實作 try-except:事件處理失敗時寫入 DLQ"
- "DLQ 設計:存儲原始事件、錯誤訊息、時間戳"
- "重放功能:dlq_replay.py 讀取 DLQ 重新處理"
- "除錯模式:--replay-file events.jsonl 本地測試"

**檢查點**:
- [ ] 格式錯誤的事件不會導致程式崩潰
- [ ] 失敗事件寫入 DLQ(JSON 文件或表)
- [ ] 可以手動重放 DLQ 中的事件
- [ ] 本地可以用文件模擬流數據除錯

---

## 💡 學習重點

### 1. Lambda 架構詳解

**三層結構**:

| 層級 | 職責 | 特性 | 實作 |
|------|------|------|------|
| **Batch Layer** | 完整、準確的歷史計算 | 高吞吐、高延遲(小時級) | DuckDB 批次 ETL |
| **Speed Layer** | 實時增量計算 | 低延遲(秒級)、可能有誤差 | DuckDB 微批處理 |
| **Serving Layer** | 合併查詢 | 統一介面 | Query merger API |

**核心理念**:
- **Batch Layer**:不變性(Immutability)、容錯(重算即可)
- **Speed Layer**:近似(Approximation)、低延遲
- **最終一致性**:批次定期校正實時結果

**AI 協作策略**:
- "解釋 Lambda 架構為什麼需要兩套計算邏輯?"
- "什麼場景適合 Lambda 架構?什麼場景應該用純流式(Kappa 架構)?"

---

### 2. 批次與流式的統一

**DuckDB 的優勢**:
- 批次:直接查詢 Parquet 文件(高吞吐)
- 流式:微批處理內存數據(低延遲)
- 統一 SQL 語法(無需學習兩套 API)

**對比其他方案**:
```
Spark:
- 批次:Spark Batch(DataFrames)
- 流式:Spark Structured Streaming
- 優點:統一 API,成熟生態
- 缺點:資源重(需要集群)

Flink:
- 批次 + 流式統一(DataStream API)
- 優點:真正的流式處理,延遲極低
- 缺點:學習曲線陡峭,運維複雜

DuckDB(本情境選擇):
- 優點:輕量、本地運行、SQL 友好
- 缺點:不適合超大規模(PB 級)
```

**AI 協作策略**:
- "對比 DuckDB vs Spark vs Flink 在批流統一上的差異"
- "什麼規模的數據適合用 DuckDB?"

---

### 3. 滑動窗口聚合

**窗口類型**:
- **Tumbling Window**(翻滾窗口):固定大小,不重疊
  - 範例:每 1 分鐘聚合一次
- **Sliding Window**(滑動窗口):固定大小,有重疊
  - 範例:最近 5 分鐘(每 10 秒更新)
- **Session Window**(會話窗口):基於事件間隔
  - 範例:用戶無操作 30 秒視為會話結束

**DuckDB 實作滑動窗口**:
```sql
-- 最近 5 分鐘銷售額(每 10 秒更新)
SELECT
    window_start,
    SUM(amount) as total_sales
FROM (
    SELECT
        time,
        amount,
        time - INTERVAL '5 minutes' as window_start
    FROM orders
    WHERE time > NOW() - INTERVAL '5 minutes'
)
GROUP BY window_start
```

**AI 協作策略**:
- "用 DuckDB 實作滑動窗口:計算最近 1 分鐘的平均訂單金額"
- "解釋為什麼流式處理需要窗口(而不是處理所有歷史數據)"

---

### 4. 數據一致性保證

**一致性層級**:
- **強一致性**:實時和批次結果完全一致(難以實現)
- **最終一致性**:短期可能不一致,長期收斂(Lambda 架構目標)
- **因果一致性**:有依賴關係的操作保持順序

**校對策略**:
1. **定期對比**:每小時比較批次和實時結果
2. **偏差告警**:偏差 > 閾值時通知
3. **自動校正**:用批次結果覆蓋實時結果(T+1 校正)
4. **人工介入**:偏差持續存在時查根因

**AI 協作策略**:
- "設計校對任務:比較 batch_sales 和 realtime_sales 的偏差"
- "什麼情況會導致批次和實時不一致?(列出 5 個原因)"

---

## 🔧 技術棧建議

### 核心工具
- **批次處理**:DuckDB、Pandas、Polars
- **流式模擬**:Python Queue、threading
- **真實流式**(可選):Kafka、Redis Streams
- **任務調度**:Cron、APScheduler
- **API 框架**:FastAPI、Flask
- **監控**:Streamlit、Grafana

### 環境設定
```bash
# 安裝依賴
pip install duckdb pandas fastapi uvicorn apscheduler

# (可選)安裝 Kafka
pip install kafka-python

# 啟動 API 服務
uvicorn main:app --reload
```

---

## 📊 成功標準

完成後應該能夠:

### 功能完整性
- [ ] 批次層:每日處理歷史訂單,生成 `daily_sales`
- [ ] 速度層:實時處理訂單事件,生成 `realtime_sales`
- [ ] 服務層:API 無縫合併批次和實時數據
- [ ] 校對任務:每小時對比批次和實時偏差
- [ ] 錯誤處理:流處理失敗的事件進入 DLQ

### 性能指標
- [ ] **批次吞吐量**:10 萬筆訂單 < 5 分鐘
- [ ] **實時延遲**:事件產生到可查詢 < 10 秒
- [ ] **查詢性能**:查詢 1 天數據 < 100ms
- [ ] **一致性**:批次和實時偏差 < 5%

### 工程品質
- [ ] 代碼重用:批次和實時共用聚合邏輯
- [ ] 可測試:本地可以用文件模擬流數據
- [ ] 可觀測:日誌記錄處理進度和錯誤
- [ ] 可恢復:流處理失敗後可以用批次補救

---

## 🚀 擴展挑戰

### 進階挑戰 1:真實 Kafka 整合
將事件流從 Queue 遷移到 Kafka,實現分佈式流處理。

**提示**:
- "使用 kafka-python 消費 `orders` topic"
- "實作消費者群組(多進程並行消費)"
- "Offset 管理:定期 commit,失敗時可以重放"

---

### 進階挑戰 2:Exactly-Once 語義
確保每個事件只被處理一次(不重複、不遺漏)。

**提示**:
- "使用 Kafka 的 transactional producer"
- "實作冪等寫入:upsert based on event_id"
- "重複檢測:記錄已處理的 event_id"

---

### 進階挑戰 3:動態視圖合併
實作智能查詢優化器,自動決定查批次還是實時。

**提示**:
- "查詢優化器:分析查詢時間範圍,決定數據源"
- "成本模型:批次 vs 實時的查詢成本估算"
- "快取策略:批次結果快取,實時結果直查"

---

### 進階挑戰 4:遲到數據處理
處理時間戳在過去的事件(如設備離線後重新上線)。

**提示**:
- "設定 Watermark:允許最多 1 小時遲到"
- "遲到事件觸發批次重算相關窗口"
- "視覺化:標記哪些窗口被遲到事件更新過"

---

## 📚 相關資源

### 理論文件
- `理論/8.1_ETL管線快速原型.md` - 批次處理基礎
- `理論/8.3_資料分析自動化.md` - 自動化概念

### 快速上手
- `快速上手/ETL管線模板.md` - 批次管線模板

### 工具參考
- `工具速查/README.md` - DuckDB 使用參考

### 其他情境
- `C01_完整ETL管線開發` - 批次管線設計
- `C03_銷售數據儀表板` - 實時儀表板
- `C04_資料品質監控系統` - 監控設計

---

## 💬 AI 提示詞範例

### 架構設計階段
```
我需要同時處理批次和實時數據:
- 批次:每日訂單報表(可延遲 1 小時,數據量 10 萬筆)
- 實時:用戶點擊追蹤(需要秒級響應,每秒 100 個事件)

請幫我:
1. 設計 Lambda 架構(批次層、速度層、服務層)
2. 推薦技術棧(是否需要 Kafka? DuckDB 夠用嗎?)
3. 給出批次和實時的處理邏輯設計

重點考量:
- 代碼重用(批次和實時不要寫兩套)
- 數據一致性(最終一致即可)
- 易於除錯(本地可測試)
```

### 實作階段
```
我實作了微批處理,但性能不好:

```python
# 當前代碼(每 10 秒處理一次)
while True:
    events = queue.get_all()  # 獲取所有事件
    for event in events:
        process_one_event(event)  # 逐個處理
    time.sleep(10)
```

問題:
1. 逐個處理太慢(1 萬個事件需要 30 秒)
2. 睡眠 10 秒期間新事件堆積
3. 錯誤處理不完善(一個失敗全失敗)

請改進:批次寫入 DuckDB、並行處理、錯誤隔離。
```

### 一致性保證階段
```
批次和實時結果不一致:
- 批次統計:今日銷售額 $120,000
- 實時統計:今日銷售額 $118,500
- 偏差:~1.3%

可能原因:
1. 實時流可能丟失了部分事件?
2. 批次和實時的時區不同?
3. 實時使用近似算法(HyperLogLog)?

請幫我:
1. 設計校對邏輯,自動檢測偏差
2. 列出可能導致不一致的原因
3. 給出修復策略
```

### 除錯階段
```
流處理在生產環境出錯,但本地無法重現:

錯誤日誌:
KeyError: 'product_id' in event processing

問題:
1. 生產環境有格式錯誤的事件,但本地測試數據都正確
2. 不知道是哪個上游系統發送了錯誤事件
3. 失敗後整個流處理停止

請給出:
1. 如何捕獲原始錯誤事件(DLQ)
2. 如何本地重放生產環境的事件(除錯)
3. 如何防止一個錯誤事件影響所有處理
```

---

**相關情境題**:
- 🟢 B01_多源數據整合原型 - 數據整合基礎
- 🟡 C01_完整ETL管線開發 - 批次處理設計
- 🟡 C03_銷售數據儀表板 - 實時儀表板
- 🟡 C04_資料品質監控系統 - 數據品質保證
