# B02: RAG ç³»çµ±åŸå‹æ­å»º - 10 åˆ†é˜å»ºç«‹æ™ºèƒ½å•ç­”ç³»çµ±

## ğŸ“‹ æƒ…å¢ƒæè¿°

**ä½ çš„å›°æ“¾**ï¼š
```
åœ˜éšŠæŠ€è¡“æ–‡æª”æ•£è½å„è™•ï¼š
- README åœ¨ GitHub
- API æ–‡æª”åœ¨ Confluence
- å…§éƒ¨è¦ç¯„åœ¨ Google Docs
- éƒ¨ç½²æŒ‡å—åœ¨ Notion

æ–°äººå…¥è·ç¶“å¸¸å•ï¼š
- "XX æœå‹™æ€éº¼éƒ¨ç½²ï¼Ÿ"
- "é€™å€‹ API æ€éº¼ç”¨ï¼Ÿ"
- "éŒ¯èª¤ç¢¼ 500 æ˜¯ä»€éº¼æ„æ€ï¼Ÿ"

ä½ ç¸½æ˜¯å›ç­”ç›¸åŒçš„å•é¡Œï¼Œæµªè²»å¤§é‡æ™‚é–“ã€‚
```

**ä½ æƒ³è¦çš„**ï¼š
```
ä¸€å€‹æ™ºèƒ½å•ç­”ç³»çµ±ï¼š
- è¼¸å…¥å•é¡Œ â†’ è‡ªå‹•æ‰¾åˆ°ç­”æ¡ˆ
- é™„ä¸Šä¾†æºæ–‡æª”é€£çµ
- 10 åˆ†é˜å¿«é€Ÿæ­å»ºåŸå‹
```

---

## ğŸ¯ å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬é¡Œå¾Œï¼Œä½ å°‡èƒ½å¤ ï¼š
- [ ] ç†è§£ RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰çš„åŸºæœ¬åŸç†
- [ ] 10 åˆ†é˜æ­å»ºå¯é‹è¡Œçš„ RAG åŸå‹
- [ ] çŸ¥é“å¦‚ä½•èª¿æ•´æª¢ç´¢ç­–ç•¥æå‡æº–ç¢ºåº¦
- [ ] ç†è§£å¸¸è¦‹ RAG å·¥å…·çš„é¸æ“‡é‚è¼¯

**æ™‚é–“**ï¼š45-60 åˆ†é˜

---

## ğŸ—ï¸ RAG ç³»çµ±æ¶æ§‹é€Ÿè¦½

### ä»€éº¼æ˜¯ RAGï¼Ÿ

```
å‚³çµ± LLM çš„å•é¡Œï¼š
Q: "æˆ‘å€‘å…¬å¸çš„éƒ¨ç½²æµç¨‹æ˜¯ä»€éº¼ï¼Ÿ"
A: "æˆ‘ä¸çŸ¥é“ä½ å€‘å…¬å¸çš„å…·é«”æµç¨‹..." âŒ

RAG ç³»çµ±çš„è§£æ±ºæ–¹æ¡ˆï¼š
Q: "æˆ‘å€‘å…¬å¸çš„éƒ¨ç½²æµç¨‹æ˜¯ä»€éº¼ï¼Ÿ"
   â†“
1. æª¢ç´¢ï¼šåœ¨å…¬å¸æ–‡æª”ä¸­æœå°‹ç›¸é—œå…§å®¹
2. å¢å¼·ï¼šæŠŠæ‰¾åˆ°çš„å…§å®¹åŠ å…¥æç¤ºè©
3. ç”Ÿæˆï¼šLLM åŸºæ–¼æ–‡æª”å›ç­”å•é¡Œ
   â†“
A: "æ ¹æ“šéƒ¨ç½²æŒ‡å—æ–‡æª”ï¼Œæµç¨‹æ˜¯..." âœ…
   ä¾†æºï¼šdocs/deployment.md
```

### RAG ç³»çµ±çš„æ ¸å¿ƒçµ„ä»¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ–‡æª”         â”‚ â†’ åˆ‡å¡Š â†’ åµŒå…¥ â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (MD/PDF/TXT)â”‚                â”‚ å‘é‡è³‡æ–™åº«   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚ (Chroma)    â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†“ æª¢ç´¢
                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
ç”¨æˆ¶å•é¡Œ â†’ åµŒå…¥ â†’ ç›¸ä¼¼åº¦æª¢ç´¢ â†’ â”‚ ç›¸é—œæ–‡æª”ç‰‡æ®µ â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†“
                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚ LLM ç”Ÿæˆå›ç­” â”‚
                               â”‚ (GPT-4ç­‰)   â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†“
                                 ç­”æ¡ˆ + ä¾†æº
```

---

## ğŸ“ ä»»å‹™éšæ®µ

### éšæ®µ 1ï¼šç’°å¢ƒæº–å‚™ï¼ˆ5 åˆ†é˜ï¼‰

**ä»»å‹™ 1.1**ï¼šå®‰è£ä¾è³´

```bash
# å‰µå»ºå°ˆæ¡ˆç›®éŒ„
mkdir rag-prototype
cd rag-prototype

# å®‰è£æ ¸å¿ƒå¥—ä»¶
pip install langchain chromadb openai python-dotenv
```

**ä»»å‹™ 1.2**ï¼šè¨­å®š API Key

å‰µå»º `.env` æª”æ¡ˆï¼š
```bash
OPENAI_API_KEY=your_openai_api_key_here
```

**ä»»å‹™ 1.3**ï¼šæº–å‚™æ¸¬è©¦æ–‡æª”

å‰µå»º `docs/` ç›®éŒ„ï¼Œæ”¾å…¥ 5-10 å€‹ Markdown æ–‡æª”ï¼š
```bash
mkdir docs

# ç¯„ä¾‹ï¼šå‰µå»ºæ¸¬è©¦æ–‡æª”
cat > docs/deployment.md << 'EOF'
# éƒ¨ç½²æŒ‡å—

## ç’°å¢ƒè¦æ±‚
- Python 3.9+
- Docker 20.10+
- Kubernetes 1.24+

## éƒ¨ç½²æ­¥é©Ÿ
1. å»ºç½® Docker image
2. æ¨é€åˆ° registry
3. æ›´æ–° k8s manifest
4. åŸ·è¡Œ kubectl apply

## å¸¸è¦‹å•é¡Œ
- å¦‚æœé‡åˆ°æ¬Šé™éŒ¯èª¤ï¼Œæª¢æŸ¥ service account
- å¦‚æœ pod ç„¡æ³•å•Ÿå‹•ï¼Œæª¢æŸ¥ image pull secrets
EOF
```

---

### éšæ®µ 2ï¼šæ­å»ºæœ€å°åŸå‹ï¼ˆ10 åˆ†é˜ï¼‰

**ä»»å‹™ 2.1**ï¼šè¼‰å…¥æ–‡æª”

å‰µå»º `rag_simple.py`ï¼š
```python
import os
from dotenv import load_dotenv
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# Step 1: è¼‰å…¥æ–‡æª”
print("æ­£åœ¨è¼‰å…¥æ–‡æª”...")
loader = DirectoryLoader('docs/', glob="**/*.md")
documents = loader.load()
print(f"è¼‰å…¥äº† {len(documents)} å€‹æ–‡æª”")

# Step 2: åˆ‡åˆ†æ–‡æª”
print("æ­£åœ¨åˆ‡åˆ†æ–‡æª”...")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # æ¯å¡Š 1000 å­—å…ƒ
    chunk_overlap=200  # é‡ç–Š 200 å­—å…ƒï¼ˆé¿å…æˆªæ–·ï¼‰
)
chunks = text_splitter.split_documents(documents)
print(f"åˆ‡åˆ†ç‚º {len(chunks)} å€‹ç‰‡æ®µ")

# Step 3: å»ºç«‹å‘é‡è³‡æ–™åº«
print("æ­£åœ¨å»ºç«‹å‘é‡è³‡æ–™åº«...")
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"  # æŒä¹…åŒ–å„²å­˜
)

# Step 4: å»ºç«‹ QA éˆ
print("æ­£åœ¨å»ºç«‹ QA ç³»çµ±...")
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # æœ€ç°¡å–®çš„ç­–ç•¥ï¼šæŠŠæ‰€æœ‰æª¢ç´¢åˆ°çš„å…§å®¹å¡å…¥æç¤ºè©
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),  # æª¢ç´¢ top 3
    return_source_documents=True  # è¿”å›ä¾†æºæ–‡æª”
)

print("âœ… RAG ç³»çµ±å·²å°±ç·’ï¼")

# Step 5: æ¸¬è©¦å•ç­”
def ask(question):
    print(f"\nå•é¡Œï¼š{question}")
    result = qa_chain({"query": question})
    print(f"ç­”æ¡ˆï¼š{result['result']}")
    print(f"\nä¾†æºï¼š")
    for doc in result['source_documents']:
        print(f"  - {doc.metadata['source']}")

# äº’å‹•å¼å•ç­”
if __name__ == "__main__":
    while True:
        question = input("\nè«‹è¼¸å…¥å•é¡Œï¼ˆè¼¸å…¥ 'quit' çµæŸï¼‰ï¼š")
        if question.lower() == 'quit':
            break
        ask(question)
```

**ä»»å‹™ 2.2**ï¼šåŸ·è¡Œæ¸¬è©¦

```bash
python rag_simple.py
```

**é æœŸè¼¸å‡º**ï¼š
```
æ­£åœ¨è¼‰å…¥æ–‡æª”...
è¼‰å…¥äº† 10 å€‹æ–‡æª”
æ­£åœ¨åˆ‡åˆ†æ–‡æª”...
åˆ‡åˆ†ç‚º 45 å€‹ç‰‡æ®µ
æ­£åœ¨å»ºç«‹å‘é‡è³‡æ–™åº«...
æ­£åœ¨å»ºç«‹ QA ç³»çµ±...
âœ… RAG ç³»çµ±å·²å°±ç·’ï¼

è«‹è¼¸å…¥å•é¡Œï¼šæˆ‘å€‘çš„éƒ¨ç½²æµç¨‹æ˜¯ä»€éº¼ï¼Ÿ

å•é¡Œï¼šæˆ‘å€‘çš„éƒ¨ç½²æµç¨‹æ˜¯ä»€éº¼ï¼Ÿ
ç­”æ¡ˆï¼šæ ¹æ“šéƒ¨ç½²æŒ‡å—æ–‡æª”ï¼Œéƒ¨ç½²æµç¨‹å¦‚ä¸‹ï¼š
1. å»ºç½® Docker image
2. æ¨é€åˆ° registry
3. æ›´æ–° k8s manifest
4. åŸ·è¡Œ kubectl apply

ä¾†æºï¼š
  - docs/deployment.md
```

---

### éšæ®µ 3ï¼šæ¸¬è©¦èˆ‡é©—è­‰ï¼ˆ10 åˆ†é˜ï¼‰

**ä»»å‹™ 3.1**ï¼šæ¸¬è©¦å¤šç¨®å•é¡Œ

æº–å‚™æ¸¬è©¦å•é¡Œæ¸…å–®ï¼š
```python
test_questions = [
    "éƒ¨ç½²éœ€è¦å“ªäº›ç’°å¢ƒï¼Ÿ",
    "å¦‚æœ pod ç„¡æ³•å•Ÿå‹•æ€éº¼è¾¦ï¼Ÿ",
    "API èªè­‰æ€éº¼åšï¼Ÿ",
    "è³‡æ–™åº«é€£ç·šå­—ä¸²åœ¨å“ªè£¡è¨­å®šï¼Ÿ",
    "æ€éº¼å›æ»¾åˆ°ä¸Šä¸€å€‹ç‰ˆæœ¬ï¼Ÿ"
]

for q in test_questions:
    ask(q)
    print("\n" + "="*50)
```

**ä»»å‹™ 3.2**ï¼šè©•ä¼°å›ç­”å“è³ª

æª¢æŸ¥ä»¥ä¸‹æŒ‡æ¨™ï¼š
- [ ] ç­”æ¡ˆæ˜¯å¦æº–ç¢ºï¼Ÿ
- [ ] ä¾†æºæ–‡æª”æ˜¯å¦ç›¸é—œï¼Ÿ
- [ ] æ˜¯å¦æœ‰éºæ¼é‡è¦ä¿¡æ¯ï¼Ÿ
- [ ] å›ç­”æ˜¯å¦è‡ªç„¶æµæš¢ï¼Ÿ

**å•é¡Œè¨˜éŒ„ç¯„æœ¬**ï¼š
```
å•é¡Œ 1ï¼šéƒ¨ç½²éœ€è¦å“ªäº›ç’°å¢ƒï¼Ÿ
ç­”æ¡ˆï¼šâœ… æ­£ç¢º / âŒ éŒ¯èª¤ / âš ï¸ éƒ¨åˆ†æ­£ç¢º
ä¾†æºï¼šâœ… ç›¸é—œ / âŒ ä¸ç›¸é—œ
å‚™è¨»ï¼š___________
```

---

### éšæ®µ 4ï¼šå„ªåŒ–æª¢ç´¢ç­–ç•¥ï¼ˆ15 åˆ†é˜ï¼‰

**å•é¡Œ**ï¼šå¦‚æœå›ç­”ä¸æº–ç¢ºæ€éº¼è¾¦ï¼Ÿ

**å„ªåŒ–ç­–ç•¥ Aï¼šèª¿æ•´ chunk size**

```python
# åŸæœ¬ï¼šchunk_size=1000
# å•é¡Œï¼šå¤ªå¤§ â†’ åŒ…å«å¤ªå¤šç„¡é—œä¿¡æ¯
#      å¤ªå° â†’ å¯èƒ½æˆªæ–·é‡è¦ä¸Šä¸‹æ–‡

# å¯¦é©—ä¸åŒå¤§å°
for size in [500, 1000, 1500]:
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=size,
        chunk_overlap=200
    )
    # æ¸¬è©¦ä¸¦è¨˜éŒ„çµæœ
```

**å„ªåŒ–ç­–ç•¥ Bï¼šèª¿æ•´æª¢ç´¢æ•¸é‡**

```python
# åŸæœ¬ï¼šk=3 ï¼ˆæª¢ç´¢ top 3ï¼‰
# èª¿æ•´ï¼šk=5 ï¼ˆæª¢ç´¢ top 5ï¼‰

retriever = vectorstore.as_retriever(
    search_type="similarity",  # æˆ– "mmr" (æœ€å¤§é‚Šéš›ç›¸é—œæ€§)
    search_kwargs={"k": 5}
)
```

**å„ªåŒ–ç­–ç•¥ Cï¼šä½¿ç”¨ MMR æª¢ç´¢**

```python
# MMR (Maximal Marginal Relevance)ï¼š
# ä¸åªçœ‹ç›¸ä¼¼åº¦ï¼Œé‚„ç¢ºä¿å¤šæ¨£æ€§

retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 5,
        "fetch_k": 20,  # å…ˆå– 20 å€‹å€™é¸
        "lambda_mult": 0.7  # å¹³è¡¡ç›¸ä¼¼åº¦å’Œå¤šæ¨£æ€§
    }
)
```

**ä»»å‹™ 4.1**ï¼šå¯¦é©—å„ªåŒ–æ•ˆæœ

å‰µå»º `optimize_rag.py`ï¼š
```python
from rag_simple import ask

# æ¸¬è©¦å•é¡Œ
test_question = "å¦‚æœ pod ç„¡æ³•å•Ÿå‹•æ€éº¼è¾¦ï¼Ÿ"

# é…ç½® 1: å° chunk + k=3
print("=== é…ç½® 1: chunk_size=500, k=3 ===")
# æ¸¬è©¦...

# é…ç½® 2: ä¸­ chunk + k=5
print("=== é…ç½® 2: chunk_size=1000, k=5 ===")
# æ¸¬è©¦...

# é…ç½® 3: MMR æª¢ç´¢
print("=== é…ç½® 3: MMR retrieval ===")
# æ¸¬è©¦...

# è¨˜éŒ„å“ªå€‹é…ç½®æ•ˆæœæœ€å¥½
```

---

### éšæ®µ 5ï¼šåŠ å…¥ä¾†æºè¿½è¹¤ï¼ˆ10 åˆ†é˜ï¼‰

**ä»»å‹™ 5.1**ï¼šé¡¯ç¤ºè©³ç´°ä¾†æºä¿¡æ¯

å„ªåŒ– `ask()` å‡½æ•¸ï¼š
```python
def ask_with_sources(question):
    print(f"\nå•é¡Œï¼š{question}")
    result = qa_chain({"query": question})

    print(f"\nç­”æ¡ˆï¼š{result['result']}")

    print(f"\nğŸ“š ä¾†æºæ–‡æª”ï¼š")
    for i, doc in enumerate(result['source_documents'], 1):
        print(f"\n[ä¾†æº {i}] {doc.metadata['source']}")
        print(f"ç›¸é—œç‰‡æ®µï¼š")
        print(f"{doc.page_content[:200]}...")  # é¡¯ç¤ºå‰ 200 å­—å…ƒ
        print(f"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
```

**ä»»å‹™ 5.2**ï¼šåŠ å…¥ä¿¡å¿ƒåº¦è©•åˆ†ï¼ˆå¯é¸ï¼‰

```python
# åŸºæ–¼æª¢ç´¢çš„ç›¸ä¼¼åº¦åˆ†æ•¸è©•ä¼°ä¿¡å¿ƒåº¦
def ask_with_confidence(question):
    # æ‰‹å‹•æª¢ç´¢ä»¥å–å¾—åˆ†æ•¸
    docs_with_scores = vectorstore.similarity_search_with_score(question, k=3)

    print(f"\næª¢ç´¢çµæœä¿¡å¿ƒåº¦ï¼š")
    for doc, score in docs_with_scores:
        confidence = f"{(1 - score) * 100:.1f}%"  # è½‰æ›ç‚ºç™¾åˆ†æ¯”
        print(f"  {confidence} - {doc.metadata['source']}")

    # ç„¶å¾Œæ­£å¸¸å•ç­”
    result = qa_chain({"query": question})
    print(f"\nç­”æ¡ˆï¼š{result['result']}")
```

---

## âœ… æª¢æŸ¥é»

### æª¢æŸ¥é» 1ï¼šåŸå‹å¯é‹è¡Œ

- [ ] æˆåŠŸå®‰è£æ‰€æœ‰ä¾è³´
- [ ] æˆåŠŸè¼‰å…¥æ–‡æª”ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«
- [ ] å¯ä»¥å•å•é¡Œä¸¦å¾—åˆ°å›ç­”
- [ ] å›ç­”é™„æœ‰ä¾†æºæ–‡æª”

### æª¢æŸ¥é» 2ï¼šç†è§£æ ¸å¿ƒæ¦‚å¿µ

- [ ] ç†è§£ RAG çš„ä¸‰å€‹æ­¥é©Ÿï¼ˆæª¢ç´¢-å¢å¼·-ç”Ÿæˆï¼‰
- [ ] çŸ¥é“ä»€éº¼æ˜¯å‘é‡åµŒå…¥ï¼ˆembeddingï¼‰
- [ ] ç†è§£ chunk size çš„å½±éŸ¿
- [ ] çŸ¥é“å¦‚ä½•èª¿æ•´æª¢ç´¢åƒæ•¸

### æª¢æŸ¥é» 3ï¼šèƒ½ç¨ç«‹å„ªåŒ–

- [ ] èƒ½æ ¹æ“šå›ç­”å“è³ªèª¿æ•´ chunk size
- [ ] èƒ½é¸æ“‡åˆé©çš„æª¢ç´¢ç­–ç•¥ï¼ˆsimilarity vs MMRï¼‰
- [ ] èƒ½è©•ä¼°ä¸åŒé…ç½®çš„æ•ˆæœ
- [ ] çŸ¥é“ä½•æ™‚éœ€è¦æ›´è¤‡é›œçš„æ–¹æ¡ˆ

---

## ğŸ’¡ å¸¸è¦‹å•é¡Œèˆ‡è§£æ±º

### Q1ï¼šå›ç­”ç¶“å¸¸èªª"æˆ‘ä¸çŸ¥é“"

**åŸå› **ï¼š
- æ–‡æª”æ²’æœ‰ç›¸é—œä¿¡æ¯
- æª¢ç´¢æ²’æœ‰æ‰¾åˆ°ç›¸é—œç‰‡æ®µ
- chunk size å¤ªå°ï¼Œä¸Šä¸‹æ–‡ä¸å®Œæ•´

**è§£æ±º**ï¼š
```python
# 1. æª¢æŸ¥æª¢ç´¢çµæœ
docs = vectorstore.similarity_search(question, k=5)
for doc in docs:
    print(doc.page_content)  # çœ‹çœ‹æª¢ç´¢åˆ°ä»€éº¼

# 2. èª¿æ•´ chunk size
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,  # å¢å¤§ chunk
    chunk_overlap=300
)

# 3. å¢åŠ æª¢ç´¢æ•¸é‡
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
```

---

### Q2ï¼šå›ç­”åŒ…å«éŒ¯èª¤ä¿¡æ¯

**åŸå› **ï¼š
- æª¢ç´¢åˆ°ä¸ç›¸é—œçš„æ–‡æª”
- LLM ç”¢ç”Ÿå¹»è¦ºï¼ˆhallucinationï¼‰

**è§£æ±º**ï¼š
```python
# ä½¿ç”¨æ›´åš´æ ¼çš„æç¤ºè©
from langchain.prompts import PromptTemplate

template = """
è«‹åƒ…æ ¹æ“šä»¥ä¸‹æ–‡æª”å›ç­”å•é¡Œã€‚
å¦‚æœæ–‡æª”ä¸­æ²’æœ‰ç›¸é—œä¿¡æ¯ï¼Œè«‹æ˜ç¢ºèªª"æ–‡æª”ä¸­æ²’æœ‰ç›¸é—œä¿¡æ¯"ã€‚
ä¸è¦è‡ªè¡Œæ¨æ¸¬æˆ–æ·»åŠ æ–‡æª”ä¹‹å¤–çš„ä¿¡æ¯ã€‚

æ–‡æª”ï¼š
{context}

å•é¡Œï¼š{question}

ç­”æ¡ˆï¼š
"""

prompt = PromptTemplate(template=template, input_variables=["context", "question"])

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt}
)
```

---

### Q3ï¼šè™•ç†é€Ÿåº¦å¾ˆæ…¢

**åŸå› **ï¼š
- æ–‡æª”å¤ªå¤š
- åµŒå…¥è¨ˆç®—æ…¢
- å‘é‡è³‡æ–™åº«æœªæŒä¹…åŒ–

**è§£æ±º**ï¼š
```python
# 1. æŒä¹…åŒ–å‘é‡è³‡æ–™åº«ï¼ˆé¿å…é‡è¤‡åµŒå…¥ï¼‰
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)

# 2. ä½¿ç”¨æ›´å¿«çš„ embedding æ¨¡å‹
from langchain.embeddings import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# 3. é™åˆ¶æª¢ç´¢æ•¸é‡
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
```

---

### Q4ï¼šå¦‚ä½•æ”¯æ´å¤šç¨®æ–‡æª”æ ¼å¼ï¼Ÿ

**è§£æ±º**ï¼š
```python
# å®‰è£é¡å¤– loader
pip install pypdf docx2txt

# è¼‰å…¥ä¸åŒæ ¼å¼
from langchain.document_loaders import (
    DirectoryLoader,
    PyPDFLoader,
    Docx2txtLoader,
    TextLoader
)

# è‡ªå‹•è­˜åˆ¥æ ¼å¼
loaders = [
    DirectoryLoader('docs/', glob="**/*.md", loader_cls=TextLoader),
    DirectoryLoader('docs/', glob="**/*.pdf", loader_cls=PyPDFLoader),
    DirectoryLoader('docs/', glob="**/*.docx", loader_cls=Docx2txtLoader),
]

all_docs = []
for loader in loaders:
    all_docs.extend(loader.load())
```

---

## ğŸ¯ æ“´å±•æŒ‘æˆ°

### æŒ‘æˆ° 1ï¼šåŠ å…¥ CLI ä»‹é¢

è®“ç³»çµ±æ›´æ˜“ç”¨ï¼š
```bash
python rag_cli.py ask "éƒ¨ç½²æµç¨‹æ˜¯ä»€éº¼ï¼Ÿ"
python rag_cli.py search "éƒ¨ç½²"
python rag_cli.py add-docs ./new_docs/
```

### æŒ‘æˆ° 2ï¼šå»ºç«‹ Web ä»‹é¢

ä½¿ç”¨ Streamlitï¼š
```python
import streamlit as st

st.title("ä¼æ¥­æ–‡æª”å•ç­”ç³»çµ±")
question = st.text_input("è«‹è¼¸å…¥å•é¡Œï¼š")

if question:
    result = qa_chain({"query": question})
    st.write(result['result'])

    st.subheader("ä¾†æºæ–‡æª”ï¼š")
    for doc in result['source_documents']:
        st.write(f"- {doc.metadata['source']}")
```

### æŒ‘æˆ° 3ï¼šå¤šèªè¨€æ”¯æ´

æ”¯æ´ä¸­è‹±æ–‡æ··åˆæ–‡æª”ï¼š
```python
# ä½¿ç”¨å¤šèªè¨€ embedding æ¨¡å‹
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)
```

### æŒ‘æˆ° 4ï¼šå¢åŠ å°è©±è¨˜æ†¶

è®“ç³»çµ±è¨˜ä½å°è©±æ­·å²ï¼š
```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory
)

# ç¾åœ¨å¯ä»¥é€²è¡Œå¤šè¼ªå°è©±
qa_chain({"question": "éƒ¨ç½²æµç¨‹æ˜¯ä»€éº¼ï¼Ÿ"})
qa_chain({"question": "é‚£éœ€è¦å“ªäº›ç’°å¢ƒï¼Ÿ"})  # ç†è§£"é‚£"æŒ‡çš„æ˜¯éƒ¨ç½²
```

---

## ğŸ“š å»¶ä¼¸å­¸ç¿’

### æ¨è–¦é–±è®€

- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)
- [Chroma Documentation](https://docs.trychroma.com/)
- [Vector Database Comparison](https://github.com/erikbern/ann-benchmarks)

### å·¥å…·é¸æ“‡

```
å ´æ™¯                           æ¨è–¦å·¥å…·
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
å¿«é€ŸåŸå‹ï¼ˆæœ¬åœ°ï¼‰                Chroma
ç”Ÿç”¢ç’°å¢ƒï¼ˆé›²ç«¯ï¼‰                Pinecone, Weaviate
å¤§è¦æ¨¡ï¼ˆç™¾è¬æ–‡æª”+ï¼‰             Qdrant, Milvus
ä¼æ¥­ç§æœ‰éƒ¨ç½²                    Weaviate (self-hosted)
```

### é€²éšä¸»é¡Œ

- **Hybrid Search**ï¼šçµåˆé—œéµå­—æœå°‹å’Œå‘é‡æœå°‹
- **Re-ranking**ï¼šå°æª¢ç´¢çµæœé‡æ–°æ’åº
- **Query Expansion**ï¼šæ“´å±•ç”¨æˆ¶å•é¡Œä»¥æå‡å¬å›ç‡
- **Multi-hop Reasoning**ï¼šå¤šæ­¥æ¨ç†

---

## ğŸ“ ç¸½çµ

å®Œæˆæœ¬é¡Œå¾Œï¼Œä½ æ‡‰è©²ï¼š

**ç†è§£çš„æ¦‚å¿µ**ï¼š
- âœ… RAG ç³»çµ±çš„åŸºæœ¬æ¶æ§‹ï¼ˆæª¢ç´¢-å¢å¼·-ç”Ÿæˆï¼‰
- âœ… å‘é‡åµŒå…¥å’Œç›¸ä¼¼åº¦æª¢ç´¢çš„åŸç†
- âœ… chunk size å’Œæª¢ç´¢ç­–ç•¥çš„å½±éŸ¿

**æŒæ¡çš„æŠ€èƒ½**ï¼š
- âœ… 10 åˆ†é˜æ­å»º RAG åŸå‹
- âœ… èª¿æ•´åƒæ•¸å„ªåŒ–å›ç­”å“è³ª
- âœ… è©•ä¼°ä¸åŒé…ç½®çš„æ•ˆæœ

**å»ºç«‹çš„èªçŸ¥**ï¼š
- âœ… RAG é©ç”¨æ–¼ä¼æ¥­å…§éƒ¨æ–‡æª”å•ç­”
- âœ… åŸå‹æ­å»ºå¾ˆå¿«ï¼Œä½†å„ªåŒ–éœ€è¦æ™‚é–“
- âœ… å·¥å…·é¸æ“‡å–æ±ºæ–¼ä½¿ç”¨å ´æ™¯ï¼ˆæœ¬åœ° vs é›²ç«¯ï¼‰

---

**ä¸‹ä¸€æ­¥**ï¼š
- ç¹¼çºŒ B03ï¼šè‡ªå‹•åŒ–åˆ†æå ±å‘Šç”Ÿæˆ
- æˆ–æŒ‘æˆ°çµ„åˆç´š C02ï¼šä¼æ¥­æ–‡æª”å•ç­”ç³»çµ±ï¼ˆå®Œæ•´ç‰ˆï¼‰

**è¨˜ä½**ï¼š
> RAG ä¸æ˜¯é­”æ³•ï¼Œæ˜¯å·¥å…·ã€‚
> åŸå‹å¾ˆå¿«ï¼Œå„ªåŒ–éœ€è¦è€å¿ƒã€‚
> å¾ç°¡å–®é–‹å§‹ï¼Œé€æ­¥å„ªåŒ–ã€‚
