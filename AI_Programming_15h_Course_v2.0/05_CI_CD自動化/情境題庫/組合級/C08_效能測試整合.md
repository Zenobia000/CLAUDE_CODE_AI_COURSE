# C08ï¼šæ•ˆèƒ½æ¸¬è©¦æ•´åˆ â­

## ğŸ“‹ æƒ…å¢ƒè³‡è¨Š

**é›£åº¦ç­‰ç´š**ï¼šâ­â­ çµ„åˆç´š
**é ä¼°æ™‚é–“**ï¼š1.5-2 å°æ™‚
**æ ¸å¿ƒæŠ€èƒ½**ï¼šè² è¼‰æ¸¬è©¦ã€æ•ˆèƒ½åŸºæº–ã€å›æ­¸æª¢æ¸¬ã€è‡ªå‹•åŒ–æ±ºç­–
**å‰ç½®çŸ¥è­˜**ï¼šåŸºç¤ç´š B01-B03ã€çµ„åˆç´š C04

---

## ğŸ¯ æƒ…å¢ƒèƒŒæ™¯

ä½ æ˜¯æ•ˆèƒ½å·¥ç¨‹å¸«,è² è²¬å°‡æ•ˆèƒ½æ¸¬è©¦æ•´åˆåˆ° CI/CD ç®¡ç·šä¸­,ç¢ºä¿æ¯æ¬¡ç™¼å¸ƒéƒ½ä¸æœƒå¼•å…¥æ•ˆèƒ½å›æ­¸ã€‚

**ç—›é»**ï¼š
```bash
å•é¡Œï¼šç”Ÿç”¢ç’°å¢ƒæ•ˆèƒ½ç½é›£
- æ–°ç‰ˆæœ¬éƒ¨ç½²å¾Œ API å»¶é²å¾ 100ms â†’ 3000ms
- è³‡æ–™åº«é€£æ¥æ± è€—ç›¡
- è¨˜æ†¶é«”æ´©æ¼å°è‡´ OOM
- æ‰€æœ‰å•é¡Œéƒ½æ˜¯éƒ¨ç½²å¾Œæ‰ç™¼ç¾
```

**ç›®æ¨™**ï¼š
- P95 å»¶é² < 200ms
- ååé‡ > 1000 TPS
- è¨˜æ†¶é«”ä½¿ç”¨ < 512MB
- é›¶æ•ˆèƒ½å›æ­¸

---

## ğŸ¬ éšæ®µå±•é–‹

### éšæ®µ 1ï¼šè² è¼‰æ¸¬è©¦æ•´åˆï¼ˆ30 åˆ†é˜ï¼‰

**Locust æ¸¬è©¦è…³æœ¬**ï¼ˆ`tests/load/locustfile.py`ï¼‰ï¼š
```python
from locust import HttpUser, task, between
import random

class APIUser(HttpUser):
    wait_time = between(1, 3)
    
    def on_start(self):
        # ç™»å…¥
        response = self.client.post("/auth/login", json={
            "email": "loadtest@example.com",
            "password": "testpass"
        })
        self.token = response.json()["access_token"]
        self.headers = {"Authorization": f"Bearer {self.token}"}
    
    @task(3)
    def list_projects(self):
        """æŸ¥è©¢å°ˆæ¡ˆåˆ—è¡¨ï¼ˆæœ€å¸¸è¦‹æ“ä½œï¼‰"""
        with self.client.get(
            "/api/v1/projects",
            headers=self.headers,
            catch_response=True
        ) as response:
            if response.elapsed.total_seconds() > 0.2:
                response.failure(f"Too slow: {response.elapsed.total_seconds()}s")
    
    @task(2)
    def create_project(self):
        """å»ºç«‹å°ˆæ¡ˆ"""
        self.client.post(
            "/api/v1/projects",
            headers=self.headers,
            json={
                "name": f"Load Test Project {random.randint(1, 10000)}",
                "description": "Performance testing"
            }
        )
    
    @task(1)
    def get_project_details(self):
        """æŸ¥è©¢å°ˆæ¡ˆè©³æƒ…"""
        project_id = random.randint(1, 100)
        self.client.get(
            f"/api/v1/projects/{project_id}",
            headers=self.headers
        )
```

**CI Workflow æ•´åˆ**ï¼ˆ`.github/workflows/performance.yml`ï¼‰ï¼š
```yaml
name: Performance Test

on:
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 2 * * *'  # æ¯å¤©å‡Œæ™¨ 2 é»

jobs:
  performance-test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15
      redis:
        image: redis:7

    steps:
      - uses: actions/checkout@v4

      - name: Setup environment
        run: |
          # éƒ¨ç½²æ‡‰ç”¨åˆ°æ¸¬è©¦ç’°å¢ƒ
          docker-compose up -d

      - name: Load test data
        run: |
          # è¼‰å…¥æ¸¬è©¦è³‡æ–™ï¼ˆæ¨¡æ“¬çœŸå¯¦è² è¼‰ï¼‰
          python scripts/load_test_data.py --users=1000 --projects=5000

      - name: Install Locust
        run: pip install locust

      - name: Run load test
        run: |
          locust -f tests/load/locustfile.py \
            --host=http://localhost:8000 \
            --users=100 \
            --spawn-rate=10 \
            --run-time=5m \
            --headless \
            --csv=results/locust

      - name: Analyze results
        id: analyze
        run: python scripts/analyze_performance.py results/locust_stats.csv

      - name: Check SLO compliance
        run: |
          # æª¢æŸ¥æ•ˆèƒ½æŒ‡æ¨™æ˜¯å¦ç¬¦åˆ SLO
          P95=$(jq -r '.p95_response_time' results/analysis.json)
          TPS=$(jq -r '.requests_per_sec' results/analysis.json)
          ERROR_RATE=$(jq -r '.error_rate' results/analysis.json)
          
          if (( $(echo "$P95 > 200" | bc -l) )); then
            echo "âŒ P95 latency too high: ${P95}ms (SLO: 200ms)"
            exit 1
          fi
          
          if (( $(echo "$TPS < 1000" | bc -l) )); then
            echo "âŒ Throughput too low: ${TPS} TPS (SLO: 1000 TPS)"
            exit 1
          fi
          
          if (( $(echo "$ERROR_RATE > 0.01" | bc -l) )); then
            echo "âŒ Error rate too high: ${ERROR_RATE}% (SLO: 1%)"
            exit 1
          fi
          
          echo "âœ… All SLO checks passed"

      - name: Upload results
        uses: actions/upload-artifact@v3
        with:
          name: performance-results
          path: results/
```

**æª¢æŸ¥é» 1**ï¼š
- [ ] è² è¼‰æ¸¬è©¦è‡ªå‹•åŸ·è¡Œ
- [ ] SLO æŒ‡æ¨™æª¢æŸ¥é€šé
- [ ] çµæœè‡ªå‹•ä¸Šå‚³
- [ ] å¤±æ•—æ™‚ CI é˜»æ“‹

---

### éšæ®µ 2ï¼šæ•ˆèƒ½åŸºæº–èˆ‡å›æ­¸æª¢æ¸¬ï¼ˆ25 åˆ†é˜ï¼‰

**æ•ˆèƒ½åŸºæº–ç®¡ç†**ï¼ˆ`scripts/benchmark.py`ï¼‰ï¼š
```python
import json
from pathlib import Path

class PerformanceBenchmark:
    def __init__(self, baseline_file='performance_baseline.json'):
        self.baseline_file = Path(baseline_file)
        self.baseline = self.load_baseline()
    
    def load_baseline(self):
        if self.baseline_file.exists():
            return json.loads(self.baseline_file.read_text())
        return {}
    
    def save_baseline(self, metrics):
        self.baseline_file.write_text(json.dumps(metrics, indent=2))
    
    def detect_regression(self, current_metrics, threshold=0.1):
        """æª¢æ¸¬æ•ˆèƒ½å›æ­¸ï¼ˆ> 10% é€€åŒ–è¦–ç‚ºå›æ­¸ï¼‰"""
        regressions = []
        
        for endpoint, metrics in current_metrics.items():
            if endpoint not in self.baseline:
                continue
            
            baseline = self.baseline[endpoint]
            
            # æª¢æŸ¥å›æ‡‰æ™‚é–“
            p95_increase = (
                (metrics['p95'] - baseline['p95']) / baseline['p95']
            )
            if p95_increase > threshold:
                regressions.append({
                    'endpoint': endpoint,
                    'metric': 'p95_latency',
                    'baseline': baseline['p95'],
                    'current': metrics['p95'],
                    'increase': f"{p95_increase * 100:.1f}%"
                })
            
            # æª¢æŸ¥ååé‡
            tps_decrease = (
                (baseline['tps'] - metrics['tps']) / baseline['tps']
            )
            if tps_decrease > threshold:
                regressions.append({
                    'endpoint': endpoint,
                    'metric': 'throughput',
                    'baseline': baseline['tps'],
                    'current': metrics['tps'],
                    'decrease': f"{tps_decrease * 100:.1f}%"
                })
        
        return regressions

# ä½¿ç”¨ç¯„ä¾‹
benchmark = PerformanceBenchmark()
current = {
    '/api/v1/projects': {'p95': 185, 'tps': 1200},
    '/api/v1/tasks': {'p95': 95, 'tps': 2500}
}

regressions = benchmark.detect_regression(current)
if regressions:
    print("âŒ Performance regressions detected:")
    for r in regressions:
        print(f"  - {r['endpoint']} {r['metric']}: {r['baseline']} â†’ {r['current']}")
    exit(1)
else:
    print("âœ… No performance regressions")
    benchmark.save_baseline(current)
```

**è¶¨å‹¢åˆ†æ**ï¼ˆ`.github/workflows/performance-trend.yml`ï¼‰ï¼š
```yaml
name: Performance Trend Analysis

on:
  schedule:
    - cron: '0 0 * * 0'  # æ¯é€±æ—¥

jobs:
  trend-analysis:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Collect historical data
        run: |
          # æ”¶é›†éå» 30 å¤©çš„æ•ˆèƒ½è³‡æ–™
          python scripts/collect_historical_perf.py --days=30

      - name: Generate trend report
        run: |
          python scripts/generate_trend_report.py

      - name: Create performance dashboard
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = JSON.parse(fs.readFileSync('perf_trend.json'));
            
            const chartUrl = `https://quickchart.io/chart?c=${encodeURIComponent(JSON.stringify({
              type: 'line',
              data: {
                labels: report.dates,
                datasets: [{
                  label: 'P95 Latency (ms)',
                  data: report.p95_values
                }]
              }
            }))}`;
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'Weekly Performance Report',
              body: `![Performance Trend](${chartUrl})`
            });
```

**æª¢æŸ¥é» 2**ï¼š
- [ ] æ•ˆèƒ½åŸºæº–å»ºç«‹
- [ ] å›æ­¸æª¢æ¸¬æ­£å¸¸é‹ä½œ
- [ ] è¶¨å‹¢åˆ†æè‡ªå‹•ç”Ÿæˆ
- [ ] Dashboard æ›´æ–°

---

### éšæ®µ 3ï¼šè³‡æºç›£æ§ï¼ˆ20 åˆ†é˜ï¼‰

**è³‡æºç›£æ§è…³æœ¬**ï¼ˆ`scripts/monitor_resources.py`ï¼‰ï¼š
```python
import psutil
import time
import json

def monitor_resources(duration=300, interval=5):
    """ç›£æ§æ‡‰ç”¨è³‡æºä½¿ç”¨ï¼ˆ5 åˆ†é˜ï¼‰"""
    metrics = {
        'cpu': [],
        'memory': [],
        'disk_io': [],
        'network_io': []
    }
    
    process = psutil.Process()
    start_time = time.time()
    
    while time.time() - start_time < duration:
        # CPU ä½¿ç”¨ç‡
        metrics['cpu'].append(process.cpu_percent(interval=1))
        
        # è¨˜æ†¶é«”ä½¿ç”¨
        mem_info = process.memory_info()
        metrics['memory'].append(mem_info.rss / 1024 / 1024)  # MB
        
        # I/O çµ±è¨ˆ
        io_counters = process.io_counters()
        metrics['disk_io'].append({
            'read_mb': io_counters.read_bytes / 1024 / 1024,
            'write_mb': io_counters.write_bytes / 1024 / 1024
        })
        
        time.sleep(interval)
    
    # åˆ†æçµæœ
    analysis = {
        'cpu': {
            'avg': sum(metrics['cpu']) / len(metrics['cpu']),
            'max': max(metrics['cpu']),
            'p95': sorted(metrics['cpu'])[int(len(metrics['cpu']) * 0.95)]
        },
        'memory': {
            'avg_mb': sum(metrics['memory']) / len(metrics['memory']),
            'max_mb': max(metrics['memory']),
            'leaked': metrics['memory'][-1] > metrics['memory'][0] * 1.2
        }
    }
    
    return analysis

# æª¢æŸ¥è³‡æºæ´©æ¼
analysis = monitor_resources()
if analysis['memory']['leaked']:
    print("âŒ Potential memory leak detected")
    exit(1)
if analysis['memory']['max_mb'] > 512:
    print(f"âŒ Memory usage too high: {analysis['memory']['max_mb']:.1f}MB")
    exit(1)
```

**æª¢æŸ¥é» 3**ï¼š
- [ ] è³‡æºç›£æ§æ•´åˆ
- [ ] è¨˜æ†¶é«”æ´©æ¼æª¢æ¸¬
- [ ] CPU ä½¿ç”¨ç‡ç›£æ§
- [ ] å‘Šè­¦æ©Ÿåˆ¶å°±ç·’

---

### éšæ®µ 4ï¼šk6 é€²éšæ¸¬è©¦ï¼ˆ10 åˆ†é˜ï¼‰

**k6 æ¸¬è©¦è…³æœ¬**ï¼ˆ`tests/load/k6-test.js`ï¼‰ï¼š
```javascript
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate, Trend } from 'k6/metrics';

// è‡ªè¨‚æŒ‡æ¨™
const errorRate = new Rate('errors');
const projectCreationDuration = new Trend('project_creation_duration');

export const options = {
  stages: [
    { duration: '2m', target: 100 },   // Ramp up
    { duration: '5m', target: 100 },   // Steady state
    { duration: '2m', target: 200 },   // Spike
    { duration: '1m', target: 0 },     // Ramp down
  ],
  thresholds: {
    http_req_duration: ['p(95)<200'],  // 95% è«‹æ±‚ < 200ms
    errors: ['rate<0.01'],             // éŒ¯èª¤ç‡ < 1%
  },
};

export default function () {
  const res = http.post('http://localhost:8000/api/v1/projects', {
    name: 'K6 Test Project',
    description: 'Performance testing'
  }, {
    headers: { 'Authorization': `Bearer ${__ENV.TOKEN}` }
  });
  
  check(res, {
    'status is 201': (r) => r.status === 201,
    'response time < 200ms': (r) => r.timings.duration < 200,
  });
  
  errorRate.add(res.status !== 201);
  projectCreationDuration.add(res.timings.duration);
  
  sleep(1);
}
```

**æª¢æŸ¥é» 4**ï¼š
- [ ] k6 æ¸¬è©¦åŸ·è¡ŒæˆåŠŸ
- [ ] è‡ªè¨‚æŒ‡æ¨™æ”¶é›†
- [ ] é–¾å€¼æª¢æŸ¥é€šé
- [ ] çµæœè¼¸å‡ºæ­£ç¢º

---

## ğŸ¯ å­¸ç¿’æª¢æŸ¥é»

### æŠ€è¡“èƒ½åŠ›
- [ ] æ•´åˆè² è¼‰æ¸¬è©¦åˆ° CI
- [ ] å»ºç«‹æ•ˆèƒ½åŸºæº–
- [ ] å¯¦ç¾å›æ­¸æª¢æ¸¬
- [ ] å®Œæˆè³‡æºç›£æ§

### æ•ˆèƒ½æŒ‡æ¨™
- [ ] P95 å»¶é² < 200ms
- [ ] ååé‡ > 1000 TPS
- [ ] è¨˜æ†¶é«” < 512MB
- [ ] é›¶æ•ˆèƒ½å›æ­¸

---

## ğŸ’¡ å»¶ä¼¸æŒ‘æˆ°

### æŒ‘æˆ° 1ï¼šçœŸå¯¦æµé‡é‡æ”¾
- éŒ„è£½ç”Ÿç”¢ç’°å¢ƒæµé‡
- é‡æ”¾åˆ°æ¸¬è©¦ç’°å¢ƒ
- åˆ†æå·®ç•°

### æŒ‘æˆ° 2ï¼šåˆ†æ•£å¼è² è¼‰æ¸¬è©¦
- å¤šå€åŸŸè² è¼‰ç”Ÿæˆ
- å¤§è¦æ¨¡ä¸¦ç™¼æ¸¬è©¦
- æ¨¡æ“¬çœŸå¯¦ç”¨æˆ¶è¡Œç‚º

### æŒ‘æˆ° 3ï¼šAI é©…å‹•çš„æ•ˆèƒ½åˆ†æ
- ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’é æ¸¬æ•ˆèƒ½è¶¨å‹¢
- è‡ªå‹•è­˜åˆ¥ç“¶é ¸
- æä¾›å„ªåŒ–å»ºè­°

---

## ğŸ“š åƒè€ƒè³‡æº

- [Locust æ–‡æª”](https://docs.locust.io/)
- [k6 æ–‡æª”](https://k6.io/docs/)
- [Performance Testing Best Practices](https://github.com/microsoft/perfview)

---

**ä¸‹ä¸€æ­¥**ï¼šå®Œæˆå¾Œå¯æŒ‘æˆ° **C09ï¼šé€šçŸ¥èˆ‡ç›£æ§æ•´åˆ** æˆ– **E01ï¼šä¼æ¥­ç´šå®Œæ•´ DevOps ç®¡ç·š**ï¼
