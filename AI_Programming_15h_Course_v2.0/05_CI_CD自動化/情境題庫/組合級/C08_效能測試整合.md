# C08：效能測試整合 ⭐

## 📋 情境資訊

**難度等級**：⭐⭐ 組合級
**預估時間**：1.5-2 小時
**核心技能**：負載測試、效能基準、回歸檢測、自動化決策
**前置知識**：基礎級 B01-B03、組合級 C04

---

## 🎯 情境背景

你是效能工程師,負責將效能測試整合到 CI/CD 管線中,確保每次發布都不會引入效能回歸。

**痛點**：
```bash
問題：生產環境效能災難
- 新版本部署後 API 延遲從 100ms → 3000ms
- 資料庫連接池耗盡
- 記憶體洩漏導致 OOM
- 所有問題都是部署後才發現
```

**目標**：
- P95 延遲 < 200ms
- 吞吐量 > 1000 TPS
- 記憶體使用 < 512MB
- 零效能回歸

---

## 🎬 階段展開

### 階段 1：負載測試整合（30 分鐘）

**Locust 測試腳本**（`tests/load/locustfile.py`）：
```python
from locust import HttpUser, task, between
import random

class APIUser(HttpUser):
    wait_time = between(1, 3)
    
    def on_start(self):
        # 登入
        response = self.client.post("/auth/login", json={
            "email": "loadtest@example.com",
            "password": "testpass"
        })
        self.token = response.json()["access_token"]
        self.headers = {"Authorization": f"Bearer {self.token}"}
    
    @task(3)
    def list_projects(self):
        """查詢專案列表（最常見操作）"""
        with self.client.get(
            "/api/v1/projects",
            headers=self.headers,
            catch_response=True
        ) as response:
            if response.elapsed.total_seconds() > 0.2:
                response.failure(f"Too slow: {response.elapsed.total_seconds()}s")
    
    @task(2)
    def create_project(self):
        """建立專案"""
        self.client.post(
            "/api/v1/projects",
            headers=self.headers,
            json={
                "name": f"Load Test Project {random.randint(1, 10000)}",
                "description": "Performance testing"
            }
        )
    
    @task(1)
    def get_project_details(self):
        """查詢專案詳情"""
        project_id = random.randint(1, 100)
        self.client.get(
            f"/api/v1/projects/{project_id}",
            headers=self.headers
        )
```

**CI Workflow 整合**（`.github/workflows/performance.yml`）：
```yaml
name: Performance Test

on:
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 2 * * *'  # 每天凌晨 2 點

jobs:
  performance-test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15
      redis:
        image: redis:7

    steps:
      - uses: actions/checkout@v4

      - name: Setup environment
        run: |
          # 部署應用到測試環境
          docker-compose up -d

      - name: Load test data
        run: |
          # 載入測試資料（模擬真實負載）
          python scripts/load_test_data.py --users=1000 --projects=5000

      - name: Install Locust
        run: pip install locust

      - name: Run load test
        run: |
          locust -f tests/load/locustfile.py \
            --host=http://localhost:8000 \
            --users=100 \
            --spawn-rate=10 \
            --run-time=5m \
            --headless \
            --csv=results/locust

      - name: Analyze results
        id: analyze
        run: python scripts/analyze_performance.py results/locust_stats.csv

      - name: Check SLO compliance
        run: |
          # 檢查效能指標是否符合 SLO
          P95=$(jq -r '.p95_response_time' results/analysis.json)
          TPS=$(jq -r '.requests_per_sec' results/analysis.json)
          ERROR_RATE=$(jq -r '.error_rate' results/analysis.json)
          
          if (( $(echo "$P95 > 200" | bc -l) )); then
            echo "❌ P95 latency too high: ${P95}ms (SLO: 200ms)"
            exit 1
          fi
          
          if (( $(echo "$TPS < 1000" | bc -l) )); then
            echo "❌ Throughput too low: ${TPS} TPS (SLO: 1000 TPS)"
            exit 1
          fi
          
          if (( $(echo "$ERROR_RATE > 0.01" | bc -l) )); then
            echo "❌ Error rate too high: ${ERROR_RATE}% (SLO: 1%)"
            exit 1
          fi
          
          echo "✅ All SLO checks passed"

      - name: Upload results
        uses: actions/upload-artifact@v3
        with:
          name: performance-results
          path: results/
```

**檢查點 1**：
- [ ] 負載測試自動執行
- [ ] SLO 指標檢查通過
- [ ] 結果自動上傳
- [ ] 失敗時 CI 阻擋

---

### 階段 2：效能基準與回歸檢測（25 分鐘）

**效能基準管理**（`scripts/benchmark.py`）：
```python
import json
from pathlib import Path

class PerformanceBenchmark:
    def __init__(self, baseline_file='performance_baseline.json'):
        self.baseline_file = Path(baseline_file)
        self.baseline = self.load_baseline()
    
    def load_baseline(self):
        if self.baseline_file.exists():
            return json.loads(self.baseline_file.read_text())
        return {}
    
    def save_baseline(self, metrics):
        self.baseline_file.write_text(json.dumps(metrics, indent=2))
    
    def detect_regression(self, current_metrics, threshold=0.1):
        """檢測效能回歸（> 10% 退化視為回歸）"""
        regressions = []
        
        for endpoint, metrics in current_metrics.items():
            if endpoint not in self.baseline:
                continue
            
            baseline = self.baseline[endpoint]
            
            # 檢查回應時間
            p95_increase = (
                (metrics['p95'] - baseline['p95']) / baseline['p95']
            )
            if p95_increase > threshold:
                regressions.append({
                    'endpoint': endpoint,
                    'metric': 'p95_latency',
                    'baseline': baseline['p95'],
                    'current': metrics['p95'],
                    'increase': f"{p95_increase * 100:.1f}%"
                })
            
            # 檢查吞吐量
            tps_decrease = (
                (baseline['tps'] - metrics['tps']) / baseline['tps']
            )
            if tps_decrease > threshold:
                regressions.append({
                    'endpoint': endpoint,
                    'metric': 'throughput',
                    'baseline': baseline['tps'],
                    'current': metrics['tps'],
                    'decrease': f"{tps_decrease * 100:.1f}%"
                })
        
        return regressions

# 使用範例
benchmark = PerformanceBenchmark()
current = {
    '/api/v1/projects': {'p95': 185, 'tps': 1200},
    '/api/v1/tasks': {'p95': 95, 'tps': 2500}
}

regressions = benchmark.detect_regression(current)
if regressions:
    print("❌ Performance regressions detected:")
    for r in regressions:
        print(f"  - {r['endpoint']} {r['metric']}: {r['baseline']} → {r['current']}")
    exit(1)
else:
    print("✅ No performance regressions")
    benchmark.save_baseline(current)
```

**趨勢分析**（`.github/workflows/performance-trend.yml`）：
```yaml
name: Performance Trend Analysis

on:
  schedule:
    - cron: '0 0 * * 0'  # 每週日

jobs:
  trend-analysis:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Collect historical data
        run: |
          # 收集過去 30 天的效能資料
          python scripts/collect_historical_perf.py --days=30

      - name: Generate trend report
        run: |
          python scripts/generate_trend_report.py

      - name: Create performance dashboard
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = JSON.parse(fs.readFileSync('perf_trend.json'));
            
            const chartUrl = `https://quickchart.io/chart?c=${encodeURIComponent(JSON.stringify({
              type: 'line',
              data: {
                labels: report.dates,
                datasets: [{
                  label: 'P95 Latency (ms)',
                  data: report.p95_values
                }]
              }
            }))}`;
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'Weekly Performance Report',
              body: `![Performance Trend](${chartUrl})`
            });
```

**檢查點 2**：
- [ ] 效能基準建立
- [ ] 回歸檢測正常運作
- [ ] 趨勢分析自動生成
- [ ] Dashboard 更新

---

### 階段 3：資源監控（20 分鐘）

**資源監控腳本**（`scripts/monitor_resources.py`）：
```python
import psutil
import time
import json

def monitor_resources(duration=300, interval=5):
    """監控應用資源使用（5 分鐘）"""
    metrics = {
        'cpu': [],
        'memory': [],
        'disk_io': [],
        'network_io': []
    }
    
    process = psutil.Process()
    start_time = time.time()
    
    while time.time() - start_time < duration:
        # CPU 使用率
        metrics['cpu'].append(process.cpu_percent(interval=1))
        
        # 記憶體使用
        mem_info = process.memory_info()
        metrics['memory'].append(mem_info.rss / 1024 / 1024)  # MB
        
        # I/O 統計
        io_counters = process.io_counters()
        metrics['disk_io'].append({
            'read_mb': io_counters.read_bytes / 1024 / 1024,
            'write_mb': io_counters.write_bytes / 1024 / 1024
        })
        
        time.sleep(interval)
    
    # 分析結果
    analysis = {
        'cpu': {
            'avg': sum(metrics['cpu']) / len(metrics['cpu']),
            'max': max(metrics['cpu']),
            'p95': sorted(metrics['cpu'])[int(len(metrics['cpu']) * 0.95)]
        },
        'memory': {
            'avg_mb': sum(metrics['memory']) / len(metrics['memory']),
            'max_mb': max(metrics['memory']),
            'leaked': metrics['memory'][-1] > metrics['memory'][0] * 1.2
        }
    }
    
    return analysis

# 檢查資源洩漏
analysis = monitor_resources()
if analysis['memory']['leaked']:
    print("❌ Potential memory leak detected")
    exit(1)
if analysis['memory']['max_mb'] > 512:
    print(f"❌ Memory usage too high: {analysis['memory']['max_mb']:.1f}MB")
    exit(1)
```

**檢查點 3**：
- [ ] 資源監控整合
- [ ] 記憶體洩漏檢測
- [ ] CPU 使用率監控
- [ ] 告警機制就緒

---

### 階段 4：k6 進階測試（10 分鐘）

**k6 測試腳本**（`tests/load/k6-test.js`）：
```javascript
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate, Trend } from 'k6/metrics';

// 自訂指標
const errorRate = new Rate('errors');
const projectCreationDuration = new Trend('project_creation_duration');

export const options = {
  stages: [
    { duration: '2m', target: 100 },   // Ramp up
    { duration: '5m', target: 100 },   // Steady state
    { duration: '2m', target: 200 },   // Spike
    { duration: '1m', target: 0 },     // Ramp down
  ],
  thresholds: {
    http_req_duration: ['p(95)<200'],  // 95% 請求 < 200ms
    errors: ['rate<0.01'],             // 錯誤率 < 1%
  },
};

export default function () {
  const res = http.post('http://localhost:8000/api/v1/projects', {
    name: 'K6 Test Project',
    description: 'Performance testing'
  }, {
    headers: { 'Authorization': `Bearer ${__ENV.TOKEN}` }
  });
  
  check(res, {
    'status is 201': (r) => r.status === 201,
    'response time < 200ms': (r) => r.timings.duration < 200,
  });
  
  errorRate.add(res.status !== 201);
  projectCreationDuration.add(res.timings.duration);
  
  sleep(1);
}
```

**檢查點 4**：
- [ ] k6 測試執行成功
- [ ] 自訂指標收集
- [ ] 閾值檢查通過
- [ ] 結果輸出正確

---

## 🎯 學習檢查點

### 技術能力
- [ ] 整合負載測試到 CI
- [ ] 建立效能基準
- [ ] 實現回歸檢測
- [ ] 完成資源監控

### 效能指標
- [ ] P95 延遲 < 200ms
- [ ] 吞吐量 > 1000 TPS
- [ ] 記憶體 < 512MB
- [ ] 零效能回歸

---

## 💡 延伸挑戰

### 挑戰 1：真實流量重放
- 錄製生產環境流量
- 重放到測試環境
- 分析差異

### 挑戰 2：分散式負載測試
- 多區域負載生成
- 大規模並發測試
- 模擬真實用戶行為

### 挑戰 3：AI 驅動的效能分析
- 使用機器學習預測效能趨勢
- 自動識別瓶頸
- 提供優化建議

---

## 📚 參考資源

- [Locust 文檔](https://docs.locust.io/)
- [k6 文檔](https://k6.io/docs/)
- [Performance Testing Best Practices](https://github.com/microsoft/perfview)

---

**下一步**：完成後可挑戰 **C09：通知與監控整合** 或 **E01：企業級完整 DevOps 管線**！
