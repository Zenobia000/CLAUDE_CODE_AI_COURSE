# B06ï¼šTDD é–‹ç™¼æ–‡å­—åˆ†æå™¨

## ğŸ“‹ æƒ…å¢ƒæè¿°

ä½ è¦é–‹ç™¼ä¸€å€‹æ–‡å­—åˆ†æå·¥å…·ï¼Œç”¨æ–¼åˆ†ææ–‡æª”å…§å®¹ã€çµ±è¨ˆè©é »ã€æå–é—œéµè³‡è¨Šç­‰ã€‚é€™å€‹å·¥å…·éœ€è¦è™•ç†å„ç¨®æ–‡å­—è™•ç†çš„éœ€æ±‚ã€‚

**éœ€æ±‚**ï¼š
æ–‡å­—åˆ†æå™¨éœ€è¦æ”¯æ´ï¼š
- åŸºæœ¬çµ±è¨ˆï¼ˆå­—æ•¸ã€è©æ•¸ã€è¡Œæ•¸ï¼‰
- è©é »åˆ†æï¼ˆæœ€é«˜é »è©å½™ã€è©é »åˆ†å¸ƒï¼‰
- æ–‡å­—æ¸…ç†ï¼ˆç§»é™¤æ¨™é»ã€è½‰å°å¯«ã€éæ¿¾åœç”¨è©ï¼‰
- é—œéµè©æå–
- æ–‡å­—ç›¸ä¼¼åº¦æ¯”è¼ƒ
- ç°¡å–®çš„æƒ…æ„Ÿåˆ†æ

**ä»»å‹™**ï¼š
ç”¨ TDD æ–¹å¼å¯¦ä½œé€™å€‹æ–‡å­—åˆ†æå™¨ï¼Œé‡é»é—œæ³¨æ¼”ç®—æ³•é‚è¼¯çš„æ¸¬è©¦ã€‚

**æ™‚é–“ä¼°è¨ˆ**ï¼š35-45 åˆ†é˜

---

## ğŸ¯ å­¸ç¿’ç›®æ¨™

- [ ] å­¸ç¿’æ¼”ç®—æ³•é‚è¼¯çš„ TDD å¯¦ä½œ
- [ ] æŒæ¡å­—ä¸²è™•ç†å’Œè³‡æ–™åˆ†æçš„æ¸¬è©¦
- [ ] ç·´ç¿’é‚Šç•Œæƒ…æ³å’Œç•°å¸¸è¼¸å…¥çš„è™•ç†
- [ ] é«”é©—æ–‡å­—è™•ç†é ˜åŸŸçš„å®Œæ•´é–‹ç™¼

---

## ğŸ› ï¸ æŠ€è¡“è¦æ±‚

**èªè¨€**ï¼šPython 3.11+
**æ¸¬è©¦æ¡†æ¶**ï¼špytest
**æª”æ¡ˆçµæ§‹**ï¼š
```
text_analyzer/
â”œâ”€â”€ text_analyzer.py       # ä¸»è¦å¯¦ä½œ
â”œâ”€â”€ text_cleaner.py       # æ–‡å­—æ¸…ç†
â”œâ”€â”€ sentiment_analyzer.py # æƒ…æ„Ÿåˆ†æ
â””â”€â”€ test_text_analyzer.py # æ¸¬è©¦æª”æ¡ˆ
```

---

## ğŸ“ å¯¦ä½œæ­¥é©Ÿ

### æº–å‚™å·¥ä½œ

**å»ºç«‹å°ˆæ¡ˆç›®éŒ„**ï¼š
```bash
mkdir -p text_analyzer
cd text_analyzer

# å»ºç«‹è™›æ“¬ç’°å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# å®‰è£ pytest
pip install pytest
```

**å»ºç«‹æª”æ¡ˆ**ï¼š
```bash
touch text_analyzer.py
touch text_cleaner.py
touch sentiment_analyzer.py
touch test_text_analyzer.py
```

---

### å¾ªç’° 1ï¼šåŸºæœ¬æ–‡å­—çµ±è¨ˆ

#### ğŸ”´ REDï¼šæ¸¬è©¦åŸºæœ¬çµ±è¨ˆåŠŸèƒ½

**test_text_analyzer.py**ï¼š
```python
import pytest
from text_analyzer import TextAnalyzer
from text_cleaner import TextCleaner
from sentiment_analyzer import SentimentAnalyzer

def test_count_characters():
    """æ¸¬è©¦å­—å…ƒæ•¸çµ±è¨ˆ"""
    analyzer = TextAnalyzer()

    result = analyzer.count_characters("Hello World!")

    assert result == 12

def test_count_characters_with_spaces():
    """æ¸¬è©¦åŒ…å«ç©ºæ ¼çš„å­—å…ƒçµ±è¨ˆ"""
    analyzer = TextAnalyzer()

    total_chars = analyzer.count_characters("Hello World!")
    chars_no_spaces = analyzer.count_characters_no_spaces("Hello World!")

    assert total_chars == 12
    assert chars_no_spaces == 10

def test_count_words():
    """æ¸¬è©¦è©æ•¸çµ±è¨ˆ"""
    analyzer = TextAnalyzer()

    word_count = analyzer.count_words("Hello world, this is a test.")

    assert word_count == 6

def test_count_lines():
    """æ¸¬è©¦è¡Œæ•¸çµ±è¨ˆ"""
    analyzer = TextAnalyzer()
    text = "Line one\nLine two\nLine three"

    line_count = analyzer.count_lines(text)

    assert line_count == 3

def test_empty_text_statistics():
    """æ¸¬è©¦ç©ºæ–‡å­—çš„çµ±è¨ˆ"""
    analyzer = TextAnalyzer()

    assert analyzer.count_characters("") == 0
    assert analyzer.count_words("") == 0
    assert analyzer.count_lines("") == 0
```

**åŸ·è¡Œæ¸¬è©¦**ï¼š
```bash
$ pytest test_text_analyzer.py -v

FAILED - ModuleNotFoundError: No module named 'text_analyzer'
```

#### ğŸŸ¢ GREENï¼šå¯¦ä½œåŸºæœ¬çµ±è¨ˆ

**text_analyzer.py**ï¼š
```python
import re
from collections import Counter
from typing import Dict, List, Tuple

class TextAnalyzer:
    """æ–‡å­—åˆ†æå™¨"""

    def count_characters(self, text: str) -> int:
        """è¨ˆç®—å­—å…ƒæ•¸ï¼ˆåŒ…å«ç©ºæ ¼ï¼‰"""
        return len(text)

    def count_characters_no_spaces(self, text: str) -> int:
        """è¨ˆç®—å­—å…ƒæ•¸ï¼ˆä¸åŒ…å«ç©ºæ ¼ï¼‰"""
        return len(text.replace(" ", ""))

    def count_words(self, text: str) -> int:
        """è¨ˆç®—è©æ•¸"""
        if not text.strip():
            return 0
        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åˆ†å‰²è©èª
        words = re.findall(r'\b\w+\b', text)
        return len(words)

    def count_lines(self, text: str) -> int:
        """è¨ˆç®—è¡Œæ•¸"""
        if not text:
            return 0
        return len(text.split('\n'))
```

**åŸ·è¡Œæ¸¬è©¦**ï¼š
```bash
$ pytest test_text_analyzer.py -v

PASSED âœ“âœ“âœ“âœ“âœ“
```

---

### å¾ªç’° 2ï¼šè©é »åˆ†æ

#### ğŸ”´ REDï¼šæ¸¬è©¦è©é »åˆ†æ

**test_text_analyzer.py**ï¼ˆæ–°å¢ï¼‰ï¼š
```python
def test_word_frequency():
    """æ¸¬è©¦è©é »çµ±è¨ˆ"""
    analyzer = TextAnalyzer()
    text = "the cat sat on the mat the cat was happy"

    freq = analyzer.get_word_frequency(text)

    assert freq["the"] == 3
    assert freq["cat"] == 2
    assert freq["sat"] == 1
    assert freq["happy"] == 1

def test_most_common_words():
    """æ¸¬è©¦æœ€é«˜é »è©å½™"""
    analyzer = TextAnalyzer()
    text = "apple banana apple cherry apple banana"

    most_common = analyzer.get_most_common_words(text, 2)

    assert len(most_common) == 2
    assert most_common[0] == ("apple", 3)
    assert most_common[1] == ("banana", 2)

def test_word_frequency_case_insensitive():
    """æ¸¬è©¦å¤§å°å¯«ä¸æ•æ„Ÿçš„è©é »"""
    analyzer = TextAnalyzer()
    text = "Apple APPLE apple"

    freq = analyzer.get_word_frequency(text, case_sensitive=False)

    assert freq["apple"] == 3

def test_unique_words():
    """æ¸¬è©¦å”¯ä¸€è©å½™çµ±è¨ˆ"""
    analyzer = TextAnalyzer()
    text = "the cat sat on the mat"

    unique_count = analyzer.count_unique_words(text)
    unique_words = analyzer.get_unique_words(text)

    assert unique_count == 5
    assert "the" in unique_words
    assert "cat" in unique_words
    assert len(unique_words) == 5
```

#### ğŸŸ¢ GREENï¼šå¯¦ä½œè©é »åˆ†æ

**text_analyzer.py**ï¼ˆæ›´æ–°ï¼‰ï¼š
```python
def get_word_frequency(self, text: str, case_sensitive: bool = True) -> Dict[str, int]:
    """å–å¾—è©é »çµ±è¨ˆ"""
    if not case_sensitive:
        text = text.lower()

    words = re.findall(r'\b\w+\b', text)
    return dict(Counter(words))

def get_most_common_words(self, text: str, n: int = 10) -> List[Tuple[str, int]]:
    """å–å¾—æœ€é«˜é »çš„ n å€‹è©å½™"""
    words = re.findall(r'\b\w+\b', text.lower())
    counter = Counter(words)
    return counter.most_common(n)

def count_unique_words(self, text: str) -> int:
    """è¨ˆç®—å”¯ä¸€è©å½™æ•¸é‡"""
    words = re.findall(r'\b\w+\b', text.lower())
    return len(set(words))

def get_unique_words(self, text: str) -> set:
    """å–å¾—æ‰€æœ‰å”¯ä¸€è©å½™"""
    words = re.findall(r'\b\w+\b', text.lower())
    return set(words)
```

**åŸ·è¡Œæ¸¬è©¦**ï¼š
```bash
$ pytest test_text_analyzer.py -v

PASSED âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“
```

---

### å¾ªç’° 3ï¼šæ–‡å­—æ¸…ç†åŠŸèƒ½

#### ğŸ”´ REDï¼šæ¸¬è©¦æ–‡å­—æ¸…ç†

**test_text_analyzer.py**ï¼ˆæ–°å¢ï¼‰ï¼š
```python
def test_text_cleaner_remove_punctuation():
    """æ¸¬è©¦ç§»é™¤æ¨™é»ç¬¦è™Ÿ"""
    cleaner = TextCleaner()

    cleaned = cleaner.remove_punctuation("Hello, world! How are you?")

    assert cleaned == "Hello world How are you"

def test_text_cleaner_to_lowercase():
    """æ¸¬è©¦è½‰æ›ç‚ºå°å¯«"""
    cleaner = TextCleaner()

    cleaned = cleaner.to_lowercase("Hello WORLD")

    assert cleaned == "hello world"

def test_remove_stop_words():
    """æ¸¬è©¦ç§»é™¤åœç”¨è©"""
    cleaner = TextCleaner()
    text = "this is a test of the system"

    cleaned = cleaner.remove_stop_words(text)

    # å‡è¨­ 'a', 'the', 'of', 'is' æ˜¯åœç”¨è©
    assert "test" in cleaned
    assert "system" in cleaned
    assert "this" in cleaned  # åªç§»é™¤å¸¸è¦‹åœç”¨è©

def test_clean_text_complete():
    """æ¸¬è©¦å®Œæ•´çš„æ–‡å­—æ¸…ç†æµç¨‹"""
    cleaner = TextCleaner()
    text = "Hello, World! This is a TEST."

    cleaned = cleaner.clean_text(text)

    # æ‡‰è©²ç§»é™¤æ¨™é»ã€è½‰å°å¯«ã€ç§»é™¤åœç”¨è©
    assert "hello" in cleaned
    assert "world" in cleaned
    assert "test" in cleaned
    assert "," not in cleaned
    assert "!" not in cleaned
```

#### ğŸŸ¢ GREENï¼šå¯¦ä½œæ–‡å­—æ¸…ç†

**text_cleaner.py**ï¼š
```python
import re
import string

class TextCleaner:
    """æ–‡å­—æ¸…ç†å·¥å…·"""

    # å¸¸è¦‹è‹±æ–‡åœç”¨è©
    STOP_WORDS = {
        'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',
        'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',
        'to', 'was', 'will', 'with'
    }

    def remove_punctuation(self, text: str) -> str:
        """ç§»é™¤æ¨™é»ç¬¦è™Ÿ"""
        translator = str.maketrans('', '', string.punctuation)
        return text.translate(translator)

    def to_lowercase(self, text: str) -> str:
        """è½‰æ›ç‚ºå°å¯«"""
        return text.lower()

    def remove_stop_words(self, text: str) -> str:
        """ç§»é™¤åœç”¨è©"""
        words = text.split()
        filtered_words = [word for word in words if word.lower() not in self.STOP_WORDS]
        return ' '.join(filtered_words)

    def clean_text(self, text: str) -> str:
        """å®Œæ•´çš„æ–‡å­—æ¸…ç†æµç¨‹"""
        # ç§»é™¤æ¨™é»
        text = self.remove_punctuation(text)
        # è½‰å°å¯«
        text = self.to_lowercase(text)
        # ç§»é™¤åœç”¨è©
        text = self.remove_stop_words(text)
        # ç§»é™¤å¤šé¤˜ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def normalize_whitespace(self, text: str) -> str:
        """æ¨™æº–åŒ–ç©ºç™½å­—å…ƒ"""
        return re.sub(r'\s+', ' ', text).strip()
```

**åŸ·è¡Œæ¸¬è©¦**ï¼š
```bash
$ pytest test_text_analyzer.py -v

PASSED âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“
```

---

### å¾ªç’° 4ï¼šé—œéµè©æå–

#### ğŸ”´ REDï¼šæ¸¬è©¦é—œéµè©æå–

**test_text_analyzer.py**ï¼ˆæ–°å¢ï¼‰ï¼š
```python
def test_extract_keywords_by_frequency():
    """æ¸¬è©¦æŒ‰é »ç‡æå–é—œéµè©"""
    analyzer = TextAnalyzer()
    text = "Python programming is fun. Python is powerful. Programming with Python is enjoyable."

    keywords = analyzer.extract_keywords(text, method="frequency", top_k=3)

    assert len(keywords) <= 3
    assert "python" in [kw.lower() for kw in keywords]
    assert "programming" in [kw.lower() for kw in keywords]

def test_extract_keywords_by_length():
    """æ¸¬è©¦æŒ‰é•·åº¦æå–é—œéµè©"""
    analyzer = TextAnalyzer()
    text = "AI machine learning deep neural networks"

    keywords = analyzer.extract_keywords(text, method="length", min_length=5)

    long_keywords = [kw for kw in keywords if len(kw) >= 5]
    assert "machine" in keywords
    assert "learning" in keywords
    assert "neural" in keywords
    assert "networks" in keywords

def test_calculate_text_statistics():
    """æ¸¬è©¦ç¶œåˆæ–‡å­—çµ±è¨ˆ"""
    analyzer = TextAnalyzer()
    text = "The quick brown fox jumps over the lazy dog. The dog was sleeping."

    stats = analyzer.get_text_statistics(text)

    assert stats["character_count"] > 0
    assert stats["word_count"] > 0
    assert stats["line_count"] > 0
    assert stats["unique_words"] > 0
    assert "most_common_words" in stats
    assert isinstance(stats["average_word_length"], float)
```

#### ğŸŸ¢ GREENï¼šå¯¦ä½œé—œéµè©æå–

**text_analyzer.py**ï¼ˆæ›´æ–°ï¼‰ï¼š
```python
def extract_keywords(self, text: str, method: str = "frequency", top_k: int = 5, min_length: int = 3) -> List[str]:
    """æå–é—œéµè©"""
    # æ¸…ç†æ–‡å­—
    from text_cleaner import TextCleaner
    cleaner = TextCleaner()
    cleaned_text = cleaner.clean_text(text)

    words = re.findall(r'\b\w+\b', cleaned_text)

    if method == "frequency":
        # æŒ‰é »ç‡æå–
        counter = Counter(words)
        return [word for word, count in counter.most_common(top_k)]

    elif method == "length":
        # æŒ‰é•·åº¦æå–
        long_words = [word for word in set(words) if len(word) >= min_length]
        return sorted(long_words, key=len, reverse=True)[:top_k]

    else:
        raise ValueError(f"Unknown method: {method}")

def get_text_statistics(self, text: str) -> Dict:
    """å–å¾—ç¶œåˆæ–‡å­—çµ±è¨ˆ"""
    stats = {
        "character_count": self.count_characters(text),
        "character_count_no_spaces": self.count_characters_no_spaces(text),
        "word_count": self.count_words(text),
        "line_count": self.count_lines(text),
        "unique_words": self.count_unique_words(text),
        "most_common_words": self.get_most_common_words(text, 5),
        "average_word_length": self._calculate_average_word_length(text)
    }
    return stats

def _calculate_average_word_length(self, text: str) -> float:
    """è¨ˆç®—å¹³å‡è©é•·"""
    words = re.findall(r'\b\w+\b', text)
    if not words:
        return 0.0
    total_length = sum(len(word) for word in words)
    return total_length / len(words)
```

---

### å¾ªç’° 5ï¼šæ–‡å­—ç›¸ä¼¼åº¦å’Œæƒ…æ„Ÿåˆ†æ

#### ğŸ”´ REDï¼šæ¸¬è©¦ç›¸ä¼¼åº¦å’Œæƒ…æ„Ÿåˆ†æ

**test_text_analyzer.py**ï¼ˆæ–°å¢ï¼‰ï¼š
```python
def test_text_similarity():
    """æ¸¬è©¦æ–‡å­—ç›¸ä¼¼åº¦"""
    analyzer = TextAnalyzer()
    text1 = "The cat sat on the mat"
    text2 = "A cat was sitting on a mat"
    text3 = "Dogs are running in the park"

    similarity1 = analyzer.calculate_similarity(text1, text2)
    similarity2 = analyzer.calculate_similarity(text1, text3)

    # text1 å’Œ text2 æ‡‰è©²æ¯”è¼ƒç›¸ä¼¼
    assert similarity1 > similarity2
    assert 0 <= similarity1 <= 1
    assert 0 <= similarity2 <= 1

def test_sentiment_analysis():
    """æ¸¬è©¦æƒ…æ„Ÿåˆ†æ"""
    sentiment = SentimentAnalyzer()

    positive_text = "I love this product! It's amazing and wonderful!"
    negative_text = "This is terrible, awful, and disappointing."
    neutral_text = "The product has basic features."

    pos_score = sentiment.analyze_sentiment(positive_text)
    neg_score = sentiment.analyze_sentiment(negative_text)
    neu_score = sentiment.analyze_sentiment(neutral_text)

    assert pos_score["sentiment"] == "positive"
    assert neg_score["sentiment"] == "negative"
    assert neu_score["sentiment"] == "neutral"

def test_find_common_words():
    """æ¸¬è©¦æ‰¾å‡ºå…±åŒè©å½™"""
    analyzer = TextAnalyzer()
    text1 = "cats and dogs are pets"
    text2 = "dogs and birds are animals"

    common = analyzer.find_common_words(text1, text2)

    assert "and" in common
    assert "are" in common
    assert "dogs" in common
    assert len(common) >= 3
```

#### ğŸŸ¢ GREENï¼šå¯¦ä½œç›¸ä¼¼åº¦å’Œæƒ…æ„Ÿåˆ†æ

**text_analyzer.py**ï¼ˆæ›´æ–°ï¼‰ï¼š
```python
def calculate_similarity(self, text1: str, text2: str) -> float:
    """è¨ˆç®—å…©å€‹æ–‡å­—çš„ç›¸ä¼¼åº¦ï¼ˆJaccard ç›¸ä¼¼åº¦ï¼‰"""
    words1 = set(re.findall(r'\b\w+\b', text1.lower()))
    words2 = set(re.findall(r'\b\w+\b', text2.lower()))

    if not words1 and not words2:
        return 1.0
    if not words1 or not words2:
        return 0.0

    intersection = words1.intersection(words2)
    union = words1.union(words2)

    return len(intersection) / len(union)

def find_common_words(self, text1: str, text2: str) -> List[str]:
    """æ‰¾å‡ºå…©å€‹æ–‡å­—çš„å…±åŒè©å½™"""
    words1 = set(re.findall(r'\b\w+\b', text1.lower()))
    words2 = set(re.findall(r'\b\w+\b', text2.lower()))

    return list(words1.intersection(words2))
```

**sentiment_analyzer.py**ï¼š
```python
class SentimentAnalyzer:
    """ç°¡å–®çš„æƒ…æ„Ÿåˆ†æå™¨"""

    # ç°¡å–®çš„æƒ…æ„Ÿè©å…¸
    POSITIVE_WORDS = {
        'love', 'amazing', 'wonderful', 'great', 'excellent', 'fantastic',
        'good', 'happy', 'joy', 'perfect', 'awesome', 'brilliant'
    }

    NEGATIVE_WORDS = {
        'hate', 'terrible', 'awful', 'bad', 'horrible', 'disappointing',
        'sad', 'angry', 'worst', 'disgusting', 'pathetic', 'useless'
    }

    def analyze_sentiment(self, text: str) -> dict:
        """åˆ†ææ–‡å­—æƒ…æ„Ÿ"""
        words = text.lower().split()

        positive_count = sum(1 for word in words if word in self.POSITIVE_WORDS)
        negative_count = sum(1 for word in words if word in self.NEGATIVE_WORDS)

        total_sentiment_words = positive_count + negative_count

        if total_sentiment_words == 0:
            sentiment = "neutral"
            confidence = 0.5
        elif positive_count > negative_count:
            sentiment = "positive"
            confidence = positive_count / len(words)
        elif negative_count > positive_count:
            sentiment = "negative"
            confidence = negative_count / len(words)
        else:
            sentiment = "neutral"
            confidence = 0.5

        return {
            "sentiment": sentiment,
            "confidence": min(confidence * 2, 1.0),  # èª¿æ•´ä¿¡å¿ƒåº¦
            "positive_words": positive_count,
            "negative_words": negative_count
        }

    def get_sentiment_words(self, text: str) -> dict:
        """å–å¾—æ–‡å­—ä¸­çš„æƒ…æ„Ÿè©å½™"""
        words = text.lower().split()

        found_positive = [word for word in words if word in self.POSITIVE_WORDS]
        found_negative = [word for word in words if word in self.NEGATIVE_WORDS]

        return {
            "positive": found_positive,
            "negative": found_negative
        }
```

---

## âœ… åŸ·è¡Œæ¸¬è©¦

**åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦**ï¼š
```bash
$ pytest test_text_analyzer.py -v

================= test session starts =================
# æ‰€æœ‰æ¸¬è©¦éƒ½æ‡‰è©²é€šé
================= XX passed in X.XXs =================
```

**æ¸¬è©¦è¦†è“‹ç‡**ï¼š
```bash
$ pytest --cov=. --cov-report=term-missing

Name                   Stmts   Miss  Cover
------------------------------------------
sentiment_analyzer.py     XX      0   100%
text_analyzer.py          XX      0   100%
text_cleaner.py           XX      0   100%
------------------------------------------
TOTAL                     XX      0   100%
```

---

## ğŸ“ å­¸ç¿’é‡é»

### æ¼”ç®—æ³•é‚è¼¯çš„ TDD

1. **æ–‡å­—è™•ç†æ¼”ç®—æ³•**ï¼š
   - å­—ä¸²åˆ†å‰²å’Œæ­£å‰‡è¡¨é”å¼çš„ä½¿ç”¨
   - çµ±è¨ˆæ¼”ç®—æ³•çš„æ­£ç¢ºæ€§é©—è­‰
   - é‚Šç•Œæƒ…æ³çš„å®Œæ•´è™•ç†

2. **è³‡æ–™çµæ§‹çš„é¸æ“‡**ï¼š
   - Counter ç”¨æ–¼é »ç‡çµ±è¨ˆ
   - Set ç”¨æ–¼å”¯ä¸€æ€§æª¢æŸ¥
   - Dict ç”¨æ–¼è¤‡é›œè³‡æ–™å­˜å„²

3. **æ¼”ç®—æ³•æ•ˆèƒ½è€ƒé‡**ï¼š
   - å¤§å°å¯«ä¸æ•æ„Ÿçš„è™•ç†
   - é‡è¤‡è¨ˆç®—çš„é¿å…
   - è¨˜æ†¶é«”æ•ˆç‡çš„è€ƒæ…®

### é—œéµæ”¶ç©«

âœ… **å­—ä¸²è™•ç†æŠ€å·§**ï¼š
- æ­£å‰‡è¡¨é”å¼çš„é«˜æ•ˆä½¿ç”¨
- æ–‡å­—æ¸…ç†çš„æ¨™æº–æµç¨‹
- Unicode å’Œç·¨ç¢¼çš„è™•ç†

âœ… **çµ±è¨ˆå’Œåˆ†æ**ï¼š
- è©é »åˆ†æçš„å¯¦ä½œ
- ç›¸ä¼¼åº¦è¨ˆç®—æ¼”ç®—æ³•
- ç°¡å–®çš„æ©Ÿå™¨å­¸ç¿’æ¦‚å¿µ

âœ… **æ¨¡çµ„åŒ–è¨­è¨ˆ**ï¼š
- TextAnalyzerã€TextCleanerã€SentimentAnalyzer çš„è·è²¬åˆ†å·¥
- å¯æ“´å±•çš„æ¶æ§‹è¨­è¨ˆ
- æ¸…æ™°çš„ä»‹é¢å®šç¾©

---

## ğŸš€ é€²éšæŒ‘æˆ°

### æŒ‘æˆ° 1ï¼šé€²éšæ–‡å­—åˆ†æ
- TF-IDF é—œéµè©æå–
- N-gram åˆ†æ
- æ–‡å­—æ‘˜è¦ç”Ÿæˆ

### æŒ‘æˆ° 2ï¼šå¤šèªè¨€æ”¯æ´
- ä¸­æ–‡è©èªåˆ†å‰²
- å¤šèªè¨€åœç”¨è©
- èªè¨€æª¢æ¸¬åŠŸèƒ½

### æŒ‘æˆ° 3ï¼šæ©Ÿå™¨å­¸ç¿’æ•´åˆ
- ä½¿ç”¨ scikit-learn é€²è¡Œåˆ†é¡
- è©å‘é‡ï¼ˆWord2Vecï¼‰ç›¸ä¼¼åº¦
- æ·±åº¦å­¸ç¿’æƒ…æ„Ÿåˆ†æ

### æŒ‘æˆ° 4ï¼šæ•ˆèƒ½å„ªåŒ–
- å¤§æ–‡ä»¶æµå¼è™•ç†
- ä¸¦è¡Œè¨ˆç®—æ”¯æ´
- è¨˜æ†¶é«”ä½¿ç”¨å„ªåŒ–

---

## ğŸ“ˆ è‡ªæˆ‘è©•é‡

- [ ] èƒ½å¯¦ä½œåŸºæœ¬çš„æ–‡å­—çµ±è¨ˆåŠŸèƒ½
- [ ] è©é »åˆ†ææ¼”ç®—æ³•æ­£ç¢º
- [ ] æ–‡å­—æ¸…ç†åŠŸèƒ½å®Œæ•´
- [ ] é—œéµè©æå–é‚è¼¯åˆç†
- [ ] ç›¸ä¼¼åº¦è¨ˆç®—æº–ç¢º
- [ ] ç¨‹å¼ç¢¼æ¨¡çµ„åŒ–è‰¯å¥½

**æ­å–œå®Œæˆæ–‡å­—åˆ†æå™¨çš„ TDD å¯¦ä½œï¼**
**ä½ ç¾åœ¨èƒ½ç”¨ TDD é–‹ç™¼æ¼”ç®—æ³•å¯†é›†çš„æ‡‰ç”¨äº†ï¼**